{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the problem addressed in this paper:\n",
    "\n",
    " - localizating an object in a large image\n",
    " - foveation\n",
    " - action (saccade)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This notebook introduces the problem addressed in this paper:\n",
      "\n",
      " - localizating an object in a large image\n",
      " - foveation\n",
      " - action (saccade)\n",
      "       \n",
      "      \n",
      "{'w': 28, 'minibatch_size': 100, 'train_batch_size': 50000, 'test_batch_size': 5000, 'noise_batch_size': 1000, 'mean': 0.1307, 'std': 0.3081, 'N_pic': 128, 'offset_std': 30, 'offset_max': 34, 'noise': 1.0, 'contrast': 1.0, 'sf_0': 0.2, 'B_sf': 0.3, 'N_theta': 6, 'N_azimuth': 26, 'N_eccentricity': 10, 'N_phase': 2, 'rho': 1.41, 'bias_deconv': True, 'p_dropout': 0.0, 'dim1': 500, 'dim2': 2000, 'lr': 0.001, 'do_adam': True, 'bn1_bn_momentum': 0.5, 'bn2_bn_momentum': 0.2, 'momentum': 0.1, 'epochs': 25, 'num_processes': 1, 'no_cuda': True, 'log_interval': 100, 'verbose': 1, 'filename': '../data/2019-03-27', 'seed': 2019, 'N_cv': 8, 'do_compute': True}\n",
      "Overwriting train.py\n",
      "2019-07-17T17:37:42+02:00\n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 7.6.1\n",
      "\n",
      "numpy 1.16.4\n",
      "matplotlib 3.1.1\n",
      "torch 1.1.0.post2\n",
      "\n",
      "compiler   : Clang 10.0.1 (clang-1001.0.46.3)\n",
      "system     : Darwin\n",
      "release    : 18.6.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : ekla\n",
      "Git hash   : fa01a8637b54d5da5ebf7e4cbdc7dbe8402afe92\n",
      "Git repo   : https://github.com/laurentperrinet/WhereIsMyMNIST\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "%run 0_parameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T10:34:13.258190Z",
     "start_time": "2018-07-03T10:34:13.251661Z"
    }
   },
   "outputs": [],
   "source": [
    "figname = '../paper/fig_result'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Where network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 13 22:14 ../data/2019-03-13_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 14 17:44 ../data/2019-03-14_b_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 14 06:17 ../data/2019-03-14_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 14 14:54 ../data/2019-03-14_train3.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 14 19:28 ../data/2019-03-14_train4.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 15 22:18 ../data/2019-03-15_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 15 15:31 ../data/2019-03-15_train_b.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   8.8M Mar 20 17:46 ../data/2019-03-16_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 19 00:19 ../data/2019-03-18_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 19 15:18 ../data/2019-03-19_bis_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 19 15:54 ../data/2019-03-19_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    12M Mar 27 16:30 ../data/2019-03-27_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    11M Mar 29 16:13 ../data/2019-03-29_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   5.7M Apr  8 23:09 ../data/2019-04-01_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   6.1M Apr  3 12:47 ../data/2019-04-03_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    16M Apr 25 17:17 ../data/2019-04-15_bis_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    16M Apr 15 14:28 ../data/2019-04-15_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff    16M Apr 19 11:00 ../data/2019-04-16_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Mar 14 12:31 ../data/MNIST_cnn.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  1 09:09 ../data/MNIST_cnn_0.05_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  1 12:31 ../data/MNIST_cnn_0.06299605249474366_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 17 22:59 ../data/MNIST_cnn_0.07071067811865475_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 18 13:22 ../data/MNIST_cnn_0.07937005259840997_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 18 15:27 ../data/MNIST_cnn_0.08908987181403394_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 18 21:44 ../data/MNIST_cnn_0.11224620483093731_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 19 02:31 ../data/MNIST_cnn_0.12599210498948732_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 19 03:59 ../data/MNIST_cnn_0.14142135623730953_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  1 14:31 ../data/MNIST_cnn_0.15874010519681994_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  1 21:43 ../data/MNIST_cnn_0.1_0.05_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  1 23:53 ../data/MNIST_cnn_0.1_0.06299605249474366_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 19 08:43 ../data/MNIST_cnn_0.1_0.07071067811865475_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 17 21:39 ../data/MNIST_cnn_0.1_0.07937005259840997_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 19 10:48 ../data/MNIST_cnn_0.1_0.08908987181403394_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 19 10:49 ../data/MNIST_cnn_0.1_0.11224620483093731_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 19 10:49 ../data/MNIST_cnn_0.1_0.12599210498948732_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 19 10:49 ../data/MNIST_cnn_0.1_0.14142135623730953_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  2 02:31 ../data/MNIST_cnn_0.1_0.15874010519681994_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  2 16:09 ../data/MNIST_cnn_0.1_0.1_0.375_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  2 23:24 ../data/MNIST_cnn_0.1_0.1_0.4724703937105774_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 20 04:19 ../data/MNIST_cnn_0.1_0.1_0.5303300858899106_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 20 04:19 ../data/MNIST_cnn_0.1_0.1_0.5952753944880748_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 20 09:33 ../data/MNIST_cnn_0.1_0.1_0.6681740386052545_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  3 06:03 ../data/MNIST_cnn_0.1_0.1_0.75_0.35.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  3 07:04 ../data/MNIST_cnn_0.1_0.1_0.75_0.44097236746320556.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 20 18:15 ../data/MNIST_cnn_0.1_0.1_0.75_0.4949747468305832.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 20 18:57 ../data/MNIST_cnn_0.1_0.1_0.75_0.5555903681888698.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 20 18:58 ../data/MNIST_cnn_0.1_0.1_0.75_0.6236291026982375.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 17 21:39 ../data/MNIST_cnn_0.1_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 21 05:18 ../data/MNIST_cnn_0.1_0.1_0.75_0.7857234338165611.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 21 08:11 ../data/MNIST_cnn_0.1_0.1_0.75_0.8819447349264111.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 21 09:04 ../data/MNIST_cnn_0.1_0.1_0.75_0.9899494936611666.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  3 09:58 ../data/MNIST_cnn_0.1_0.1_0.75_1.1111807363777395.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  3 11:21 ../data/MNIST_cnn_0.1_0.1_0.75_1.4.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 20 17:23 ../data/MNIST_cnn_0.1_0.1_0.8418465362320298_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 20 17:57 ../data/MNIST_cnn_0.1_0.1_0.9449407874211548_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Apr 20 18:08 ../data/MNIST_cnn_0.1_0.1_1.0606601717798214_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  3 01:31 ../data/MNIST_cnn_0.1_0.1_1.1905507889761495_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  3 04:02 ../data/MNIST_cnn_0.1_0.1_1.5_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  2 02:30 ../data/MNIST_cnn_0.1_0.2_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M May  1 14:41 ../data/MNIST_cnn_0.2_0.1_0.75_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.1_5epoques_2019-06-20_14h05.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.2_5epoques_2019-06-20_13h58.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.3_5epoques_2019-06-20_13h52.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.4_5epoques_2019-06-20_13h46.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.5_5epoques_2019-06-20_13h40.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.6_5epoques_2019-06-20_13h34.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.7_1epoques_2019-06-14_14h32.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.7_5epoques_2019-06-14_15h00.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.7_5epoques_2019-06-17_16h55.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.7_5epoques_2019-06-19_11h50.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.7_5epoques_2019-06-20_13h27.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.7_60epoques_2019-06-17_10h20.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1.0_0.7_60epoques_2019-06-17_16h54.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1_0.7.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1_0.7_60epoques.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   1.7M Jul  4 14:43 ../data/MNIST_cnn_robust_what_0.1_0.1_1_0.7_60epoques_2019-06-13_10h47.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   6.1M Apr  8 16:34 ../data/debug_train.pt\n",
      "-rw-r--r--  1 laurentperrinet  staff   150M Apr 15 13:22 ../data/noisy-MNIST.pt\n"
     ]
    }
   ],
   "source": [
    "%ls -lh ../data/*pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'what.WhatNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retina vectorizing...\n",
      "ok\n",
      "success\n",
      "Done vectorizing...\n",
      "Inversing retina transform...\n",
      "Done Inversing retina transform...\n",
      "Generating training dataset\n",
      "0 100\n",
      "1 200\n",
      "2 300\n",
      "3 400\n",
      "4 500\n",
      "5 600\n",
      "6 700\n",
      "7 800\n",
      "8 900\n",
      "9 1000\n",
      "10 1100\n",
      "11 1200\n",
      "12 1300\n",
      "13 1400\n",
      "14 1500\n",
      "15 1600\n",
      "16 1700\n",
      "17 1800\n",
      "18 1900\n",
      "19 2000\n",
      "20 2100\n",
      "21 2200\n",
      "22 2300\n",
      "23 2400\n",
      "24 2500\n",
      "25 2600\n",
      "26 2700\n",
      "27 2800\n",
      "28 2900\n",
      "29 3000\n",
      "30 3100\n",
      "31 3200\n",
      "32 3300\n",
      "33 3400\n",
      "34 3500\n",
      "35 3600\n",
      "36 3700\n",
      "37 3800\n",
      "38 3900\n",
      "39 4000\n",
      "40 4100\n",
      "41 4200\n",
      "42 4300\n",
      "43 4400\n",
      "44 4500\n",
      "45 4600\n",
      "46 4700\n",
      "47 4800\n",
      "48 4900\n",
      "49 5000\n",
      "50 5100\n",
      "51 5200\n",
      "52 5300\n",
      "53 5400\n",
      "54 5500\n",
      "55 5600\n",
      "56 5700\n",
      "57 5800\n",
      "58 5900\n",
      "59 6000\n",
      "60 6100\n",
      "61 6200\n",
      "62 6300\n",
      "63 6400\n",
      "64 6500\n",
      "65 6600\n",
      "66 6700\n",
      "67 6800\n",
      "68 6900\n",
      "69 7000\n",
      "70 7100\n",
      "71 7200\n",
      "72 7300\n",
      "73 7400\n",
      "74 7500\n",
      "75 7600\n",
      "76 7700\n",
      "77 7800\n",
      "78 7900\n",
      "79 8000\n",
      "80 8100\n",
      "81 8200\n",
      "82 8300\n",
      "83 8400\n",
      "84 8500\n",
      "85 8600\n",
      "86 8700\n",
      "87 8800\n",
      "88 8900\n",
      "89 9000\n",
      "90 9100\n",
      "91 9200\n",
      "92 9300\n",
      "93 9400\n",
      "94 9500\n",
      "95 9600\n",
      "96 9700\n",
      "97 9800\n",
      "98 9900\n",
      "99 10000\n",
      "100 10100\n",
      "101 10200\n",
      "102 10300\n",
      "103 10400\n",
      "104 10500\n",
      "105 10600\n",
      "106 10700\n",
      "107 10800\n",
      "108 10900\n",
      "109 11000\n",
      "110 11100\n",
      "111 11200\n",
      "112 11300\n",
      "113 11400\n",
      "114 11500\n",
      "115 11600\n",
      "116 11700\n",
      "117 11800\n",
      "118 11900\n",
      "119 12000\n",
      "120 12100\n",
      "121 12200\n",
      "122 12300\n",
      "123 12400\n",
      "124 12500\n",
      "125 12600\n",
      "126 12700\n",
      "127 12800\n",
      "128 12900\n",
      "129 13000\n",
      "130 13100\n",
      "131 13200\n",
      "132 13300\n",
      "133 13400\n",
      "134 13500\n",
      "135 13600\n",
      "136 13700\n",
      "137 13800\n",
      "138 13900\n",
      "139 14000\n",
      "140 14100\n",
      "141 14200\n",
      "142 14300\n",
      "143 14400\n",
      "144 14500\n",
      "145 14600\n",
      "146 14700\n",
      "147 14800\n",
      "148 14900\n",
      "149 15000\n",
      "150 15100\n",
      "151 15200\n",
      "152 15300\n",
      "153 15400\n",
      "154 15500\n",
      "155 15600\n",
      "156 15700\n",
      "157 15800\n",
      "158 15900\n",
      "159 16000\n",
      "160 16100\n",
      "161 16200\n",
      "162 16300\n",
      "163 16400\n",
      "164 16500\n",
      "165 16600\n",
      "166 16700\n",
      "167 16800\n",
      "168 16900\n",
      "169 17000\n",
      "170 17100\n",
      "171 17200\n",
      "172 17300\n",
      "173 17400\n",
      "174 17500\n",
      "175 17600\n",
      "176 17700\n",
      "177 17800\n",
      "178 17900\n",
      "179 18000\n",
      "180 18100\n",
      "181 18200\n",
      "182 18300\n",
      "183 18400\n",
      "184 18500\n",
      "185 18600\n",
      "186 18700\n",
      "187 18800\n",
      "188 18900\n",
      "189 19000\n",
      "190 19100\n",
      "191 19200\n",
      "192 19300\n",
      "193 19400\n",
      "194 19500\n",
      "195 19600\n",
      "196 19700\n",
      "197 19800\n",
      "198 19900\n",
      "199 20000\n",
      "200 20100\n",
      "201 20200\n",
      "202 20300\n",
      "203 20400\n",
      "204 20500\n",
      "205 20600\n",
      "206 20700\n",
      "207 20800\n",
      "208 20900\n",
      "209 21000\n",
      "210 21100\n",
      "211 21200\n",
      "212 21300\n",
      "213 21400\n",
      "214 21500\n",
      "215 21600\n",
      "216 21700\n",
      "217 21800\n",
      "218 21900\n",
      "219 22000\n",
      "220 22100\n",
      "221 22200\n",
      "222 22300\n",
      "223 22400\n",
      "224 22500\n",
      "225 22600\n",
      "226 22700\n",
      "227 22800\n",
      "228 22900\n",
      "229 23000\n",
      "230 23100\n",
      "231 23200\n",
      "232 23300\n",
      "233 23400\n",
      "234 23500\n",
      "235 23600\n",
      "236 23700\n",
      "237 23800\n",
      "238 23900\n",
      "239 24000\n",
      "240 24100\n",
      "241 24200\n",
      "242 24300\n",
      "243 24400\n",
      "244 24500\n",
      "245 24600\n",
      "246 24700\n",
      "247 24800\n",
      "248 24900\n",
      "249 25000\n",
      "250 25100\n",
      "251 25200\n",
      "252 25300\n",
      "253 25400\n",
      "254 25500\n",
      "255 25600\n",
      "256 25700\n",
      "257 25800\n",
      "258 25900\n",
      "259 26000\n",
      "260 26100\n",
      "261 26200\n",
      "262 26300\n",
      "263 26400\n",
      "264 26500\n",
      "265 26600\n",
      "266 26700\n",
      "267 26800\n",
      "268 26900\n",
      "269 27000\n",
      "270 27100\n",
      "271 27200\n",
      "272 27300\n",
      "273 27400\n",
      "274 27500\n",
      "275 27600\n",
      "276 27700\n",
      "277 27800\n",
      "278 27900\n",
      "279 28000\n",
      "280 28100\n",
      "281 28200\n",
      "282 28300\n",
      "283 28400\n",
      "284 28500\n",
      "285 28600\n",
      "286 28700\n",
      "287 28800\n",
      "288 28900\n",
      "289 29000\n",
      "290 29100\n",
      "291 29200\n",
      "292 29300\n",
      "293 29400\n",
      "294 29500\n",
      "295 29600\n",
      "296 29700\n",
      "297 29800\n",
      "298 29900\n",
      "299 30000\n",
      "300 30100\n",
      "301 30200\n",
      "302 30300\n",
      "303 30400\n",
      "304 30500\n",
      "305 30600\n",
      "306 30700\n",
      "307 30800\n",
      "308 30900\n",
      "309 31000\n",
      "310 31100\n",
      "311 31200\n",
      "312 31300\n",
      "313 31400\n",
      "314 31500\n",
      "315 31600\n",
      "316 31700\n",
      "317 31800\n",
      "318 31900\n",
      "319 32000\n",
      "320 32100\n",
      "321 32200\n",
      "322 32300\n",
      "323 32400\n",
      "324 32500\n",
      "325 32600\n",
      "326 32700\n",
      "327 32800\n",
      "328 32900\n",
      "329 33000\n",
      "330 33100\n",
      "331 33200\n",
      "332 33300\n",
      "333 33400\n",
      "334 33500\n",
      "335 33600\n",
      "336 33700\n",
      "337 33800\n",
      "338 33900\n",
      "339 34000\n",
      "340 34100\n",
      "341 34200\n",
      "342 34300\n",
      "343 34400\n",
      "344 34500\n",
      "345 34600\n",
      "346 34700\n",
      "347 34800\n",
      "348 34900\n",
      "349 35000\n",
      "350 35100\n",
      "351 35200\n",
      "352 35300\n",
      "353 35400\n",
      "354 35500\n",
      "355 35600\n",
      "356 35700\n",
      "357 35800\n",
      "358 35900\n",
      "359 36000\n",
      "360 36100\n",
      "361 36200\n",
      "362 36300\n",
      "363 36400\n",
      "364 36500\n",
      "365 36600\n",
      "366 36700\n",
      "367 36800\n",
      "368 36900\n",
      "369 37000\n",
      "370 37100\n",
      "371 37200\n",
      "372 37300\n",
      "373 37400\n",
      "374 37500\n",
      "375 37600\n",
      "376 37700\n",
      "377 37800\n",
      "378 37900\n"
     ]
    }
   ],
   "source": [
    "from main import init\n",
    "#args = init(filename='debug')\n",
    "#args = init(filename='../data/2019-03-19_bis')\n",
    "#args = init()\n",
    "args = init(filename='../data/2019-04-16')\n",
    "\n",
    "from where import Where\n",
    "from what import WhatNet\n",
    "where = Where(args)\n",
    "\n",
    "filename_train = args.filename + '_train.pt'\n",
    "#filename_train = \"../data/2019-03-14_train4.pt\"\n",
    "#filename_train = \"../data/2019-03-29.pt\"\n",
    "#%ls -lh {filename_train}\n",
    "#%rm {filename_train}\n",
    "#%rm  ../data/debug_train.pt\n",
    "\n",
    "where.train(filename_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(where.display.loader_test))\n",
    "idx_start, idx_stop = 10, 20\n",
    "\n",
    "positions, data_fullfield, retina_data, accuracy_colliculus = where.minibatch(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knowing the target position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.zeros((args.test_batch_size, args.w, args.w))\n",
    "for idx in range(args.test_batch_size):\n",
    "    im[idx, :, :] = where.extract(data_fullfield[idx, :, :], positions[idx]['i_offset'], positions[idx]['j_offset'])\n",
    "proba = where.classify_what(im).numpy()\n",
    "pred = proba.argmax(axis=1) # get the index of the max log-probability\n",
    "#acc = proba[:, pred]\n",
    "acc_max = (pred==label.numpy()).mean()\n",
    "print('Accuracy max (knowing the position)=', acc_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(idx_start, idx_stop):\n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "    \n",
    "    ax = fig.add_subplot(141)\n",
    "    ax = where.display.show(ax, data_fullfield[idx, :, :])\n",
    "    ax.set_title(f\"i={positions[idx]['i_offset']}, j={positions[idx]['j_offset']}\")\n",
    "    \n",
    "    ax = fig.add_subplot(142)\n",
    "    data_retina = where.retina.retina(data_fullfield[idx, :, :])\n",
    "    ax = where.retina.show(ax, where.retina.retina_invert(data_retina))\n",
    "    ax.set_title(f\"idx={idx}\")\n",
    "              \n",
    "    ax = fig.add_subplot(143, projection='polar')\n",
    "    ax.pcolor(where.retina.theta, where.retina.log_r, accuracy_colliculus[idx, :].numpy().reshape((args.N_azimuth, args.N_eccentricity)), cmap=plt.plasma())\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_title(\"True\")\n",
    "\n",
    "    ax = fig.add_subplot(144)\n",
    "    ax = where.display.show(ax, im[idx, :, :], do_cross=False)\n",
    "    result = '' if pred[idx]==label[idx].numpy() else 'FALSE'\n",
    "    ax.set_title(f\"pred={pred[idx]} acc={proba[idx,pred[idx]]:.2f} {result}\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting the position of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_accuracy_colliculus = where.pred_accuracy(retina_data)\n",
    "print('pred_accuracy_colliculus.shape=', pred_accuracy_colliculus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(idx_start, idx_stop):\n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "\n",
    "    ax = fig.add_subplot(131, projection='polar')\n",
    "    ax.pcolor(where.retina.theta, where.retina.log_r, pred_accuracy_colliculus[idx, :].reshape((args.N_azimuth, args.N_eccentricity)), cmap=plt.plasma())\n",
    "    ax.set_title(\"Predicted\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "              \n",
    "    x = where.retina.accuracy_invert(pred_accuracy_colliculus[idx, :])\n",
    "    ax = fig.add_subplot(132)\n",
    "    ax.imshow(x, vmin=0, cmap=plt.plasma())\n",
    "    ax.set_title(\"Pred visual space\")\n",
    "    \n",
    "    i_pred, j_pred = where.index_prediction(pred_accuracy_colliculus[idx, :])\n",
    "    \n",
    "    ax = fig.add_subplot(133)\n",
    "    ax = where.display.show(ax, data_fullfield[idx, :, :])\n",
    "    ax.set_title(f\"i={positions[idx]['i_offset']}/{i_pred}, j={positions[idx]['j_offset']}/{j_pred}\")\n",
    "    ax.plot([positions[idx]['j_offset']+args.N_pic//2], [positions[idx]['i_offset']+args.N_pic//2], '+', c='r', ms=26, markeredgewidth=2, alpha=.5)\n",
    "    ax.plot([j_pred+args.N_pic//2], [i_pred+args.N_pic//2], '+', c='b', ms=26, markeredgewidth=2, alpha=.5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doing a saccade at the predicted the position of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_fullfield.shape, retina_data.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = where.test_what(data_fullfield, pred_accuracy_colliculus, label)\n",
    "print('mean accuracy =', accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(idx_start, idx_stop):\n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "    \n",
    "    ax = fig.add_subplot(151)\n",
    "    ax = where.display.show(ax, data_fullfield[idx, :, :])\n",
    "    ax.set_title(f\"i={positions[idx]['i_offset']}, j={positions[idx]['j_offset']}\")\n",
    "    \n",
    "    ax = fig.add_subplot(152)\n",
    "    data_retina = where.retina.retina(data_fullfield[idx, :, :])\n",
    "    ax = where.retina.show(ax, where.retina.retina_invert(data_retina))\n",
    "    ax.set_title(f\"idx={idx}\")\n",
    "              \n",
    "    ax = fig.add_subplot(153, projection='polar')\n",
    "    ax.pcolor(where.retina.theta, where.retina.log_r, accuracy_colliculus[idx, :].numpy().reshape((args.N_azimuth, args.N_eccentricity)), cmap=plt.plasma())\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_title(\"True\")\n",
    "\n",
    "    ax = fig.add_subplot(154, projection='polar')\n",
    "    ax.pcolor(where.retina.theta, where.retina.log_r, pred_accuracy_colliculus[idx, :].reshape((args.N_azimuth, args.N_eccentricity)), cmap=plt.plasma())\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_title(\"Predicted\")\n",
    "\n",
    "    i_pred, j_pred = where.index_prediction(pred_accuracy_colliculus[idx, :])\n",
    "    ax = fig.add_subplot(155)                 \n",
    "    ax = where.display.show(ax, where.extract(data_fullfield[idx, :, :], i_pred, j_pred), do_cross=False)\n",
    "    result = '' if pred[idx]==label[idx].numpy() else 'FALSE'\n",
    "    ax.set_title(f\"pred={pred[idx]} acc={proba[idx,pred[idx]]:.2f} {result}\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = where.test()\n",
    "print('Average accuracy on the test set = ', correct.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy as a function of eccentricity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eccentricities(N_eccentricities=9, ecc_max=.8, do_control=False):\n",
    "    import torch\n",
    "    from torch.autograd import Variable\n",
    "\n",
    "    #eccentricities = args.N_pic / 2 * ecc_max * (1/args.rho)**(args.N_eccentricity - np.arange(N_eccentricities))\n",
    "    eccentricities = np.linspace(where.args.offset_max, 0, N_eccentricities, endpoint=False)\n",
    "    batch_size = where.args.test_batch_size # data.shape[0]\n",
    "    from retina import get_data_loader\n",
    "    loader_test = get_data_loader(batch_size=1, train=False, \n",
    "                                  mean=where.args.mean, std=where.args.std, seed=where.args.seed+10)\n",
    "        \n",
    "    accuracy_mean, accuracy_std = [], []\n",
    "    for eccentricity in eccentricities:\n",
    "\n",
    "        retina_data = np.zeros((batch_size, where.retina.vsize))\n",
    "        labels = np.zeros((batch_size))\n",
    "        data_fullfield = np.zeros((batch_size, where.args.N_pic, where.args.N_pic))        \n",
    "        accuracy_colliculus = np.zeros((batch_size, where.args.N_azimuth * where.args.N_eccentricity))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            data, label = next(iter(loader_test))\n",
    "            data_fullfield[i, :, :], i_offset, j_offset = where.display.draw(data[0, 0, :, :].numpy(), \n",
    "                                                                   radius=eccentricity)\n",
    "            positions.append(dict(i_offset=i_offset, j_offset=j_offset))\n",
    "            retina_data[i, :]  =  where.retina.retina(data_fullfield[i, :, :])\n",
    "            labels[i] = label\n",
    "        labels =  Variable(torch.FloatTensor(labels))\n",
    "        retina_data =  Variable(torch.FloatTensor(retina_data))\n",
    "        pred_accuracy_colliculus = where.pred_accuracy(retina_data)\n",
    "        \n",
    "        accuracy_ = where.test_what(data_fullfield, pred_accuracy_colliculus, labels.squeeze(), do_control=do_control)\n",
    "        accuracy_mean.append(accuracy_.mean())\n",
    "        accuracy_std.append(accuracy_.std()) # TODO fit with beta distribution\n",
    "        \n",
    "    return eccentricities, np.array(accuracy_mean), np.array(accuracy_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_eccentricities = 9\n",
    "eccentricities, accuracy_mean, accuracy_std = test_eccentricities(N_eccentricities)\n",
    "print('eccentricities=', eccentricities, ', accuracy_data=', accuracy_mean, ' +/- ', accuracy_std)\n",
    "\n",
    "eccentricities, ctl_accuracy_mean, ctl_accuracy_std = test_eccentricities(N_eccentricities, do_control=True)\n",
    "print('eccentricities=', eccentricities, ', ctl_accuracy_data=', ctl_accuracy_mean, ' +/- ', ctl_accuracy_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (figwidth, figwidth/1.618))\n",
    "ax_D = fig.add_subplot(1, 1, 1)\n",
    "width = .8*np.abs(np.gradient(eccentricities)[0])\n",
    "ax_D.bar(eccentricities, accuracy_mean, width=width, alpha = .5, label = 'One saccade')#yerr=accuracy_std, \n",
    "# TODO what instead? ax_D.bar(eccentricities, accuracy_data, alpha = .5, label = 'No saccade') #accuracy_map[27,27:55])\n",
    "ax_D.bar(eccentricities, ctl_accuracy_mean, width=width, color='orange', alpha = 1., label = 'No saccade')\n",
    "ax_D.plot([eccentricities.min()-width/2, eccentricities.max()+width/2], [0.1]*2, ':', c='k', label = 'Baseline')\n",
    "plt.legend(loc='best')\n",
    "#ax_D.set_title('Class accuracy', fontsize = 14)\n",
    "ax_D.set_xlabel('Target eccentricity (pixels)', fontsize = 12)\n",
    "ax_D.set_xticks(eccentricities)\n",
    "ax_D.set_xticklabels(['%.1f' % d for  d in eccentricities])\n",
    "ax_D.set_ylim([0,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (figwidth, figwidth/2.5))#1.618))\n",
    "ax_A = plt.subplot(1, 4, 1) \n",
    "data_retina = where.retina.retina(data_fullfield[idx, :, :])\n",
    "ax_A = where.retina.show(ax_A, where.retina.retina_invert(data_retina))\n",
    "    \n",
    "ax_B = plt.subplot(2, 4, 2, projection='polar', autoscale_on=False)\n",
    "ax_B.pcolor(where.retina.theta, where.retina.log_r, accuracy_colliculus[idx, :].numpy().reshape((args.N_azimuth, args.N_eccentricity)))\n",
    "ax_B.grid('off')\n",
    "plt.title('True', fontsize = 14)\n",
    "ax_B.set_yticklabels([])\n",
    "ax_B.set_xticklabels([])\n",
    "\n",
    "ax_Bb = plt.subplot(2, 4, 6, projection='polar')\n",
    "ax_Bb.pcolor(where.retina.theta, where.retina.log_r, pred_accuracy_colliculus[idx, :].reshape((args.N_azimuth, args.N_eccentricity)))\n",
    "ax_Bb.set_title('Predicted', fontsize = 14)\n",
    "ax_Bb.set_yticklabels([])\n",
    "ax_Bb.set_xticklabels([])\n",
    "\n",
    "ax_C = plt.subplot(1, 4, 3)\n",
    "data_fullfield_ = where.display.place_object(data[idx, 0, :, :].numpy(), 0, 0)\n",
    "input_vector  =  where.retina.retina(data_fullfield_)\n",
    "ax_C = where.retina.show(ax_C, where.retina.retina_invert(input_vector))\n",
    "i_pred, j_pred = where.index_prediction(pred_accuracy_colliculus[idx, :])\n",
    "#ax_A.arrow(64.5, 64.5, j_pred, i_pred, width=.3, color='r', head_width=4., length_includes_head=True, edgecolor='k')\n",
    "ax_C.arrow(args.N_pic//2+j_pred+14, args.N_pic//2+i_pred+14, -j_pred, -i_pred, width=.3, color='r', head_width=4., length_includes_head=True, edgecolor='k')\n",
    "ax_C.arrow(args.N_pic//2+j_pred+14, args.N_pic//2+i_pred-14, -j_pred, -i_pred, width=.3, color='r', head_width=4., length_includes_head=True, edgecolor='k')\n",
    "\n",
    "ax_D = plt.subplot(1, 4, 4)\n",
    "width = .8*np.abs(np.gradient(eccentricities)[0])\n",
    "ax_D.bar(eccentricities, accuracy_mean, width=width, color='blue', alpha = .5, label = 'One saccade')\n",
    "ax_D.bar(eccentricities, ctl_accuracy_mean, width=width, color='orange', alpha = 1., label = 'No saccade')\n",
    "ax_D.plot([eccentricities.min()-width/2, eccentricities.max()+width/2], [0.1]*2, ':', c='k', label = 'Baseline')\n",
    "#ax_D.plot([eccentricities.min()-width/2, eccentricities.max()+width/2], [acc_max]*2, ':', c='k', label = 'Max')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "ax_D.set_xlabel('Target eccentricity (pixels)', fontsize = 12)\n",
    "ax_D.set_xticks(eccentricities)\n",
    "ax_D.set_xticklabels(['%.1f' % d for  d in eccentricities])\n",
    "ax_D.set_ylim([0,1])\n",
    "\n",
    "for ax, text in [[ax_A, 'DIS'], [ax_C, 'SAC']]:\n",
    "    ax.text(4, 15, text, fontsize=24,\n",
    "          bbox={'facecolor':'white','alpha':1,'edgecolor':'none','pad':1},\n",
    "          ha='left', va='center') \n",
    "\n",
    "offset = -.015\n",
    "for ax, text, x_offset, y_offset in [[ax_A, 'A', offset, 1.15], [ax_B, 'B', -.25, 1.225], [ax_C, 'C', offset, 1.15], [ax_D, 'D', offset, 1.15]]:\n",
    "    ax.text(x_offset, y_offset, '(' + text + ')', fontsize=24,\n",
    "              bbox={'facecolor':'white','alpha':1,'edgecolor':'none','pad':1},\n",
    "              ha='left', va='center', transform=ax.transAxes) \n",
    "\n",
    "# pos : [left, bottom, width, height] =    The new position of the in `.Figure` coordinates.    \n",
    "plt.tight_layout()\n",
    "ax_A.set_position([0.025, 0.1, .3, .45])\n",
    "ax_B.set_position( [0.24, 0.375, .2, 0.2])\n",
    "ax_Bb.set_position([0.24, 0.1, .2, 0.2])\n",
    "ax_C.set_position([0.35, .1, .3, .45])\n",
    "ax_D.set_position([0.65, .1, .3, .45])\n",
    "fig.savefig(figname + '.pdf', bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T14:43:27.591507Z",
     "start_time": "2018-10-08T14:43:24.722160Z"
    }
   },
   "outputs": [],
   "source": [
    "!convert  -density {dpi_export} {figname}.pdf {figname}.jpg\n",
    "!convert  -density {dpi_export} {figname}.pdf {figname}.png\n",
    "#!convert  -density {dpi_export} -resize 5400  -units pixelsperinch -flatten  -compress lzw  -depth 8 {fname}.pdf {fname}.tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T10:36:00.981927Z",
     "start_time": "2018-07-03T10:36:00.949864Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('{figname}.png'.format(figname=figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T10:36:00.939193Z",
     "start_time": "2018-07-03T10:36:00.766218Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls  -l {figname}*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
