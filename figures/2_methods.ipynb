{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a classifier\n",
    "\n",
    "On commence par la fonction de base apprise de la librairie torch, cf https://raw.githubusercontent.com/pytorch/examples/master/mnist/main.py :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%run what.py --epochs 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from what import Net\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On apprend une matrice de poids qui est fixée dans la suite et que nous allons utiliser pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 laurentperrinet  staff  1725644 Mar  7 10:35 ../data/mnist_cnn.pt\n"
     ]
    }
   ],
   "source": [
    "%ls -ltr ../data/mnist_cnn.pt\n",
    "#%rm -f ../data/mnist_cnn.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:35.317080Z",
     "start_time": "2018-02-16T19:37:35.297625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../data/mnist_cnn.pt\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "if os.path.isfile(model_path):\n",
    "    print('Loading')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    print('Learning')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    %run what.py --epochs 10 --save-model\n",
    "    print('Done in ', time.time() - t0, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier in a standalone class\n",
    "\n",
    "Maintenant qu'on a appris les points qui permet une classification d'à peu près 98 % on va utiliser le modèle fead-forward pour faire la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        log_interval = 100\n",
    "args = Args()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.236066Z",
     "start_time": "2018-02-16T19:37:37.902628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from retina import get_data_loader\n",
    "\n",
    "test_loader = get_data_loader(batch_size=1000, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.236066Z",
     "start_time": "2018-02-16T19:37:37.902628Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0322, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "accuracy= 0.9894\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "from what import test\n",
    "accuracy = test(args, model, torch.device(\"cpu\"), test_loader)\n",
    "print('accuracy=', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shifting the input images\n",
    "\n",
    "\n",
    "Je vais maintenant générer des données en utilisant les données originales de MNIST translatées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.247845Z",
     "start_time": "2018-02-16T19:37:42.240563Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "i_shift, j_shift = 12, 17\n",
    "N_pix = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.835649Z",
     "start_time": "2018-02-16T19:37:42.250490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11a5a85c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for data, target in test_loader:\n",
    "    break\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.225992Z",
     "start_time": "2018-02-16T19:37:42.839053Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mul(): argument 'other' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-257c5e457514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_translate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_pix\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_pix\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_translate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_translate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN_pix\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN_pix\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN_pix\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN_pix\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: mul(): argument 'other' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 2, N_pix*3 - 2))\n",
    "print(data_translate.shape)\n",
    "data_translate[:, :, (N_pix-i_shift):(2*N_pix-i_shift), (N_pix-j_shift):(2*N_pix-j_shift)] = data\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_translate[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.539188Z",
     "start_time": "2018-02-16T19:37:43.228276Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.550455Z",
     "start_time": "2018-02-16T19:37:43.543831Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.arange(-N_pix+1, N_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.960102Z",
     "start_time": "2018-02-16T19:37:43.552774Z"
    }
   },
   "outputs": [],
   "source": [
    "def shift_data(data, i_shift, j_shift):\n",
    "    N_pix = data.shape[-1]\n",
    "    assert(N_pix == data.shape[-2])\n",
    "    import numpy as np\n",
    "    data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 1, N_pix*3 - 1))\n",
    "    data_translate[:, :, (N_pix+i_shift):(2*N_pix+i_shift), (N_pix+j_shift):(2*N_pix+j_shift)] = data\n",
    "    data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "    return data_cropped\n",
    "\n",
    "data_cropped = shift_data(data, i_shift = 12, j_shift = -12)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier on the shifted data\n",
    "\n",
    "On peut maintenant tester le classifieur sur les images Translatées en calculant la valeur de classification en  fonction de l'erreur de localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:44.165284Z",
     "start_time": "2018-02-16T19:37:43.964216Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_shift(test_loader, i_shift, j_shift, verbose=0):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data_cropped = shift_data(data, i_shift=i_shift, j_shift=j_shift)        \n",
    "        data_cropped = torch.FloatTensor(data_cropped) #transforms.ToTensor()(data_cropped)\n",
    "        data_cropped, target = Variable(data_cropped, volatile=True), Variable(target)\n",
    "        output = model(data_cropped)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if verbose: print('\\nTest set: at ({}, {}), the  average loss is {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        i_shift, j_shift, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "path = \"../data/MNIST_accuracy.npy\"\n",
    "\n",
    "import os\n",
    "if os.path.isfile(path):\n",
    "    print('Loading accuracy')\n",
    "    accuracy = np.load(path)\n",
    "else:\n",
    "    print('Computing accuracy')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    accuracy = np.zeros((2*N_pix-1, 2*N_pix-1))\n",
    "    from tqdm import tqdm\n",
    "    N_step = 1\n",
    "\n",
    "    with tqdm(total=(2*N_pix-1)**2/N_step**2) as pbar:\n",
    "        for i_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "            for j_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "                accuracy[i_shift+N_pix-1, j_shift+N_pix-1] = test_shift(test_loader, i_shift, j_shift)\n",
    "                pbar.update()\n",
    "    np.save(path, accuracy)\n",
    "    print('Done in ', time.time() - t0, 'seconds')\n",
    "    \n",
    "print('accuracy=', accuracy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'avoue que c'est un peu bourrin de calculer la classification sur les 128 × 128 pixels pour 1000 batch multiplié par 10 type d'entrées.... Mais bon on doit faire ça seulement une fois :-) (et sur CPU une classif = environ 300µs ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:53:37.955718Z",
     "start_time": "2018-02-16T19:53:37.258057Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 10.725))\n",
    "cmap = ax.pcolor(np.arange(-N_pix+1, N_pix+1), np.arange(-N_pix+1, N_pix+1), accuracy)\n",
    "ax.axis('equal')\n",
    "fig.colorbar(cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction de performance du classifieur  est calculée indépendamment de la forme spécifique du chiffre entre 0 et 9. Elle donne donc la carte de performance qu'on attend Au niveau de la classification/ On va pouvoir maintenant l'utiliser ceomm label pour apprendre de façon supervisée la correspondance entre la carte log-polaire obtenue depuis l'image brute et cette carte de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Retinotopic mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orientation invariant power encoding (colliculus??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus = (retina_transform**2).sum(axis=(0, 3))\n",
    "#colliculus = colliculus**.5\n",
    "colliculus /= colliculus.sum(axis=-1)[:, :, None]\n",
    "print(colliculus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus_vector = colliculus.reshape((N_azimuth*N_eccentricity, N_X*N_Y))\n",
    "print(colliculus_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus_inverse = np.linalg.pinv(colliculus_vector)\n",
    "print(colliculus_inverse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = (retina**2).sum(axis=(0,3)) \n",
    "energy /= energy.sum(axis=-1)[:, :, None]\n",
    "energy_vector = energy.reshape((N_azimuth*N_eccentricity, N_X*N_Y))\n",
    "energy_plus = np.linalg.pinv(energy_vector)\n",
    "FIG_WIDTH = 5 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(FIG_WIDTH, FIG_WIDTH))\n",
    "for i_orient in range(N_azimuth):\n",
    "    for i_scale in range(N_eccentricity):\n",
    "        env = np.sqrt(energy[i_orient, i_scale, :]**2.5).reshape((N_X, N_Y))\n",
    "        ax.contour(energy[i_orient, i_scale, :].reshape((N_X, N_Y)), levels=[env.max()/2], lw=1,\n",
    "                  colors=[plt.cm.rainbow(i_scale * 1.5/N_azimuth)])\n",
    "fig.suptitle('Tiling of visual space using energy', y=1.02)\n",
    "ax.set_xlabel(r'$Y$')\n",
    "ax.set_ylabel(r'$X$')\n",
    "ax.axis('equal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
