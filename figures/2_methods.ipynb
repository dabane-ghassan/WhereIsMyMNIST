{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a classifier\n",
    "\n",
    "On commence par la fonction de base apprise de la librairie torch, cf https://raw.githubusercontent.com/pytorch/examples/master/mnist/main.py :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run what.py --epochs 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable         Type        Data/Info\n",
      "--------------------------------------\n",
      "F                module      <module 'torch.nn.functio<...>/torch/nn/functional.py'>\n",
      "Net              type        <class '__main__.Net'>\n",
      "argparse         module      <module 'argparse' from '<...>b/python3.7/argparse.py'>\n",
      "datasets         module      <module 'torchvision.data<...>on/datasets/__init__.py'>\n",
      "main             function    <function main at 0x1114cef28>\n",
      "nn               module      <module 'torch.nn' from '<...>es/torch/nn/__init__.py'>\n",
      "optim            module      <module 'torch.optim' fro<...>torch/optim/__init__.py'>\n",
      "print_function   _Feature    _Feature((2, 6, 0, 'alpha<...>0, 0, 'alpha', 0), 65536)\n",
      "test             function    <function test at 0x1114ceea0>\n",
      "torch            module      <module 'torch' from '/us<...>kages/torch/__init__.py'>\n",
      "train            function    <function train at 0x10bb02488>\n",
      "transforms       module      <module 'torchvision.tran<...>/transforms/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from what import Net\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On apprend une matrice de poids qui est fixée dans la suite et que nous allons utiliser pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 laurentperrinet  staff  1725644 Mar  7 10:35 ../data/mnist_cnn.pt\n"
     ]
    }
   ],
   "source": [
    "%ls -ltr ../data/mnist_cnn.pt\n",
    "#%rm -f ../data/mnist_cnn.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:35.317080Z",
     "start_time": "2018-02-16T19:37:35.297625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../data/mnist_cnn.pt\"\n",
    "\n",
    "import os\n",
    "if os.path.isfile(model_path):\n",
    "    print('Loading')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    print('Learning')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    %run what.py --epochs 10 --save-model\n",
    "    print('Done in ', time.time() - t0, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier in a standalone class\n",
    "\n",
    "Maintenant qu'on a appris les points qui permet une classification d'à peu près 98 % on va utiliser le modèle fead-forward pour faire la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        log_interval = 100\n",
    "args = Args()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.236066Z",
     "start_time": "2018-02-16T19:37:37.902628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from retina import get_data_loader\n",
    "\n",
    "test_loader = get_data_loader(batch_size=1000, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.236066Z",
     "start_time": "2018-02-16T19:37:37.902628Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0322, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "accuracy= 0.9894\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "from what import test\n",
    "accuracy = test(args, model, torch.device(\"cpu\"), test_loader)\n",
    "print('accuracy=', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shifting the input images\n",
    "\n",
    "\n",
    "Je vais maintenant générer des données en utilisant les données originales de MNIST translatées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.247845Z",
     "start_time": "2018-02-16T19:37:42.240563Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "i_shift, j_shift = 12, 17\n",
    "N_pix = 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.835649Z",
     "start_time": "2018-02-16T19:37:42.250490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11e0f9f98>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for data, target in test_loader:\n",
    "    break\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.225992Z",
     "start_time": "2018-02-16T19:37:42.839053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-257c5e457514>\", line 2, in <module>\n",
      "    data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 2, N_pix*3 - 2))\n",
      "TypeError: mul(): argument 'other' (position 1) must be Tensor, not numpy.ndarray\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "AttributeError: module has no attribute '__name__'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mul(): argument 'other' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 2, N_pix*3 - 2))\n",
    "print(data_translate.shape)\n",
    "data_translate[:, :, (N_pix-i_shift):(2*N_pix-i_shift), (N_pix-j_shift):(2*N_pix-j_shift)] = data\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_translate[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.539188Z",
     "start_time": "2018-02-16T19:37:43.228276Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.550455Z",
     "start_time": "2018-02-16T19:37:43.543831Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.arange(-N_pix+1, N_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.960102Z",
     "start_time": "2018-02-16T19:37:43.552774Z"
    }
   },
   "outputs": [],
   "source": [
    "def shift_data(data, i_shift, j_shift):\n",
    "    N_pix = data.shape[-1]\n",
    "    assert(N_pix == data.shape[-2])\n",
    "    import numpy as np\n",
    "    data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 1, N_pix*3 - 1))\n",
    "    data_translate[:, :, (N_pix+i_shift):(2*N_pix+i_shift), (N_pix+j_shift):(2*N_pix+j_shift)] = data\n",
    "    data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "    return data_cropped\n",
    "\n",
    "data_cropped = shift_data(data, i_shift = 12, j_shift = -12)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier on the shifted data\n",
    "\n",
    "On peut maintenant tester le classifieur sur les images Translatées en calculant la valeur de classification en  fonction de l'erreur de localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:44.165284Z",
     "start_time": "2018-02-16T19:37:43.964216Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_shift(test_loader, i_shift, j_shift, verbose=0):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data_cropped = shift_data(data, i_shift=i_shift, j_shift=j_shift)        \n",
    "        data_cropped = torch.FloatTensor(data_cropped) #transforms.ToTensor()(data_cropped)\n",
    "        data_cropped, target = Variable(data_cropped, volatile=True), Variable(target)\n",
    "        output = model(data_cropped)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if verbose: print('\\nTest set: at ({}, {}), the  average loss is {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        i_shift, j_shift, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "path = \"MNIST_accuracy.npy\"\n",
    "\n",
    "import os\n",
    "if os.path.isfile(path):\n",
    "    print('Loading accuracy')\n",
    "    accuracy = np.load(path)\n",
    "else:\n",
    "    print('Computing accuracy')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    accuracy = np.zeros((2*N_pix-1, 2*N_pix-1))\n",
    "    from tqdm import tqdm\n",
    "    N_step = 1\n",
    "\n",
    "    with tqdm(total=(2*N_pix-1)**2/N_step**2) as pbar:\n",
    "        for i_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "            for j_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "                accuracy[i_shift+N_pix-1, j_shift+N_pix-1] = test_shift(test_loader, i_shift, j_shift)\n",
    "                pbar.update()\n",
    "    np.save(path, accuracy)\n",
    "    print('Done in ', time.time() - t0, 'seconds')\n",
    "    \n",
    "print('accuracy=', accuracy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'avoue que c'est un peu bourrin de calculer la classification sur les 128 × 128 pixels pour 1000 batch multiplié par 10 type d'entrées.... Mais bon on doit faire ça seulement une fois :-) (et sur CPU une classif = environ 300µs ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:53:37.955718Z",
     "start_time": "2018-02-16T19:53:37.258057Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 10.725))\n",
    "cmap = ax.pcolor(np.arange(-N_pix+1, N_pix+1), np.arange(-N_pix+1, N_pix+1), accuracy)\n",
    "ax.axis('equal')\n",
    "fig.colorbar(cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction de performance du classifieur  est calculée indépendamment de la forme spécifique du chiffre entre 0 et 9. Elle donne donc la carte de performance qu'on attend Au niveau de la classification/ On va pouvoir maintenant l'utiliser ceomm label pour apprendre de façon supervisée la correspondance entre la carte log-polaire obtenue depuis l'image brute et cette carte de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Retinotopic mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orientation invariant power encoding (colliculus??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus = (retina_transform**2).sum(axis=(0, 3))\n",
    "#colliculus = colliculus**.5\n",
    "colliculus /= colliculus.sum(axis=-1)[:, :, None]\n",
    "print(colliculus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus_vector = colliculus.reshape((N_azimuth*N_eccentricity, N_X*N_Y))\n",
    "print(colliculus_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus_inverse = np.linalg.pinv(colliculus_vector)\n",
    "print(colliculus_inverse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = (retina**2).sum(axis=(0,3)) \n",
    "energy /= energy.sum(axis=-1)[:, :, None]\n",
    "energy_vector = energy.reshape((N_azimuth*N_eccentricity, N_X*N_Y))\n",
    "energy_plus = np.linalg.pinv(energy_vector)\n",
    "FIG_WIDTH = 5 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(FIG_WIDTH, FIG_WIDTH))\n",
    "for i_orient in range(N_azimuth):\n",
    "    for i_scale in range(N_eccentricity):\n",
    "        env = np.sqrt(energy[i_orient, i_scale, :]**2.5).reshape((N_X, N_Y))\n",
    "        ax.contour(energy[i_orient, i_scale, :].reshape((N_X, N_Y)), levels=[env.max()/2], lw=1,\n",
    "                  colors=[plt.cm.rainbow(i_scale * 1.5/N_azimuth)])\n",
    "fig.suptitle('Tiling of visual space using energy', y=1.02)\n",
    "ax.set_xlabel(r'$Y$')\n",
    "ax.set_ylabel(r'$X$')\n",
    "ax.axis('equal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
