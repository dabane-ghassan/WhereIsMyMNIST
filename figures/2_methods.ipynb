{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a classifier\n",
    "\n",
    "On commence par la fonction de base apprise de la librairie torch, cf https://raw.githubusercontent.com/pytorch/examples/master/mnist/main.py :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%run what.py --epochs 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from what import Net\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On apprend une matrice de poids qui est fixée dans la suite et que nous allons utiliser pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 laurentperrinet  staff  1725644 Mar  7 10:35 ../data/mnist_cnn.pt\n"
     ]
    }
   ],
   "source": [
    "%ls -ltr ../data/mnist_cnn.pt\n",
    "#%rm -f ../data/mnist_cnn.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:35.317080Z",
     "start_time": "2018-02-16T19:37:35.297625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../data/mnist_cnn.pt\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "if os.path.isfile(model_path):\n",
    "    print('Loading')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    print('Learning')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    %run what.py --epochs 10 --save-model\n",
    "    print('Done in ', time.time() - t0, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier in a standalone class\n",
    "\n",
    "Maintenant qu'on a appris les points qui permet une classification d'à peu près 98 % on va utiliser le modèle fead-forward pour faire la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        log_interval = 100\n",
    "args = Args()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.236066Z",
     "start_time": "2018-02-16T19:37:37.902628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from retina import get_data_loader\n",
    "\n",
    "test_loader = get_data_loader(batch_size=1000, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.236066Z",
     "start_time": "2018-02-16T19:37:37.902628Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0322, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "accuracy= 0.9894\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "from what import test\n",
    "accuracy = test(args, model, torch.device(\"cpu\"), test_loader)\n",
    "print('accuracy=', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shifting the input images\n",
    "\n",
    "\n",
    "Je vais maintenant générer des données en utilisant les données originales de MNIST translatées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.247845Z",
     "start_time": "2018-02-16T19:37:42.240563Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "i_shift, j_shift = 12, 17\n",
    "N_pix = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.835649Z",
     "start_time": "2018-02-16T19:37:42.250490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11c568668>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for data, target in test_loader:\n",
    "    break\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.225992Z",
     "start_time": "2018-02-16T19:37:42.839053Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mul(): argument 'other' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-257c5e457514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_translate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_pix\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_pix\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_translate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_translate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN_pix\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN_pix\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN_pix\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN_pix\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: mul(): argument 'other' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 2, N_pix*3 - 2))\n",
    "print(data_translate.shape)\n",
    "data_translate[:, :, (N_pix-i_shift):(2*N_pix-i_shift), (N_pix-j_shift):(2*N_pix-j_shift)] = data\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_translate[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.539188Z",
     "start_time": "2018-02-16T19:37:43.228276Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.550455Z",
     "start_time": "2018-02-16T19:37:43.543831Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.arange(-N_pix+1, N_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.960102Z",
     "start_time": "2018-02-16T19:37:43.552774Z"
    }
   },
   "outputs": [],
   "source": [
    "def shift_data(data, i_shift, j_shift):\n",
    "    N_pix = data.shape[-1]\n",
    "    assert(N_pix == data.shape[-2])\n",
    "    import numpy as np\n",
    "    data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 1, N_pix*3 - 1))\n",
    "    data_translate[:, :, (N_pix+i_shift):(2*N_pix+i_shift), (N_pix+j_shift):(2*N_pix+j_shift)] = data\n",
    "    data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "    return data_cropped\n",
    "\n",
    "data_cropped = shift_data(data, i_shift = 12, j_shift = -12)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier on the shifted data\n",
    "\n",
    "On peut maintenant tester le classifieur sur les images Translatées en calculant la valeur de classification en  fonction de l'erreur de localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:44.165284Z",
     "start_time": "2018-02-16T19:37:43.964216Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_shift(test_loader, i_shift, j_shift, verbose=0):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data_cropped = shift_data(data, i_shift=i_shift, j_shift=j_shift)        \n",
    "        data_cropped = torch.FloatTensor(data_cropped) #transforms.ToTensor()(data_cropped)\n",
    "        data_cropped, target = Variable(data_cropped, volatile=True), Variable(target)\n",
    "        output = model(data_cropped)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if verbose: print('\\nTest set: at ({}, {}), the  average loss is {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        i_shift, j_shift, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "path = \"../data/MNIST_accuracy.npy\"\n",
    "\n",
    "import os\n",
    "if os.path.isfile(path):\n",
    "    print('Loading accuracy')\n",
    "    accuracy = np.load(path)\n",
    "else:\n",
    "    print('Computing accuracy')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    accuracy = np.zeros((2*N_pix-1, 2*N_pix-1))\n",
    "    from tqdm import tqdm\n",
    "    N_step = 1\n",
    "\n",
    "    with tqdm(total=(2*N_pix-1)**2/N_step**2) as pbar:\n",
    "        for i_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "            for j_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "                accuracy[i_shift+N_pix-1, j_shift+N_pix-1] = test_shift(test_loader, i_shift, j_shift)\n",
    "                pbar.update()\n",
    "    np.save(path, accuracy)\n",
    "    print('Done in ', time.time() - t0, 'seconds')\n",
    "    \n",
    "print('accuracy=', accuracy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'avoue que c'est un peu bourrin de calculer la classification sur les 128 × 128 pixels pour 1000 batch multiplié par 10 type d'entrées.... Mais bon on doit faire ça seulement une fois :-) (et sur CPU une classif = environ 300µs ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:53:37.955718Z",
     "start_time": "2018-02-16T19:53:37.258057Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 10.725))\n",
    "cmap = ax.pcolor(np.arange(-N_pix+1, N_pix+1), np.arange(-N_pix+1, N_pix+1), accuracy)\n",
    "ax.axis('equal')\n",
    "fig.colorbar(cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction de performance du classifieur  est calculée indépendamment de la forme spécifique du chiffre entre 0 et 9. Elle donne donc la carte de performance qu'on attend Au niveau de la classification/ On va pouvoir maintenant l'utiliser ceomm label pour apprendre de façon supervisée la correspondance entre la carte log-polaire obtenue depuis l'image brute et cette carte de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Retinotopic mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orientation invariant power encoding (colliculus??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus = (retina_transform**2).sum(axis=(0, 3))\n",
    "#colliculus = colliculus**.5\n",
    "colliculus /= colliculus.sum(axis=-1)[:, :, None]\n",
    "print(colliculus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus_vector = colliculus.reshape((N_azimuth*N_eccentricity, N_X*N_Y))\n",
    "print(colliculus_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus_inverse = np.linalg.pinv(colliculus_vector)\n",
    "print(colliculus_inverse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = (retina**2).sum(axis=(0,3)) \n",
    "energy /= energy.sum(axis=-1)[:, :, None]\n",
    "energy_vector = energy.reshape((N_azimuth*N_eccentricity, N_X*N_Y))\n",
    "energy_plus = np.linalg.pinv(energy_vector)\n",
    "FIG_WIDTH = 5 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(FIG_WIDTH, FIG_WIDTH))\n",
    "for i_orient in range(N_azimuth):\n",
    "    for i_scale in range(N_eccentricity):\n",
    "        env = np.sqrt(energy[i_orient, i_scale, :]**2.5).reshape((N_X, N_Y))\n",
    "        ax.contour(energy[i_orient, i_scale, :].reshape((N_X, N_Y)), levels=[env.max()/2], lw=1,\n",
    "                  colors=[plt.cm.rainbow(i_scale * 1.5/N_azimuth)])\n",
    "fig.suptitle('Tiling of visual space using energy', y=1.02)\n",
    "ax.set_xlabel(r'$Y$')\n",
    "ax.set_ylabel(r'$X$')\n",
    "ax.axis('equal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
