%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Laurent Perrinet at 2019-03-08 16:38:28 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{Dauce18,
	Abstract = {What motivates the action in the absence of a definite reward? Taking the case of visuomotor control, we consider a minimal control problem that is how select the next saccade, in a sequence of discrete eye movements, when the final objective is to better interpret the current visual scene. The visual scene is modeled here as a partially-observed environment, with a generative model explaining how the visual data is shaped by action. This allows to interpret different action selection metrics proposed in the literature, including the Salience, the Infomax and the Variational Free Energy, under a single information theoretic construct, namely the view-based Information Gain. Pursuing this analytic track, two original action selection metrics named the Information Gain Lower Bound (IGLB) and the Information Gain Upper Bound (IGUB) are then proposed. Showing either a conservative or an optimistic bias regarding the Information Gain, they strongly simplify its calculation. An original fovea-based visual scene decoding setup is then proposed, with numerical experiments highlighting different facets of artificial fovea-based vision. A first and principal result is that state-of-the-art recognition rates are obtained with fovea-based saccadic exploration, using less than 10\% of the original image's data. Those satisfactory results illustrate the advantage of mixing predictive control with accurate state-of-the-art predictors, namely a deep neural network. A second result is the sub-optimality of some classical action-selection metrics widely used in literature, that is not manifest with finely-tuned inference models, but becomes patent when coarse or faulty models are used. Last, a computationally-effective predictive model is developed using the IGLB objective, with pre-processed visual scan-path read-out from memory, bypassing computationally-demanding predictive calculations. This last simplified setting is shown effective in our case, showing both a competing accuracy and a good robustness to model flaws.},
	Author = {Dauc\'e, Emmanuel},
	Date-Added = {2019-03-08 16:12:29 +0100},
	Date-Modified = {2019-03-08 16:37:30 +0100},
	Doi = {10/gfrhgj},
	File = {/Users/laurentperrinet/Zotero/storage/QUYVDZTT/Dauc{\'e} - 2018 - Active Fovea-Based Vision Through Computationally-.pdf;/Users/laurentperrinet/Zotero/storage/VS9U4UNG/Dauc{\'e} - 2018 - Active Fovea-Based Vision Through Computationally-.pdf},
	Issn = {1662-5218},
	Journal = {Frontiers in Neurorobotics},
	Keywords = {active inference,active vision,Convolutional Neural Networks (CNN),End-effector control,Foveated vision,information gain,intrinsic motivation,Saccadic eye movements,visual scene interpretation},
	Title = {Active Fovea-Based Vision Through Computationally-Effective Model-Based Prediction},
	Volume = {12},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10/gfrhgj}}

@article{Sandler18,
	Abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	Archiveprefix = {arXiv},
	Author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	Date-Added = {2019-03-08 14:02:48 +0100},
	Date-Modified = {2019-03-08 14:02:48 +0100},
	Eprint = {1801.04381},
	Eprinttype = {arxiv},
	File = {/Users/laurentperrinet/Zotero/storage/E5QJ6BJ7/Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf;/Users/laurentperrinet/Zotero/storage/JGFJJX8B/1801.html},
	Journal = {arXiv:1801.04381 [cs]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = jan,
	Note = {00241},
	Primaryclass = {cs},
	Shorttitle = {{{MobileNetV2}}},
	Title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
	Year = {2018}}

@article{Vo12,
	Abstract = {One might assume that familiarity with a scene or previous encounters with objects embedded in a scene would benefit subsequent search for those items. However, in a series of experiments we show that this is not the case: When participants were asked to subsequently search for multiple objects in the same scene, search performance remained essentially unchanged over the course of searches despite increasing scene familiarity. Similarly, looking at target objects during previews, which included letter search, 30 seconds of free viewing, or even 30 seconds of memorizing a scene, also did not benefit search for the same objects later on. However, when the same object was searched for again memory for the previous search was capable of producing very substantial speeding of search despite many different intervening searches. This was especially the case when the previous search engagement had been active rather than supported by a cue. While these search benefits speak to the strength of memory-guided search when the same search target is repeated, the lack of memory guidance during initial object searches\textemdash{}despite previous encounters with the target objects\textemdash{}demonstrates the dominance of guidance by generic scene knowledge in real-world search. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	Author = {V\~o, Melissa L.-H. and Wolfe, Jeremy M.},
	Doi = {10.1037/a0024147},
	File = {/Users/lolo/Zotero/storage/3AUVQLFG/V{\~o} and Wolfe - 2012 - When does repeated search in scenes involve memory.pdf;/Users/lolo/Zotero/storage/DUPQTZQJ/2011-12267-001.html},
	Issn = {1939-1277(Electronic),0096-1523(Print)},
	Journal = {Journal of Experimental Psychology: Human Perception and Performance},
	Keywords = {Contextual Associations,Episodic Memory,Visual Perception,Visual Search},
	Number = {1},
	Pages = {23-41},
	Shorttitle = {When Does Repeated Search in Scenes Involve Memory?},
	Title = {When Does Repeated Search in Scenes Involve Memory? {{Looking}} at versus Looking for Objects in Scenes},
	Volume = {38},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1037/a0024147}}

@article{He15,
	Abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	Archiveprefix = {arXiv},
	Author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	Eprint = {1502.01852},
	Eprinttype = {arxiv},
	File = {/Users/laurentperrinet/Zotero/storage/VHTT3FKQ/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;/Users/laurentperrinet/Zotero/storage/TZD8MFU5/1502.html},
	Journal = {arXiv:1502.01852 [cs]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
	Month = feb,
	Note = {04208},
	Primaryclass = {cs},
	Shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
	Title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
	Year = {2015}}

@article{Kummerer16,
	Abstract = {Here we present DeepGaze II, a model that predicts where people look in images. The model uses the features from the VGG-19 deep neural network trained to identify objects in images. Contrary to other saliency models that use deep features, here we use the VGG features for saliency prediction with no additional fine-tuning (rather, a few readout layers are trained on top of the VGG features to predict saliency). The model is therefore a strong test of transfer learning. After conservative cross-validation, DeepGaze II explains about 87\% of the explainable information gain in the patterns of fixations and achieves top performance in area under the curve metrics on the MIT300 hold-out benchmark. These results corroborate the finding from DeepGaze I (which explained 56\% of the explainable information gain), that deep features trained on object recognition provide a versatile feature space for performing related visual tasks. We explore the factors that contribute to this success and present several informative image examples. A web service is available to compute model predictions at http://deepgaze.bethgelab.org.},
	Archiveprefix = {arXiv},
	Author = {K\"ummerer, Matthias and Wallis, Thomas S. A. and Bethge, Matthias},
	Eprint = {1610.01563},
	Eprinttype = {arxiv},
	File = {/Users/laurentperrinet/Zotero/storage/XBMG4DQN/K{\"u}mmerer et al. - 2016 - DeepGaze II Reading fixations from deep features .pdf;/Users/laurentperrinet/Zotero/storage/CMPHRVXY/1610.html},
	Journal = {arXiv:1610.01563 [cs, q-bio, stat]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition,Statistics - Applications},
	Month = oct,
	Note = {00075},
	Primaryclass = {cs, q-bio, stat},
	Shorttitle = {{{DeepGaze II}}},
	Title = {{{DeepGaze II}}: Reading Fixations from Deep Features Trained on Object Recognition},
	Year = {2016}}

@article{Treisman80,
	Author = {Treisman, Anne M and Gelade, Garry},
	Doi = {10.1016/0010-0285(80)90005},
	Journal = {Cognitive psychology},
	Keywords = {****,attention},
	Note = {11957},
	Number = {1},
	Pages = {97--136},
	Title = {A Feature-Integration Theory of Attention},
	Volume = {12},
	Year = {1980},
	Bdsk-Url-1 = {https://doi.org/10.1016/0010-0285(80)90005}}

@article{Samonds18,
	Abstract = {Saccadic eye movements during free viewing exhibit patterns that reflect a strategy to increase neural responses by matching motor behavior with the statistics of the natural world and with the processing limitations of sensory systems.},
	Author = {Samonds, Jason M. and Geisler, Wilson S. and Priebe, Nicholas J.},
	Copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	Doi = {10/gfgt3k},
	Issn = {1546-1726},
	Journal = {Nature Neuroscience},
	Language = {En},
	Month = nov,
	Note = {00002},
	Number = {11},
	Pages = {1591},
	Title = {Natural Image and Receptive Field Statistics Predict Saccade Sizes},
	Volume = {21},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10/gfgt3k}}

@article{Kirchner06,
	Abstract = {Previous ultra-rapid go/no-go categorization studies with manual responses have demonstrated the remarkable speed and efficiency with which humans process natural scenes. Using a forced-choice saccade task we show here that when two scenes are simultaneously flashed in the left and right hemifields, human participants can reliably make saccades to the side containing an animal in as little as 120 ms. Low level differences between target and distractor images were unable to account for these exceptionally fast responses. The results suggest a very fast and unexpected route linking visual processing in the ventral stream with the programming of saccadic eye movements.},
	Author = {Kirchner, H and Thorpe, Sj},
	Doi = {10.1016/j.visres.2005.10.002},
	Issn = {0042-6989},
	Journal = {Vision Research},
	Keywords = {Visual Pathways,Humans,Psychomotor Performance,Reaction Time,Saccades,Photic Stimulation,Female,Male,Adult,Learning,Learning: physiology,Pattern Recognition,Visual,Visual: physiology,perrinet11sfn,assofield,Ocular,Ocular: physiology,Visual Pathways: physiology,Fixation,Saccades: physiology,Psychomotor Performance: physiology,Electrooculography,Electrooculography: methods,\#nosource},
	Number = {11},
	Pages = {1762--76},
	Pmid = {16289663},
	Title = {Ultra-Rapid Object Detection with Saccadic Eye Movements: Visual Processing Speed Revisited},
	Volume = {46},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.visres.2005.10.002}}

@article{Itti01,
	Abstract = {Five important trends have emerged from recent work on computational models of focal visual attention that emphasize the bottom-up, image-based control of attentional deployment. First, the perceptual saliency of stimuli critically depends on the surrounding context. Second, a unique 'saliency map' that topographically encodes for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Third, inhibition of return, the process by which the currently attended location is prevented from being attended again, is a crucial element of attentional deployment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. And last, scene understanding and object recognition strongly constrain the selection of attended locations. Insights from these five key areas provide a framework for a computational and neurobiological understanding of visual attention.},
	Author = {Itti, Laurent and Koch, Christof},
	Date-Modified = {2019-03-08 16:38:23 +0100},
	Doi = {10/chw2bk},
	Issn = {1471-0048},
	Journal = {Nature Reviews Neuroscience},
	Number = {3},
	Pages = {194-203},
	Title = {Computational Modelling of Visual Attention},
	Volume = {2},
	Year = {2001},
	Bdsk-Url-1 = {https://doi.org/10/chw2bk}}

@article{Najemnik05,
	Abstract = {To perform visual search, humans, like many mammals, encode a large field of view with retinas having variable spatial resolution, and then use high-speed eye movements to direct the highest-resolution region, the fovea, towards potential target locations1,2. Good search performance is essential for survival, and hence mammals may have evolved efficient strategies for selecting fixation locations. Here we address two questions: what are the optimal eye movement strategies for a foveated visual system faced with the problem of finding a target in a cluttered environment, and do humans employ optimal eye movement strategies during a search? We derive the ideal bayesian observer3,4,5,6 for search tasks in which a target is embedded at an unknown location within a random background that has the spectral characteristics of natural scenes7. Our ideal searcher uses precise knowledge about the statistics of the scenes in which the target is embedded, and about its own visual system, to make eye movements that gain the most information about target location. We find that humans achieve nearly optimal search performance, even though humans integrate information poorly across fixations8,9,10. Analysis of the ideal searcher reveals that there is little benefit from perfect integration across fixations\textemdash{}much more important is efficient processing of information on each fixation. Apparently, evolution has exploited this fact to achieve efficient eye movement strategies with minimal neural resources devoted to memory.},
	Author = {Najemnik, Jiri and Geisler, Wilson S.},
	Date-Modified = {2019-03-08 16:36:32 +0100},
	Doi = {10/bcbw2b},
	Journal = {Nature},
	Number = {7031},
	Pages = {387-391},
	Title = {Optimal Eye Movement Strategies in Visual Search},
	Volume = {434},
	Year = {2005},
	Bdsk-Url-1 = {https://doi.org/10/bcbw2b}}

@article{Redmon15,
	Abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	Author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = jun,
	Note = {03448},
	Title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
	Year = {2015}}

@article{Ren17,
	Abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	Author = {Ren, S. and He, K. and Girshick, R. and Sun, J.},
	Doi = {10/gc7rmb},
	Issn = {0162-8828},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Keywords = {Feature extraction,convolutional neural network,Training,attention mechanisms,COCO 2015 competitions,Convolutional codes,deep VGG-16 model,Detectors,faster-R-CNN,full-image convolutional features,GPU,graphics processing units,high-quality region proposals,ILSVRC,MS COCO datasets,neural nets,object detection,Object detection,object detection accuracy,PASCAL VOC 2007,PASCAL VOC 2012,Proposals,real-time object detection,region proposal,region proposal networks,RPN,Search problems},
	Month = jun,
	Note = {08051},
	Number = {6},
	Pages = {1137-1149},
	Shorttitle = {Faster {{R}}-{{CNN}}},
	Title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
	Volume = {39},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10/gc7rmb}}

@article{Mirza18,
	Abstract = {In previous papers, we introduced a normative scheme for scene construction and epistemic (visual) searches based upon active inference. This scheme provides a principled account of how people decide where to look, when categorising a visual scene based on its contents. In this paper, we use active inference to explain the visual searches of normal human subjects; enabling us to answer some key questions about visual foraging and salience attribution. First, we asked whether there is any evidence for 'epistemic foraging'; i.e. exploration that resolves uncertainty about a scene. In brief, we used Bayesian model comparison to compare Markov decision process (MDP) models of scan-paths that did-and did not-contain the epistemic, uncertainty-resolving imperatives for action selection. In the course of this model comparison, we discovered that it was necessary to include non-epistemic (heuristic) policies to explain observed behaviour (e.g., a reading-like strategy that involved scanning from left to right). Despite this use of heuristic policies, model comparison showed that there is substantial evidence for epistemic foraging in the visual exploration of even simple scenes. Second, we compared MDP models that did-and did not-allow for changes in prior expectations over successive blocks of the visual search paradigm. We found that implicit prior beliefs about the speed and accuracy of visual searches changed systematically with experience. Finally, we characterised intersubject variability in terms of subject-specific prior beliefs. Specifically, we used canonical correlation analysis to see if there were any mixtures of prior expectations that could predict between-subject differences in performance; thereby establishing a quantitative link between different behavioural phenotypes and Bayesian belief updating. We demonstrated that better scene categorisation performance is consistently associated with lower reliance on heuristics; i.e., a greater use of a generative model of the scene to direct its exploration.},
	Author = {Mirza, M. Berk and Adams, Rick A. and Mathys, Christoph and Friston, Karl J.},
	Doi = {10.1371/journal.pone.0190429},
	Editor = {Kiebel, Stefan},
	Issn = {1932-6203},
	Journal = {PLOS ONE},
	Month = jan,
	Number = {1},
	Pages = {e0190429},
	Pmid = {29304087},
	Title = {Human visual exploration reduces uncertainty about the sensed world},
	Url = {http://www.ncbi.nlm.nih.gov/pubmed/29304087},
	Volume = {13},
	Year = {2018},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pubmed/29304087},
	Bdsk-Url-2 = {https://doi.org/10.1371/journal.pone.0190429}}

@article{Friston12,
	Abstract = {If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organized behavior: namely, the imperative to minimize the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free-energy minimization. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or assimilated into beliefs about the world.},
	Author = {Friston, Karl J and Adams, Rick A. and Perrinet, Laurent U and Breakspear, Michael},
	Date = {2012},
	Date-Modified = {2019-02-24 23:49:43 +0100},
	Doi = {10.3389/fpsyg.2012.00151},
	Issn = {1664-1078},
	Journal = {Frontiers in Psychology},
	Keywords = {free energy,active inference,Bayesian model,eye movements},
	Title = {Perceptions as Hypotheses: Saccades as Experiments},
	Url = {http://dx.doi.org/10.3389/fpsyg.2012.00151},
	Volume = {3},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.3389/fpsyg.2012.00151}}

@article{Traver10,
	Author = {Javier Traver, V and Bernardino, Alexandre},
	Date-Modified = {2019-02-27 10:10:54 +0100},
	Journal = {Robotics and Autonomous Systems},
	Number = {4},
	Pages = {378--398},
	Publisher = {Elsevier},
	Title = {A review of log-polar imaging for visual perception in robotics},
	Volume = {58},
	Year = {2010}}

@article{Butko2010infomax,
	Author = {Butko, Nicholas J and Movellan, Javier R},
	Journal = {IEEE Transactions on Autonomous Mental Development},
	Number = {2},
	Pages = {91--107},
	Publisher = {IEEE},
	Title = {Infomax control of eye movements},
	Volume = {2},
	Year = {2010}}
