%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Laurent Perrinet at 2019-02-27 20:38:47 +0100


%% Saved with string encoding Unicode (UTF-8)

@article{Najemnik05,
  title = {Optimal Eye Movement Strategies in Visual Search},
  volume = {434},
  issn = {1476-4687},
  doi = {10/bcbw2b},
  abstract = {To perform visual search, humans, like many mammals, encode a large field of view with retinas having variable spatial resolution, and then use high-speed eye movements to direct the highest-resolution region, the fovea, towards potential target locations1,2. Good search performance is essential for survival, and hence mammals may have evolved efficient strategies for selecting fixation locations. Here we address two questions: what are the optimal eye movement strategies for a foveated visual system faced with the problem of finding a target in a cluttered environment, and do humans employ optimal eye movement strategies during a search? We derive the ideal bayesian observer3,4,5,6 for search tasks in which a target is embedded at an unknown location within a random background that has the spectral characteristics of natural scenes7. Our ideal searcher uses precise knowledge about the statistics of the scenes in which the target is embedded, and about its own visual system, to make eye movements that gain the most information about target location. We find that humans achieve nearly optimal search performance, even though humans integrate information poorly across fixations8,9,10. Analysis of the ideal searcher reveals that there is little benefit from perfect integration across fixations\textemdash{}much more important is efficient processing of information on each fixation. Apparently, evolution has exploited this fact to achieve efficient eye movement strategies with minimal neural resources devoted to memory.},
  language = {en},
  number = {7031},
  journal = {Nature},
  author = {Najemnik, Jiri and Geisler, Wilson S.},
  month = mar,
  year = {2005},
  pages = {387-391},
  file = {/Users/laurentperrinet/Zotero/storage/QJWMGANK/Najemnik and Geisler - 2005 - Optimal eye movement strategies in visual search.pdf;/Users/laurentperrinet/Zotero/storage/G2T6F425/nature03390.html},
  note = {00730}
}



@article{Redmon15,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/laurentperrinet/Zotero/storage/76U3U3ZK/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Dete.pdf;/Users/laurentperrinet/Zotero/storage/XSC25BKG/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Dete.pdf;/Users/laurentperrinet/Zotero/storage/888UZQG7/1506.html;/Users/laurentperrinet/Zotero/storage/XH46VP38/1506.html},
  note = {03448}
}


@article{Ren17,
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  volume = {39},
  issn = {0162-8828},
  shorttitle = {Faster {{R}}-{{CNN}}},
  doi = {10/gc7rmb},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  number = {6},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Ren, S. and He, K. and Girshick, R. and Sun, J.},
  month = jun,
  year = {2017},
  keywords = {Feature extraction,convolutional neural network,Training,attention mechanisms,COCO 2015 competitions,Convolutional codes,deep VGG-16 model,Detectors,faster-R-CNN,full-image convolutional features,GPU,graphics processing units,high-quality region proposals,ILSVRC,MS COCO datasets,neural nets,object detection,Object detection,object detection accuracy,PASCAL VOC 2007,PASCAL VOC 2012,Proposals,real-time object detection,region proposal,region proposal networks,RPN,Search problems},
  pages = {1137-1149},
  file = {/Users/laurentperrinet/Zotero/storage/3YLDEDRA/7485869.html},
  note = {08051}
}


@article{Mirza18,
	title = {Human visual exploration reduces uncertainty about the sensed world},
	volume = {13},
	issn = {1932-6203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/29304087},
	doi = {10.1371/journal.pone.0190429},
	abstract = {In previous papers, we introduced a normative scheme for scene construction and epistemic (visual) searches based upon active inference. This scheme provides a principled account of how people decide where to look, when categorising a visual scene based on its contents. In this paper, we use active inference to explain the visual searches of normal human subjects; enabling us to answer some key questions about visual foraging and salience attribution. First, we asked whether there is any evidence for 'epistemic foraging'; i.e. exploration that resolves uncertainty about a scene. In brief, we used Bayesian model comparison to compare Markov decision process (MDP) models of scan-paths that did-and did not-contain the epistemic, uncertainty-resolving imperatives for action selection. In the course of this model comparison, we discovered that it was necessary to include non-epistemic (heuristic) policies to explain observed behaviour (e.g., a reading-like strategy that involved scanning from left to right). Despite this use of heuristic policies, model comparison showed that there is substantial evidence for epistemic foraging in the visual exploration of even simple scenes. Second, we compared MDP models that did-and did not-allow for changes in prior expectations over successive blocks of the visual search paradigm. We found that implicit prior beliefs about the speed and accuracy of visual searches changed systematically with experience. Finally, we characterised intersubject variability in terms of subject-specific prior beliefs. Specifically, we used canonical correlation analysis to see if there were any mixtures of prior expectations that could predict between-subject differences in performance; thereby establishing a quantitative link between different behavioural phenotypes and Bayesian belief updating. We demonstrated that better scene categorisation performance is consistently associated with lower reliance on heuristics; i.e., a greater use of a generative model of the scene to direct its exploration.},
	number = {1},
	journal = {PLOS ONE},
	author = {Mirza, M. Berk and Adams, Rick A. and Mathys, Christoph and Friston, Karl J.},
	editor = {Kiebel, Stefan},
	month = jan,
	year = {2018},
	pmid = {29304087},
	pages = {e0190429}
}

@article{Friston12,
    abstract = {If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organized behavior: namely, the imperative to minimize the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free-energy minimization. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or assimilated into beliefs about the world.},
    author = {Friston, Karl J and Adams, Rick A. and Perrinet, Laurent U and Breakspear, Michael},
    bdsk-url-1 = {http://dx.doi.org/10.3389/fpsyg.2012.00151},
    date = {2012},
    date-modified = {2019-02-24 23:49:43 +0100},
    doi = {10.3389/fpsyg.2012.00151},
    issn = {1664-1078},
    journal = {Frontiers in Psychology},
    keywords = {free energy,active inference,Bayesian model,eye movements},
    title = {Perceptions as Hypotheses: Saccades as Experiments},
    url = {http://dx.doi.org/10.3389/fpsyg.2012.00151},
    volume = {3},
    year = {2012}
}

@article{Traver10,
	Author = {Javier Traver, V and Bernardino, Alexandre},
	Date-Modified = {2019-02-27 10:10:54 +0100},
	Journal = {Robotics and Autonomous Systems},
	Number = {4},
	Pages = {378--398},
	Publisher = {Elsevier},
	Title = {A review of log-polar imaging for visual perception in robotics},
	Volume = {58},
	Year = {2010}}
