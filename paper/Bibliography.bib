%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Laurent Perrinet at 2020-08-28 08:24:26 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{Connant70,
	Author = {Connant, ROGER C. and Ashby, W Ross},
	Date = {1970-10},
	Date-Added = {2020-08-06 14:43:17 +0200},
	Date-Modified = {2020-08-06 14:44:11 +0200},
	Doi = {10/bbgr9b},
	Issn = {0020-7721},
	Journaltitle = {International Journal of Systems Science},
	Keywords = {\#nosource},
	Number = {2},
	Pages = {89--97},
	Title = {Every Good Regulator of a System Must Be a Model of That System},
	Url = {http://www.tandfonline.com/doi/abs/10.1080/00207727008920220},
	Volume = {1},
	Bdsk-Url-1 = {http://www.tandfonline.com/doi/abs/10.1080/00207727008920220},
	Bdsk-Url-2 = {https://doi.org/10/bbgr9b}}

@article{Ashby,
	Author = {Ashby, W. Ross},
	Date-Added = {2020-08-06 14:41:31 +0200},
	Date-Modified = {2020-08-06 14:41:43 +0200},
	Keywords = {\#nosource},
	Title = {Principles of the Self-Organizing System}}

@article{Moore17,
	Abstract = {Animals move their heads and eyes to compensate for movements of the body and background, search, fixate, and track objects visually. Avian saccadic head/eye movements have been shown to vary considerably between species. We tested the hypothesis that the configuration of the retina (i.e., changes in retinal ganglion cell density from the retinal periphery to the center of acute vision-fovea) would account for the inter-specific variation in avian head/eye movement behavior. We characterized retinal configuration, head movement rate, and degree of eye movement of 29 bird species with a single fovea, controlling for the effects of phylogenetic relatedness. First, we found the avian fovea is off the retinal center towards the dorso-temporal region of the retina. Second, species with a more pronounced rate of change in ganglion cell density across the retina generally showed a higher degree of eye movement and higher head movement rate likely because a smaller retinal area with relatively high visual acuity leads to greater need to move the head/eye to align this area that contains the fovea with objects of interest. Our findings have implications for anti-predator behavior, as many predator-prey interaction models assume that the sensory system of prey (and hence their behavior) varies little between species.},
	Author = {Moore, Bret A. and Tyrrell, Luke P. and Pita, Diana and Bininda-Emonds, Olaf R. P. and Fern{\'a}ndez-Juricic, Esteban},
	Date = {2017-01-12},
	Date-Added = {2020-08-06 14:36:07 +0200},
	Date-Modified = {2020-08-06 14:36:07 +0200},
	Doi = {10/f9k78h},
	Eprint = {28079062},
	Eprinttype = {pmid},
	Issn = {2045-2322},
	Journaltitle = {Scientific Reports},
	Pmcid = {PMC5228126},
	Shortjournal = {Sci Rep},
	Title = {Does Retinal Configuration Make the Head and Eyes of Foveate Birds Move?},
	Url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5228126/},
	Urldate = {2020-08-06},
	Volume = {7},
	Bdsk-Url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5228126/},
	Bdsk-Url-2 = {https://doi.org/10/f9k78h}}

@article{Banks15,
	Abstract = {There is a striking correlation between terrestrial species' pupil shape and ecological niche (that is, foraging mode and time of day they are active). Species with vertically elongated pupils are very likely to be ambush predators and active day and night. Species with horizontally elongated pupils are very likely to be prey and to have laterally placed eyes. Vertically elongated pupils create astigmatic depth of field such that images of vertical contours nearer or farther than the distance to which the eye is focused are sharp, whereas images of horizontal contours at different distances are blurred. This is advantageous for ambush predators to use stereopsis to estimate distances of vertical contours and defocus blur to estimate distances of horizontal contours. Horizontally elongated pupils create sharp images of horizontal contours ahead and behind, creating a horizontally panoramic view that facilitates detection of predators from various directions and forward locomotion across uneven terrain.
A novel explanation of why certain pupil shapes are visually advantageous for terrestrial animals in different ecological niches.
A novel explanation of why certain pupil shapes are visually advantageous for terrestrial animals in different ecological niches.},
	Author = {Banks, Martin S. and Sprague, William W. and Schmoll, J{\"u}rgen and Parnell, Jared A. Q. and Love, Gordon D.},
	Date = {2015-08-01},
	Date-Added = {2020-08-06 14:28:10 +0200},
	Date-Modified = {2020-08-06 14:28:10 +0200},
	Doi = {10/gg66t6},
	Issn = {2375-2548},
	Journaltitle = {Science Advances},
	Langid = {english},
	Number = {7},
	Pages = {e1500391},
	Publisher = {{American Association for the Advancement of Science}},
	Title = {Why Do Animal Eyes Have Pupils of Different Shapes?},
	Url = {https://advances.sciencemag.org/content/1/7/e1500391},
	Urldate = {2020-08-06},
	Volume = {1},
	Bdsk-Url-1 = {https://advances.sciencemag.org/content/1/7/e1500391},
	Bdsk-Url-2 = {https://doi.org/10/gg66t6}}

@incollection{Wolfe15,
	Abstract = {In generalizing from laboratory search tasks to tasks in the real world, we need to be clever and careful. In a typical laboratory search experiment, observers (Os) with very little at stake typically do hundreds of trials of one type of search with each search lasting on the order of a second. The search stimuli and the response are generally quite artificial, and the search target is typically present on half of the trials or on all of them if the response is some sort of localization or identification task. These tasks are intended to tell us about search in the world where, in contrast to the lab, the stakes could be very high. (Where is my child? Is there a bomb here?) Unlike in the lab, most real tasks will not be repeated over and over again. How often do you need to find the baking powder on the supermarket shelf? Unlike a single search trial in the lab, many real search tasks take quite a while. For instance, searching a patient's chest CT scan for signs of lung cancer will take minutes, not seconds. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	Author = {Wolfe, Jeremy M.},
	Booktitle = {The Handbook of Attention.},
	Date = {2015},
	Date-Added = {2020-08-05 14:09:35 +0200},
	Date-Modified = {2020-08-05 14:09:35 +0200},
	Isbn = {978-0-262-02969-8 (Hardcover); 978-0-262-33187-6 (Digital (undefined format))},
	Keywords = {*Attention,*Psychology,Stimulus Control},
	Location = {{Cambridge, MA, US}},
	Pages = {27--56},
	Publisher = {{MIT Press}},
	Title = {Visual Search},
	Url = {https://b-ok.cc/book/2956574/11f6d9},
	Bdsk-Url-1 = {https://b-ok.cc/book/2956574/11f6d9}}

@article{Hoppe19,
	Abstract = {The capability of directing gaze to relevant parts in the environment is crucial for our survival. Computational models have proposed quantitative accounts of human gaze selection in a range of visual search tasks. Initially, models suggested that gaze is directed to the locations in a visual scene at which some criterion such as the probability of target location, the reduction of uncertainty or the maximization of reward appear to be maximal. But subsequent studies established, that in some tasks humans instead direct their gaze to locations, such that after the single next look the criterion is expected to become maximal. However, in tasks going beyond a single action, the entire action sequence may determine future rewards thereby necessitating planning beyond a single next gaze shift. While previous empirical studies have suggested that human gaze sequences are planned, quantitative evidence for whether the human visual system is capable of finding optimal eye movement sequences according to probabilistic planning is missing. Here we employ a series of computational models to investigate whether humans are capable of looking ahead more than the next single eye movement. We found clear evidence that subjects' behavior was better explained by the model of a planning observer compared to a myopic, greedy observer, which selects only a single saccade at a time. In particular, the location of our subjects' first fixation differed depending on the stimulus and the time available for the search, which was well predicted quantitatively by a probabilistic planning model. Overall, our results are the first evidence that the human visual system's gaze selection agrees with optimal planning under uncertainty.},
	Author = {Hoppe, David and Rothkopf, Constantin A.},
	Date = {2019-01-15},
	Date-Added = {2020-08-05 13:06:52 +0200},
	Date-Modified = {2020-08-05 13:06:58 +0200},
	Doi = {10/gfwcvc},
	Issn = {2045-2322},
	Issue = {1},
	Journaltitle = {Scientific Reports},
	Langid = {english},
	Number = {1},
	Pages = {144},
	Publisher = {{Nature Publishing Group}},
	Title = {Multi-Step Planning of Eye Movements in Visual Search},
	Url = {https://www.nature.com/articles/s41598-018-37536-0},
	Urldate = {2020-08-05},
	Volume = {9},
	Bdsk-Url-1 = {https://www.nature.com/articles/s41598-018-37536-0},
	Bdsk-Url-2 = {https://doi.org/10/gfwcvc}}

@article{Tang20,
	Abstract = {Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight. It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details. Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a self-attention bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space. We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning (RL) tasks, allowing us to incorporate modules that can include discrete, non-differentiable operations which are useful for our agent. We argue that self-attention has similar properties as indirect encoding, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods. Since our agent attends to only task critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail. Videos of our results and source code available at https://attentionagent.github.io/},
	Archiveprefix = {arXiv},
	Author = {Tang, Yujin and Nguyen, Duong and Ha, David},
	Date = {2020-06-25},
	Date-Added = {2020-08-05 13:01:00 +0200},
	Date-Modified = {2020-08-05 13:01:08 +0200},
	Doi = {10/gg64b3},
	Eprint = {2003.08165},
	Eprinttype = {arxiv},
	Journaltitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
	Pages = {414--424},
	Title = {Neuroevolution of {{Self}}-{{Interpretable Agents}}},
	Url = {http://arxiv.org/abs/2003.08165},
	Urldate = {2020-08-05},
	Bdsk-Url-1 = {http://arxiv.org/abs/2003.08165},
	Bdsk-Url-2 = {https://doi.org/10/gg64b3}}

@article{Isard98,
	Abstract = {The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimo dal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses ``factored sampling'', previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together with visual observations, to propagate the random set over time. The result is highly robust tracking of agile motion. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time.},
	Author = {Isard, Michael and Blake, Andrew},
	Date = {1998},
	Date-Modified = {2020-08-05 12:58:52 +0200},
	Doi = {10.1023/A:1008078328650},
	Eprint = {437},
	Eprinttype = {pmid},
	File = {/Users/laurentperrinet/Zotero/storage/IWUIKMCH/Isard, Blake - 1998 - Condensation 膒 conditional density propagation for visual tracking(3).pdf;/Users/laurentperrinet/Zotero/storage/UAL2ZJ3F/Isard, Blake - 1998 - Condensation 膒 conditional density propagation for visual tracking(3).pdf},
	Issn = {09205691},
	Journaltitle = {International journal of computer vision},
	Keywords = {bicv-motion,bicv-sparse,free energy,freemove,khoei12jpp,khoei13jpp,particle-filter,perrinet12pred,perrinetadamsfriston14,thesis},
	Number = {1},
	Pages = {5--28},
	Title = {Condensation - Conditional Density Propagation for Visual Tracking},
	Url = {http://link.springer.com/article/10.1023/A:1008078328650},
	Volume = {29},
	Bdsk-Url-1 = {http://link.springer.com/article/10.1023/A:1008078328650},
	Bdsk-Url-2 = {https://doi.org/10.1023/A:1008078328650}}

@article{Martin18,
	Abstract = {A number of studies have shown human subjects' impressive ability to detect faces in individual images, with saccade reaction times starting as fast as 100\,ms after stimulus onset. Here, we report evidence that humans can rapidly and continuously saccade towards single faces embedded in different scenes at rates approaching 6 faces/scenes each second (including blinks and eye movement times). These observations are impressive, given that humans usually make no more than 2 to 5 saccades per second when searching a single scene with eye movements. Surprisingly, attempts to hide the faces by blending them into a large background scene had little effect on targeting rates, saccade reaction times, or targeting accuracy. Upright faces were found more quickly and more accurately than inverted faces; both with and without a cluttered background scene, and over a large range of eccentricities (4$\,^{\circ}$--16$\,^{\circ}$). The fastest subject in our study made continuous saccades to 500 small 3$\,^{\circ}$ upright faces at 4$\,^{\circ}$ eccentricities in only 96\,seconds. The maximum face targeting rate ever achieved by any subject during any sequence of 7 faces during Experiment 3 for the no scene and upright face condition was 6.5 faces targeted/second. Our data provide evidence that the human visual system includes an ultra-rapid and continuous object localization system for upright faces. Furthermore, these observations indicate that continuous paradigms such as the one we have used can push humans to make remarkably fast reaction times that impose strong constraints and challenges on models of how, where, and when visual processing occurs in the human brain.},
	Articleurl = {https://www.nature.com/articles/s41598-018-30245-8},
	Author = {Martin, Jacob G. and Davis, Charles E. and Riesenhuber, Maximilian and Thorpe, Simon J.},
	Date = {2018-08-20},
	Doi = {10/gd5x5d},
	File = {/Users/laurentperrinet/Zotero/storage/4S5IGGZD/Martin et al. - 2018 - Zapping 500 faces in less than 100 seconds Eviden.pdf;/Users/laurentperrinet/Zotero/storage/8SB58WC7/s41598-018-30245-8.html},
	Issn = {2045-2322},
	Journal = {Scientific Reports},
	Langid = {english},
	Note = {00006},
	Number = {1},
	Pages = {1--12},
	Shortjournal = {Sci Rep},
	Shorttitle = {Zapping 500 Faces in Less than 100 Seconds},
	Title = {Zapping 500 Faces in Less than 100 Seconds: {{Evidence}} for Extremely Fast and Sustained Continuous Visual Search},
	Urldate = {2020-01-23},
	Volume = {8},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10/gd5x5d}}

@article{Kaplanyan19,
	Address = {New York, NY, USA},
	Articleno = {212},
	Articleurl = {https://dl.acm.org/doi/abs/10.1145/3355089.3356557},
	Author = {Kaplanyan, Anton and Sochenov, Anton and Leimk{\"u}hler, Thomas and Okunev, Mikhail and Goodall, Todd and Rufo, Gizem},
	Date = {2019-11-08},
	Doi = {10.1145/3355089.3356557},
	Issn = {0730-0301},
	Issue_Date = {November 2019},
	Journal = {ACM Transactions on Graphics (TOG)},
	Keywords = {generative networks, gaze-contingent rendering, perceptual rendering, video generation, video compression, foveated rendering, virtual reality, deep learning},
	Month = nov,
	Number = {6},
	Numpages = {13},
	Publisher = {Association for Computing Machinery},
	Title = {{DeepFovea}: Neural Reconstruction for Foveated Rendering and Video Compression Using Learned Statistics of Natural Videos},
	Url = {https://doi.org/10.1145/3355089.3356557},
	Volume = {38},
	Year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1145/3355089.3356557}}

@article{Cullen20,
	Abstract = {We propose a computational model of visual search that incorporates Bayesian interpretations of the neural mechanisms that underlie categorical perception and saccade planning. To enable meaningful comparisons between simulated and human behaviours, we employ a gaze-contingent paradigm that required participants to classify occluded MNIST digits through a window that followed their gaze. The conditional independencies imposed by a separation of time scales in this task are embodied by constraints on the hierarchical structure of our model; planning and decision making are cast as a partially observable Markov Decision Process while proprioceptive and exteroceptive signals are integrated by a dynamic model that facilitates approximate inference on visual information and its latent causes. Our model is able to recapitulate human behavioural metrics such as classification accuracy while retaining a high degree of interpretability, which we demonstrate by recovering subject-specific parameters from observed human behaviour.},
	Archiveprefix = {arXiv},
	Author = {Cullen, Maell and Monney, Jonathan and Mirza, M. Berk and Moran, Rosalyn},
	Date = {2020-06-05},
	Date-Modified = {2020-08-05 12:58:41 +0200},
	Eprint = {2006.03531},
	Eprinttype = {arxiv},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Primaryclass = {cs},
	Title = {A {{Meta}}-{{Bayesian Model}} of {{Intentional Visual Search}}},
	Url = {http://arxiv.org/abs/2006.03531},
	Urldate = {2020-06-15},
	Bdsk-Url-1 = {http://arxiv.org/abs/2006.03531}}

@inproceedings{Kummerer17,
	Articleurl = {http://openaccess.thecvf.com/content_iccv_2017/html/Kummerer_Understanding_Low-_and_ICCV_2017_paper.html},
	Author = {Kummerer, Matthias and Wallis, Thomas S. A. and Gatys, Leon A. and Bethge, Matthias},
	Booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
	Pages = {4789--4798},
	Title = {Understanding {{Low}}- and {{High}}-{{Level Contributions}} to {{Fixation Prediction}}},
	Urldate = {2020-01-14},
	Year = {2017}}

@article{Bahill75,
	Abstract = {The astronomical term ``main sequence'' has been applied to the relationships between duration, peak velocity, and magnitude of human saccades over a thousandfold range of magnitude. Infrared photodiodes aimed at the iris-sclera border and a digital computer were used in experiments to derive the main sequence curves. In the pulse width modulation model, the duration of the controller signal pulse determines saccadic amplitude and peak velocity. The high-frequency burst of the oculomotoneurons needs to be only one-half the duration of the saccade, because of the ``apparent inertia'' of the eyeball.},
	Articleurl = {http://www.sciencedirect.com/science/article/pii/0025556475900759},
	Author = {Bahill, A. Terry and Clark, Michael R. and Stark, Lawrence},
	Doi = {10/b655hq},
	File = {/Users/laurentperrinet/Zotero/storage/RJJ469GV/Bahill et al. - 1975 - The main sequence, a tool for studying human eye m.pdf;/Users/laurentperrinet/Zotero/storage/F49PMD68/0025556475900759.html},
	Issn = {0025-5564},
	Journal = {Mathematical Biosciences},
	Journaltitle = {Mathematical Biosciences},
	Langid = {english},
	Note = {01092},
	Number = {3},
	Pages = {191--204},
	Title = {The Main Sequence, a Tool for Studying Human Eye Movements},
	Urldate = {2020-01-14},
	Volume = {24},
	Year = {1975},
	Bdsk-Url-1 = {https://doi.org/10/b655hq}}

@article{Strasburger11,
	Articleurl = {https://jov.arvojournals.org/article.aspx?articleid=2191825},
	Author = {Strasburger, Hans and Rentschler, Ingo and J{\"u}ttner, Martin},
	Doi = {10/fx28dx},
	File = {/Users/laurentperrinet/Zotero/storage/UAHQ53WB/Strasburger et al. - 2011 - Peripheral vision and pattern recognition A revie.pdf;/Users/laurentperrinet/Zotero/storage/VCRPMPSZ/article.html},
	Issn = {1534-7362},
	Journal = {Journal of Vision},
	Journaltitle = {Journal of Vision},
	Langid = {english},
	Note = {00489},
	Number = {5},
	Pages = {13--13},
	Shorttitle = {Peripheral Vision and Pattern Recognition},
	Title = {Peripheral Vision and Pattern Recognition: {{A}} Review},
	Urldate = {2020-01-14},
	Volume = {11},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10/fx28dx}}

@article{fischer2007self,
	Author = {Fischer, Sylvain and {\v{S}}roubek, Filip and Perrinet, Laurent U and Redondo, Rafael and Crist{\'o}bal, Gabriel},
	Journal = {International Journal of Computer Vision},
	Number = {2},
	Pages = {231--246},
	Publisher = {Springer},
	Title = {Self-invertible {2D} log-{G}abor wavelets},
	Volume = {75},
	Year = {2007}}

@article{Fischer2007a,
	Abstract = {Abstract--- Meanwhile biorthogonal wavelets got a very popu- lar image processing tool, alternative multiresolution transforms have been proposed for solving some of their drawbacks, namely the poor selectivity in orientation and the lack of translation in- variance due to the aliasing between subbands. These transforms are generally overcomplete and consequently offer huge degrees of freedom in their design. At the same time their optimization get a challenging task. We proposed here a log-Gabor wavelet transform gathering the excellent mathematical properties of the Gabor functions with a carefully construction to maintain the properties of the filters and to permit exact reconstruction. Two major improvements are proposed: first the highest frequency bands are covered by narrowly localized oriented filters. And second, all the frequency bands including the highest and lowest frequencies are uniformly covered so as exact reconstruction is achieved using the same filters in both the direct and the inverse transforms (which means that the transform is self-invertible). The transform is optimized not only mathematically but it also follows as much as possible the knowledge on the receptive field of the simple cells of the Primary Visual Cortex (V1) of primates and on the statistics of natural images. Compared to the state of the art, the log-Gabor wavelets show excellent behavior in their ability to segregate the image information (e.g. the contrast edges) from incoherent Gaussian noise by hard thresholding and to code the image features through a reduced set of coefficients with large magnitude. Such characteristics make the transform a promising tool for general image processing tasks.},
	Articleurl = {http://link.springer.com/10.1007/s11263-006-0026-8},
	Author = {Fischer, Sylvain and Sroubek, Filip and Perrinet, Laurent and Redondo, Rafael and Cristobal, Gabriel},
	Date = {2007-01},
	Doi = {10.1007/s11263-006-0026-8},
	File = {/Users/laurentperrinet/Zotero/storage/ES4G7AYA/Fischer et al. - 2007 - Self-Invertible 2D Log-Gabor Wavelets.pdf},
	Issn = {0920-5691},
	Journal = {International Journal of Computer Vision},
	Keywords = {bicv-sparse,Wavelet transforms,motion_clouds,motion-clouds,sanz12jnp,vision,vacher14,image denoising,Image denoising,log-gabor,log-Gabor filters,Log-Gabor filters,oriented high-pass filters,Oriented high-pass filters,perrinet11sfn,visual system,Visual system,wavelet transforms,wavelets,assofield,denoising,filters,high-pass,image,oriented,system,transforms,wavelet},
	Number = {2},
	Pages = {231--246},
	Title = {Self-invertible {2D} log-{G}abor wavelets},
	Volume = {75},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11263-006-0026-8}}

@article{Sanz12,
	Abstract = {Choosing an appropriate set of stimuli is essential to characterize the response of a sensory system to a particular functional dimension, such as the eye movement following the motion of a visual scene. Here, we describe a framework to generate random texture movies with controlled information content, i.e., Motion Clouds. These stimuli are defined using a generative model that is based on controlled experimental parametrization. We show that Motion Clouds correspond to dense mixing of localized moving gratings with random positions. Their global envelope is similar to natural-like stimulation with an approximate full-field translation corresponding to a retinal slip. We describe the construction of these stimuli mathematically and propose an open-source Python-based implementation. Examples of the use of this framework are shown. We also propose extensions to other modalities such as color vision, touch, and audition.},
	Articleurl = {http://dx.doi.org/10.1152/jn.00737.2011},
	Author = {Sanz-Leon, Paula and Vanzetta, Ivo and Masson, G.S. and Perrinet, Laurent U.},
	Date = {2012-03},
	Doi = {10.1152/jn.00737.2011},
	Issn = {1522-1598},
	Journal = {Journal of Neurophysiology},
	Keywords = {anr-trax,bicv-sparse,perrinetadamsfriston14,Natural scenes,Motion detection,motion-clouds,sanz12jnp,vacher14,Eye movements,kaplan13,Low-level sensory systems,Optimal stimulation,Python},
	Number = {11},
	Pages = {3217--3226},
	Title = {Motion Clouds: Model-Based Stimulus Synthesis of Natural-like Random Textures for the Study of Motion Perception},
	Volume = {107},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1152/jn.00737.2011}}

@inproceedings{Paszke17,
	Abstract = {In this article, we describe an automatic differentiation module of PyTorch --- a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua...},
	Articlearticleurl = {https://openreview.net/forum?id=BJJsrmfCZ},
	Author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	Booktitle = {NIPS 2017 Workshop Autodiff Program Chairs},
	Date = {2017-10-28},
	Note = {00918},
	Title = {Automatic Differentiation in {PyTorch}},
	Urldate = {2019-03-18},
	Year = {2017}}

@incollection{NEURIPS2019_9015,
	Articleurl = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	Author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	Booktitle = {Advances in Neural Information Processing Systems 32},
	Editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlch\'{e}-Buc, F. and Fox, E. and Garnett, R.},
	Pages = {8024--8035},
	Publisher = {Curran Associates, Inc.},
	Title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	Year = {2019}}

@article{Lecun1998,
	Abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	Articleurl = {https://ieeexplore.ieee.org/abstract/document/726791},
	Author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	Date = {1998-11},
	Doi = {10/d89c25},
	Issn = {0018-9219},
	Journal = {Proceedings of the IEEE},
	Number = {11},
	Pages = {2278-2324},
	Title = {Gradient-based learning applied to document recognition},
	Volume = {86},
	Year = {1998},
	Bdsk-Url-1 = {https://doi.org/10/d89c25}}

@article{Dauce18,
	Abstract = {What motivates the action in the absence of a definite reward? Taking the case of visuomotor control, we consider a minimal control problem that is how select the next saccade, in a sequence of discrete eye movements, when the final objective is to better interpret the current visual scene. The visual scene is modeled here as a partially-observed environment, with a generative model explaining how the visual data is shaped by action. This allows to interpret different action selection metrics proposed in the literature, including the Salience, the Infomax and the Variational Free Energy, under a single information theoretic construct, namely the view-based Information Gain. Pursuing this analytic track, two original action selection metrics named the Information Gain Lower Bound (IGLB) and the Information Gain Upper Bound (IGUB) are then proposed. Showing either a conservative or an optimistic bias regarding the Information Gain, they strongly simplify its calculation. An original fovea-based visual scene decoding setup is then proposed, with numerical experiments highlighting different facets of artificial fovea-based vision. A first and principal result is that state-of-the-art recognition rates are obtained with fovea-based saccadic exploration, using less than 10\% of the original image's data. Those satisfactory results illustrate the advantage of mixing predictive control with accurate state-of-the-art predictors, namely a deep neural network. A second result is the sub-optimality of some classical action-selection metrics widely used in literature, that is not manifest with finely-tuned inference models, but becomes patent when coarse or faulty models are used. Last, a computationally-effective predictive model is developed using the IGLB objective, with pre-processed visual scan-path read-out from memory, bypassing computationally-demanding predictive calculations. This last simplified setting is shown effective in our case, showing both a competing accuracy and a good robustness to model flaws.},
	Articleurl = {https://doi.org/10/gfrhgj},
	Author = {Dauc\'e, Emmanuel},
	Date-Added = {2019-03-08 16:12:29 +0100},
	Date-Modified = {2019-03-08 16:37:30 +0100},
	Doi = {10/gfrhgj},
	File = {/Users/laurentperrinet/Zotero/storage/QUYVDZTT/Dauc{\'e} - 2018 - Active Fovea-Based Vision Through Computationally-.pdf;/Users/laurentperrinet/Zotero/storage/VS9U4UNG/Dauc{\'e} - 2018 - Active Fovea-Based Vision Through Computationally-.pdf},
	Issn = {1662-5218},
	Journal = {Frontiers in Neurorobotics},
	Keywords = {active inference,active vision,Convolutional Neural Networks (CNN),End-effector control,Foveated vision,information gain,intrinsic motivation,Saccadic eye movements,visual scene interpretation},
	Title = {Active fovea-based vision through computationally-effective model-based prediction},
	Volume = {12},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10/gfrhgj}}

@article{Sandler18,
	Abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	Archiveprefix = {arXiv},
	Author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	Date-Added = {2019-03-08 14:02:48 +0100},
	Date-Modified = {2019-03-08 14:02:48 +0100},
	Journal = {arXiv:1801.04381 [cs]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = jan,
	Note = {00241},
	Shorttitle = {{{MobileNetV2}}},
	Title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
	Year = {2018}}

@article{Vo12,
	Abstract = {One might assume that familiarity with a scene or previous encounters with objects embedded in a scene would benefit subsequent search for those items. However, in a series of experiments we show that this is not the case: When participants were asked to subsequently search for multiple objects in the same scene, search performance remained essentially unchanged over the course of searches despite increasing scene familiarity. Similarly, looking at target objects during previews, which included letter search, 30 seconds of free viewing, or even 30 seconds of memorizing a scene, also did not benefit search for the same objects later on. However, when the same object was searched for again memory for the previous search was capable of producing very substantial speeding of search despite many different intervening searches. This was especially the case when the previous search engagement had been active rather than supported by a cue. While these search benefits speak to the strength of memory-guided search when the same search target is repeated, the lack of memory guidance during initial object searches\textemdash{}despite previous encounters with the target objects\textemdash{}demonstrates the dominance of guidance by generic scene knowledge in real-world search. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	Articleurl = {https://doi.org/10.1037/a0024147},
	Author = {V\~o, Melissa L.-H. and Wolfe, Jeremy M.},
	Doi = {10.1037/a0024147},
	File = {/Users/lolo/Zotero/storage/3AUVQLFG/V{\~o} and Wolfe - 2012 - When does repeated search in scenes involve memory.pdf;/Users/lolo/Zotero/storage/DUPQTZQJ/2011-12267-001.html},
	Issn = {1939-1277(Electronic),0096-1523(Print)},
	Journal = {Journal of Experimental Psychology: Human Perception and Performance},
	Keywords = {Contextual Associations,Episodic Memory,Visual Perception,Visual Search},
	Number = {1},
	Pages = {23-41},
	Shorttitle = {When Does Repeated Search in Scenes Involve Memory?},
	Title = {When Does Repeated Search in Scenes Involve Memory? {{Looking}} at versus Looking for Objects in Scenes},
	Volume = {38},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1037/a0024147}}

@article{He15,
	Abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	Author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	File = {/Users/laurentperrinet/Zotero/storage/VHTT3FKQ/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;/Users/laurentperrinet/Zotero/storage/TZD8MFU5/1502.html},
	Journal = {arXiv:1502.01852 [cs]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
	Month = feb,
	Note = {04208},
	Shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
	Title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
	Year = {2015}}

@article{Kummerer16,
	Abstract = {Here we present DeepGaze II, a model that predicts where people look in images. The model uses the features from the VGG-19 deep neural network trained to identify objects in images. Contrary to other saliency models that use deep features, here we use the VGG features for saliency prediction with no additional fine-tuning (rather, a few readout layers are trained on top of the VGG features to predict saliency). The model is therefore a strong test of transfer learning. After conservative cross-validation, DeepGaze II explains about 87\% of the explainable information gain in the patterns of fixations and achieves top performance in area under the curve metrics on the MIT300 hold-out benchmark. These results corroborate the finding from DeepGaze I (which explained 56\% of the explainable information gain), that deep features trained on object recognition provide a versatile feature space for performing related visual tasks. We explore the factors that contribute to this success and present several informative image examples. A web service is available to compute model predictions at http://deepgaze.bethgelab.org.},
	Archiveprefix = {arXiv},
	Author = {K\"ummerer, Matthias and Wallis, Thomas S. A. and Bethge, Matthias},
	Eprint = {1610.01563},
	Eprinttype = {arxiv},
	File = {/Users/laurentperrinet/Zotero/storage/XBMG4DQN/K{\"u}mmerer et al. - 2016 - DeepGaze II Reading fixations from deep features .pdf;/Users/laurentperrinet/Zotero/storage/CMPHRVXY/1610.html},
	Journal = {arXiv:1610.01563 [cs, q-bio, stat]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition,Statistics - Applications},
	Month = oct,
	Note = {00075},
	Primaryclass = {cs, q-bio, stat},
	Shorttitle = {{{DeepGaze II}}},
	Title = {{{DeepGaze II}}: Reading Fixations from Deep Features Trained on Object Recognition},
	Year = {2016}}

@article{Treisman80,
	Author = {Treisman, Anne M and Gelade, Garry},
	Date-Modified = {2020-08-05 14:13:04 +0200},
	Doi = {10.1016/0010-0285(80)90005-5},
	Journal = {Cognitive psychology},
	Keywords = {****,attention},
	Note = {11957},
	Number = {1},
	Pages = {97--136},
	Title = {A Feature-Integration Theory of Attention},
	Url = {http://www2.psychology.uiowa.edu/faculty/hollingworth/prosem/Treisman-1980-FEATURE-INTEGRATION.pdf},
	Volume = {12},
	Year = {1980},
	Bdsk-Url-1 = {http://www2.psychology.uiowa.edu/faculty/hollingworth/prosem/Treisman-1980-FEATURE-INTEGRATION.pdf},
	Bdsk-Url-2 = {https://doi.org/10.1016/0010-0285(80)90005-5}}

@article{Samonds18,
	Abstract = {Saccadic eye movements during free viewing exhibit patterns that reflect a strategy to increase neural responses by matching motor behavior with the statistics of the natural world and with the processing limitations of sensory systems.},
	Articleurl = {https://doi.org/10/gfgt3k},
	Author = {Samonds, Jason M. and Geisler, Wilson S. and Priebe, Nicholas J.},
	Copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	Doi = {10/gfgt3k},
	Issn = {1546-1726},
	Journal = {Nature Neuroscience},
	Language = {En},
	Month = nov,
	Note = {00002},
	Number = {11},
	Pages = {1591},
	Title = {Natural Image and Receptive Field Statistics Predict Saccade Sizes},
	Volume = {21},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10/gfgt3k}}

@article{Kirchner06,
	Abstract = {Previous ultra-rapid go/no-go categorization studies with manual responses have demonstrated the remarkable speed and efficiency with which humans process natural scenes. Using a forced-choice saccade task we show here that when two scenes are simultaneously flashed in the left and right hemifields, human participants can reliably make saccades to the side containing an animal in as little as 120 ms. Low level differences between target and distractor images were unable to account for these exceptionally fast responses. The results suggest a very fast and unexpected route linking visual processing in the ventral stream with the programming of saccadic eye movements.},
	Articleurl = {https://doi.org/10.1016/j.visres.2005.10.002},
	Author = {Kirchner, Holle and Thorpe, Simon J.},
	Doi = {10.1016/j.visres.2005.10.002},
	Issn = {0042-6989},
	Journal = {Vision Research},
	Keywords = {Visual Pathways,Humans,Psychomotor Performance,Reaction Time,Saccades,Photic Stimulation,Female,Male,Adult,Learning,Learning: physiology,Pattern Recognition,Visual,Visual: physiology,perrinet11sfn,assofield,Ocular,Ocular: physiology,Visual Pathways: physiology,Fixation,Saccades: physiology,Psychomotor Performance: physiology,Electrooculography,Electrooculography: methods,\#nosource},
	Number = {11},
	Pages = {1762--76},
	Pmid = {16289663},
	Title = {Ultra-Rapid Object Detection with Saccadic Eye Movements: Visual Processing Speed Revisited},
	Volume = {46},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.visres.2005.10.002}}

@article{Bruce2009,
	Author = {Bruce, Neil D. B. and Tsotsos, John K.},
	Doi = {10/bzzq2x},
	File = {/Users/laurentperrinet/Zotero/storage/35JDA4R7/Bruce and Tsotsos - 2009 - Saliency, attention, and visual search An informa.pdf;/Users/laurentperrinet/Zotero/storage/L84F46DZ/article.html},
	Issn = {1534-7362},
	Journal = {Journal of Vision},
	Language = {en},
	Month = mar,
	Number = {3},
	Pages = {5--5},
	Publisher = {{The Association for Research in Vision and Ophthalmology}},
	Shorttitle = {Saliency, Attention, and Visual Search},
	Title = {Saliency, Attention, and Visual Search: {{An}} Information Theoretic Approach},
	Url = {https://jov.arvojournals.org/article.aspx?articleid=2193531},
	Urldate = {2020-03-30},
	Volume = {9},
	Year = {2009},
	Bdsk-Url-1 = {https://jov.arvojournals.org/article.aspx?articleid=2193531},
	Bdsk-Url-2 = {https://doi.org/10/bzzq2x}}

@article{Zhang2008,
	Author = {Zhang, Lingyun and Tong, Matthew H. and Marks, Tim K. and Shan, Honghao and Cottrell, Garrison W.},
	Doi = {10/fcdh2d},
	File = {/Users/laurentperrinet/Zotero/storage/GBMCXIQQ/Zhang et al. - 2008 - SUN A Bayesian framework for saliency using natur.pdf;/Users/laurentperrinet/Zotero/storage/MAYIDD5M/article.html},
	Issn = {1534-7362},
	Journal = {Journal of Vision},
	Language = {en},
	Month = may,
	Number = {7},
	Pages = {32--32},
	Publisher = {{The Association for Research in Vision and Ophthalmology}},
	Shorttitle = {{{SUN}}},
	Title = {{{SUN}}: {{A Bayesian}} Framework for Saliency Using Natural Statistics},
	Url = {https://jov.arvojournals.org/article.aspx?articleid=2297284},
	Urldate = {2020-03-30},
	Volume = {8},
	Year = {2008},
	Bdsk-Url-1 = {https://jov.arvojournals.org/article.aspx?articleid=2297284},
	Bdsk-Url-2 = {https://doi.org/10/fcdh2d}}

@article{Itti2009,
	Abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer's beliefs yield surprise, irrespectively of how rare or informative in Shannon's sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72\% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84\% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction. \textcopyright{} 2008 Elsevier Ltd. All rights reserved.},
	Author = {Itti, Laurent and Baldi, Pierre},
	Doi = {10.1016/j.visres.2008.09.007},
	File = {/Users/laurentperrinet/Zotero/storage/MVQDBP64/Itti and Baldi - 2009 - Bayesian surprise attracts human attention.pdf;/Users/laurentperrinet/Zotero/storage/RDM4PS4W/S0042698908004380.html},
	Issn = {00426989},
	Journal = {Vision Research},
	Keywords = {\#nosource,Attention,Bayes theorem,Eye movements,Free viewing,Information theory,Natural vision,Novelty,Saliency,Surprise},
	Month = jun,
	Number = {10},
	Pages = {1295--1306},
	Pmid = {18834898},
	Title = {Bayesian surprise attracts human attention},
	Url = {http://linkinghub.elsevier.com/retrieve/pii/S0042698908004380},
	Volume = {49},
	Year = {2009},
	Bdsk-Url-1 = {http://linkinghub.elsevier.com/retrieve/pii/S0042698908004380},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.visres.2008.09.007}}

@article{Itti01,
	Abstract = {Five important trends have emerged from recent work on computational models of focal visual attention that emphasize the bottom-up, image-based control of attentional deployment. First, the perceptual saliency of stimuli critically depends on the surrounding context. Second, a unique 'saliency map' that topographically encodes for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Third, inhibition of return, the process by which the currently attended location is prevented from being attended again, is a crucial element of attentional deployment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. And last, scene understanding and object recognition strongly constrain the selection of attended locations. Insights from these five key areas provide a framework for a computational and neurobiological understanding of visual attention.},
	Articleurl = {https://doi.org/10/chw2bk},
	Author = {Itti, Laurent and Koch, Christof},
	Date-Modified = {2019-03-08 16:38:23 +0100},
	Doi = {10/chw2bk},
	Issn = {1471-0048},
	Journal = {Nature Reviews Neuroscience},
	Number = {3},
	Pages = {194-203},
	Title = {Computational Modelling of Visual Attention},
	Volume = {2},
	Year = {2001},
	Bdsk-Url-1 = {https://doi.org/10/chw2bk}}

@article{Najemnik05,
	Abstract = {To perform visual search, humans, like many mammals, encode a large field of view with retinas having variable spatial resolution, and then use high-speed eye movements to direct the highest-resolution region, the fovea, towards potential target locations1,2. Good search performance is essential for survival, and hence mammals may have evolved efficient strategies for selecting fixation locations. Here we address two questions: what are the optimal eye movement strategies for a foveated visual system faced with the problem of finding a target in a cluttered environment, and do humans employ optimal eye movement strategies during a search? We derive the ideal bayesian observer3,4,5,6 for search tasks in which a target is embedded at an unknown location within a random background that has the spectral characteristics of natural scenes7. Our ideal searcher uses precise knowledge about the statistics of the scenes in which the target is embedded, and about its own visual system, to make eye movements that gain the most information about target location. We find that humans achieve nearly optimal search performance, even though humans integrate information poorly across fixations8,9,10. Analysis of the ideal searcher reveals that there is little benefit from perfect integration across fixations - much more important is efficient processing of information on each fixation. Apparently, evolution has exploited this fact to achieve efficient eye movement strategies with minimal neural resources devoted to memory.},
	Articleurl = {https://doi.org/10/bcbw2b},
	Author = {Najemnik, Jiri and Geisler, Wilson S.},
	Date-Modified = {2019-03-08 16:36:32 +0100},
	Doi = {10/bcbw2b},
	Journal = {Nature},
	Number = {7031},
	Pages = {387-391},
	Title = {Optimal Eye Movement Strategies in Visual Search},
	Volume = {434},
	Year = {2005},
	Bdsk-Url-1 = {https://doi.org/10/bcbw2b}}

@inproceedings{Redmon16,
	Articleurl = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
	Author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	Booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
	Note = {07285},
	Pages = {779--788},
	Shorttitle = {You {{Only Look Once}}},
	Title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
	Urldate = {2020-01-14},
	Year = {2016}}

@article{Redmon15,
	Abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	Author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	Journal = {arxiv},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = jun,
	Note = {03448},
	Title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
	Year = {2015}}

@article{Ren17,
	Abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	Articleurl = {https://doi.org/10/gc7rmb},
	Author = {Ren, S. and He, K. and Girshick, R. and Sun, J.},
	Doi = {10/gc7rmb},
	Issn = {0162-8828},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Keywords = {Feature extraction,convolutional neural network,Training,attention mechanisms,COCO 2015 competitions,Convolutional codes,deep VGG-16 model,Detectors,faster-R-CNN,full-image convolutional features,GPU,graphics processing units,high-quality region proposals,ILSVRC,MS COCO datasets,neural nets,object detection,Object detection,object detection accuracy,PASCAL VOC 2007,PASCAL VOC 2012,Proposals,real-time object detection,region proposal,region proposal networks,RPN,Search problems},
	Month = jun,
	Note = {08051},
	Number = {6},
	Pages = {1137-1149},
	Shorttitle = {Faster {{R}}-{{CNN}}},
	Title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
	Volume = {39},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10/gc7rmb}}

@article{Mirza18,
	Abstract = {In previous papers, we introduced a normative scheme for scene construction and epistemic (visual) searches based upon active inference. This scheme provides a principled account of how people decide where to look, when categorising a visual scene based on its contents. In this paper, we use active inference to explain the visual searches of normal human subjects; enabling us to answer some key questions about visual foraging and salience attribution. First, we asked whether there is any evidence for 'epistemic foraging'; i.e. exploration that resolves uncertainty about a scene. In brief, we used Bayesian model comparison to compare Markov decision process (MDP) models of scan-paths that did-and did not-contain the epistemic, uncertainty-resolving imperatives for action selection. In the course of this model comparison, we discovered that it was necessary to include non-epistemic (heuristic) policies to explain observed behaviour (e.g., a reading-like strategy that involved scanning from left to right). Despite this use of heuristic policies, model comparison showed that there is substantial evidence for epistemic foraging in the visual exploration of even simple scenes. Second, we compared MDP models that did-and did not-allow for changes in prior expectations over successive blocks of the visual search paradigm. We found that implicit prior beliefs about the speed and accuracy of visual searches changed systematically with experience. Finally, we characterised intersubject variability in terms of subject-specific prior beliefs. Specifically, we used canonical correlation analysis to see if there were any mixtures of prior expectations that could predict between-subject differences in performance; thereby establishing a quantitative link between different behavioural phenotypes and Bayesian belief updating. We demonstrated that better scene categorisation performance is consistently associated with lower reliance on heuristics; i.e., a greater use of a generative model of the scene to direct its exploration.},
	Articleurl = {http://www.ncbi.nlm.nih.gov/pubmed/29304087},
	Author = {Mirza, M. Berk and Adams, Rick A. and Mathys, Christoph and Friston, Karl J.},
	Doi = {10.1371/journal.pone.0190429},
	Editor = {Kiebel, Stefan},
	Issn = {1932-6203},
	Journal = {PLOS ONE},
	Month = jan,
	Number = {1},
	Pages = {e0190429},
	Pmid = {29304087},
	Title = {Human visual exploration reduces uncertainty about the sensed world},
	Volume = {13},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pone.0190429}}

@article{Friston12,
	Abstract = {If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organized behavior: namely, the imperative to minimize the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free-energy minimization. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or assimilated into beliefs about the world.},
	Articleurl = {http://dx.doi.org/10.3389/fpsyg.2012.00151},
	Author = {Friston, Karl J and Adams, Rick A. and Perrinet, Laurent U. and Breakspear, Michael},
	Date-Modified = {2019-02-24 23:49:43 +0100},
	Doi = {10.3389/fpsyg.2012.00151},
	Issn = {1664-1078},
	Journal = {Frontiers in Psychology},
	Keywords = {free energy,active inference,Bayesian model,eye movements},
	Title = {Perceptions as hypotheses: Saccades as experiments},
	Volume = {3},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.3389/fpsyg.2012.00151}}

@article{PerrinetAdamsFriston14,
	Abstract = {This paper considers the problem of sensorimotor delays in the optimal control of (smooth) eye movements under uncertainty. Specifically, we consider delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple way of compensating for both sensory and oculomotor delays. The efficacy of this scheme is illustrated using neuronal simulations of pursuit initiation responses, with and without compensation. We then consider an extension of the generative model to simulate smooth pursuit eye movements in which the visuo-oculomotor system believes both the target and its centre of gaze are attracted to a (hidden) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can recognise and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.},
	Annote = {https://arxiv.org/abs/1610.05564},
	Author = {Perrinet, Laurent U and Adams, Rick A and Friston, Karl J},
	Booktitle = {Biological Cybernetics},
	Date = {2014-12-16},
	Date-Modified = {2020-03-31 11:07:29 +0200},
	Doi = {10.1007/s00422-014-0620-8},
	Issn = {1432-0770},
	Journal = {Biological Cybernetics},
	Keywords = {active inference,Bayesian model,Biologically Inspired Computer vision,eye movements,free energy,motion detection},
	Number = {6},
	Pages = {777--801},
	Preprint = {https://hal-amu.archives-ouvertes.fr/hal-01382350},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Active inference, eye movements and oculomotor delays},
	Url = {http://link.springer.com/article/10.1007%2Fs00422-014-0620-8},
	Volume = {108},
	Year = {2014},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-01382350},
	Bdsk-Url-2 = {https://doi.org/10.1007/s00422-014-0620-8}}

@article{Traver10,
	Author = {Javier Traver, V and Bernardino, Alexandre},
	Date-Modified = {2019-02-27 10:10:54 +0100},
	Journal = {Robotics and Autonomous Systems},
	Number = {4},
	Pages = {378--398},
	Publisher = {Elsevier},
	Title = {A review of log-polar imaging for visual perception in robotics},
	Volume = {58},
	Year = {2010}}

@article{Butko2010infomax,
	Author = {Butko, Nicholas J and Movellan, Javier R},
	Journal = {IEEE Transactions on Autonomous Mental Development},
	Number = {2},
	Pages = {91--107},
	Publisher = {IEEE},
	Title = {Infomax control of eye movements},
	Volume = {2},
	Year = {2010}}

@inproceedings{kortum1996implementation,
	Author = {Kortum, Philip and Geisler, Wilson S},
	Booktitle = {Human Vision and Electronic Imaging},
	Pages = {350--360},
	Title = {Implementation of a foveated image coding system for image bandwidth reduction.},
	Volume = {2657},
	Year = {1996}}

@article{dauce2018active,
	Author = {Dauc{\'e}, Emmanuel},
	Journal = {Frontiers in neurorobotics},
	Pages = {76},
	Publisher = {Frontiers},
	Title = {Active Fovea-Based Vision Through Computationally-Effective Model-Based Prediction},
	Volume = {12},
	Year = {2018}}

@inproceedings{mnih2014recurrent,
	Author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
	Booktitle = {Advances in Neural Information Processing Systems 27},
	Editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
	Pages = {2204--2212},
	Publisher = {Curran Associates, Inc.},
	Title = {Recurrent Models of Visual Attention},
	Url = {http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf},
	Year = {2014},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf}}

@inproceedings{fu2017look,
	Author = {Fu, Jianlong and Zheng, Heliang and Mei, Tao},
	Booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Pages = {4438--4446},
	Title = {Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition},
	Year = {2017}}

@article{denil2012learning,
	Author = {Denil, Misha and Bazzani, Loris and Larochelle, Hugo and de Freitas, Nando},
	Journal = {Neural computation},
	Number = {8},
	Pages = {2151--2184},
	Publisher = {MIT Press},
	Title = {Learning where to attend with deep architectures for image tracking},
	Volume = {24},
	Year = {2012}}

@article{mishkin1983object,
	Author = {Mishkin, Mortimer and Ungerleider, Leslie G and Macko, Kathleen A},
	Journal = {Trends in neurosciences},
	Pages = {414--417},
	Publisher = {Elsevier},
	Title = {Object vision and spatial vision: two cortical pathways},
	Volume = {6},
	Year = {1983}}

@article{sparks1987sensory,
	Author = {Sparks, David L and Nelson, Ion S},
	Journal = {Trends in Neurosciences},
	Number = {8},
	Pages = {312--317},
	Publisher = {Elsevier},
	Title = {Sensory and motor maps in the mammalian superior colliculus},
	Volume = {10},
	Year = {1987}}

@article{connolly1984representation,
	Author = {Connolly, Michael and Van Essen, David},
	Journal = {Journal of Comparative Neurology},
	Number = {4},
	Pages = {544--564},
	Publisher = {Wiley Online Library},
	Title = {The representation of the visual field in parvocellular and magnocellular layers of the lateral geniculate nucleus in the macaque monkey},
	Volume = {226},
	Year = {1984}}

@article{kingma2014adam,
	Author = {Kingma, Diederik P and Ba, Jimmy},
	Journal = {arXiv preprint arXiv:1412.6980},
	Title = {Adam: A method for stochastic optimization},
	Year = {2014}}

@article{joel2002actor,
	Author = {Joel, Daphna and Niv, Yael and Ruppin, Eytan},
	Journal = {Neural networks},
	Number = {4-6},
	Pages = {535--547},
	Publisher = {Elsevier},
	Title = {Actor--critic models of the basal ganglia: New anatomical and computational perspectives},
	Volume = {15},
	Year = {2002}}

@book{sutton1998reinforcement,
	Author = {Sutton, Richard S and Barto, Andrew G},
	Publisher = {MIT press Cambridge},
	Title = {Reinforcement learning: An introduction},
	Volume = {1},
	Year = {1998}}

@article{takahashi2008silencing,
	Author = {Takahashi, Yuji and Schoenbaum, Geoffrey and Niv, Yael},
	Journal = {Frontiers in neuroscience},
	Pages = {14},
	Publisher = {Frontiers},
	Title = {Silencing the critics: understanding the effects of cocaine sensitization on dorsolateral and ventral striatum in the context of an actor/critic model},
	Volume = {2},
	Year = {2008}}

@article{simonyan2014very,
	Author = {Simonyan, Karen and Zisserman, Andrew},
	Journal = {arXiv preprint arXiv:1409.1556},
	Title = {Very deep convolutional networks for large-scale image recognition},
	Year = {2014}}

@inproceedings{szegedy2015going,
	Author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	Booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Pages = {1--9},
	Title = {Going deeper with convolutions},
	Year = {2015}}

@article{viola2004robust,
	Author = {Viola, Paul and Jones, Michael J},
	Journal = {International journal of computer vision},
	Number = {2},
	Pages = {137--154},
	Publisher = {Springer},
	Title = {Robust real-time face detection},
	Volume = {57},
	Year = {2004}}

@inproceedings{strengert2006pyramid,
	Author = {Strengert, Magnus and Kraus, Martin and Ertl, Thomas},
	Booktitle = {Vision, modeling, and visualization 2006: proceedings, November 22-24, 2006, Aachen, Germany},
	Isbn = {3898380815},
	Organization = {IOS Press},
	Pages = {169--176},
	Title = {Pyramid methods in GPU-based image processing},
	Volume = {2006},
	Year = {2006}}

@article{feng2019computer,
	Author = {Feng, Xin and Jiang, Youni and Yang, Xuejiao and Du, Ming and Li, Xin},
	Journal = {Integration},
	Pages = {309-320},
	Publisher = {Elsevier},
	Title = {Computer vision algorithms and hardware implementations: A survey},
	Volume = 69,
	Year = {2019}}

@article{akbas2017object,
	Author = {Akbas, Emre and Eckstein, Miguel P},
	Doi = {10.1371/journal.pcbi.1005743},
	Journal = {{PLoS Computational Biology}},
	Number = {10},
	Pages = {e1005743},
	Publisher = {Public Library of Science},
	Title = {Object detection through search with a foveated visual system},
	Volume = {13},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pcbi.1005743}}

@article{Eckstein11,
	Author = {Eckstein, Miguel P.},
	Date = {2011-05-01},
	Doi = {10/fx9zd9},
	Issn = {1534-7362},
	Journaltitle = {Journal of Vision},
	Langid = {english},
	Number = {5},
	Pages = {14--14},
	Publisher = {{The Association for Research in Vision and Ophthalmology}},
	Shortjournal = {Journal of Vision},
	Shorttitle = {Visual Search},
	Title = {Visual search: {A} Retrospective},
	Url = {https://jov.arvojournals.org/article.aspx?articleid=2191835},
	Urldate = {2020-06-22},
	Volume = {11},
	Bdsk-Url-1 = {https://jov.arvojournals.org/article.aspx?articleid=2191835},
	Bdsk-Url-2 = {https://doi.org/10/fx9zd9}}

@article{friston2010free,
	Author = {Friston, Karl},
	Journal = {Nature reviews neuroscience},
	Number = {2},
	Publisher = {Nature publishing group},
	Title = {The free-energy principle: a unified brain theory?},
	Volume = {11},
	Year = {2010}}

@article{watson2014formula,
	Author = {Watson, Andrew B},
	Journal = {Journal of vision},
	Number = {7},
	Pages = {15--15},
	Publisher = {The Association for Research in Vision and Ophthalmology},
	Title = {A formula for human retinal ganglion cell receptive field density as a function of visual field location},
	Volume = {14},
	Year = {2014}}
