%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Laurent Perrinet at 2019-02-27 20:38:47 +0100


%% Saved with string encoding Unicode (UTF-8)


@article{Vo12,
  title = {When Does Repeated Search in Scenes Involve Memory? {{Looking}} at versus Looking for Objects in Scenes},
  volume = {38},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  shorttitle = {When Does Repeated Search in Scenes Involve Memory?},
  doi = {10.1037/a0024147},
  abstract = {One might assume that familiarity with a scene or previous encounters with objects embedded in a scene would benefit subsequent search for those items. However, in a series of experiments we show that this is not the case: When participants were asked to subsequently search for multiple objects in the same scene, search performance remained essentially unchanged over the course of searches despite increasing scene familiarity. Similarly, looking at target objects during previews, which included letter search, 30 seconds of free viewing, or even 30 seconds of memorizing a scene, also did not benefit search for the same objects later on. However, when the same object was searched for again memory for the previous search was capable of producing very substantial speeding of search despite many different intervening searches. This was especially the case when the previous search engagement had been active rather than supported by a cue. While these search benefits speak to the strength of memory-guided search when the same search target is repeated, the lack of memory guidance during initial object searches\textemdash{}despite previous encounters with the target objects\textemdash{}demonstrates the dominance of guidance by generic scene knowledge in real-world search. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {1},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  author = {V\~o, Melissa L.-H. and Wolfe, Jeremy M.},
  year = {2012},
  keywords = {Contextual Associations,Episodic Memory,Visual Perception,Visual Search},
  pages = {23-41},
  file = {/Users/lolo/Zotero/storage/3AUVQLFG/Võ and Wolfe - 2012 - When does repeated search in scenes involve memory.pdf;/Users/lolo/Zotero/storage/DUPQTZQJ/2011-12267-001.html}
}



@article{He15,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.01852},
  primaryClass = {cs},
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  journal = {arXiv:1502.01852 [cs]},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/laurentperrinet/Zotero/storage/VHTT3FKQ/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;/Users/laurentperrinet/Zotero/storage/TZD8MFU5/1502.html},
  note = {04208}
}



@article{Kummerer16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.01563},
  primaryClass = {cs, q-bio, stat},
  title = {{{DeepGaze II}}: Reading Fixations from Deep Features Trained on Object Recognition},
  shorttitle = {{{DeepGaze II}}},
  abstract = {Here we present DeepGaze II, a model that predicts where people look in images. The model uses the features from the VGG-19 deep neural network trained to identify objects in images. Contrary to other saliency models that use deep features, here we use the VGG features for saliency prediction with no additional fine-tuning (rather, a few readout layers are trained on top of the VGG features to predict saliency). The model is therefore a strong test of transfer learning. After conservative cross-validation, DeepGaze II explains about 87\% of the explainable information gain in the patterns of fixations and achieves top performance in area under the curve metrics on the MIT300 hold-out benchmark. These results corroborate the finding from DeepGaze I (which explained 56\% of the explainable information gain), that deep features trained on object recognition provide a versatile feature space for performing related visual tasks. We explore the factors that contribute to this success and present several informative image examples. A web service is available to compute model predictions at http://deepgaze.bethgelab.org.},
  journal = {arXiv:1610.01563 [cs, q-bio, stat]},
  author = {K\"ummerer, Matthias and Wallis, Thomas S. A. and Bethge, Matthias},
  month = oct,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition,Statistics - Applications},
  file = {/Users/laurentperrinet/Zotero/storage/XBMG4DQN/Kümmerer et al. - 2016 - DeepGaze II Reading fixations from deep features .pdf;/Users/laurentperrinet/Zotero/storage/CMPHRVXY/1610.html},
  note = {00075}
}





@article{Treisman80,
  title = {A Feature-Integration Theory of Attention},
  volume = {12},
  doi = {10.1016/0010-0285(80)90005},
  number = {1},
  journal = {Cognitive psychology},
  author = {Treisman, Anne M and Gelade, Garry},
  year = {1980},
  keywords = {****,attention},
  pages = {97--136},
  note = {11957}
}



@article{Samonds18,
  title = {Natural Image and Receptive Field Statistics Predict Saccade Sizes},
  volume = {21},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  issn = {1546-1726},
  doi = {10/gfgt3k},
  abstract = {Saccadic eye movements during free viewing exhibit patterns that reflect a strategy to increase neural responses by matching motor behavior with the statistics of the natural world and with the processing limitations of sensory systems.},
  language = {En},
  number = {11},
  journal = {Nature Neuroscience},
  author = {Samonds, Jason M. and Geisler, Wilson S. and Priebe, Nicholas J.},
  month = nov,
  year = {2018},
  pages = {1591},
  note = {00002}
}



@article{Kirchner06,
  title = {Ultra-Rapid Object Detection with Saccadic Eye Movements: Visual Processing Speed Revisited},
  volume = {46},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2005.10.002},
  abstract = {Previous ultra-rapid go/no-go categorization studies with manual responses have demonstrated the remarkable speed and efficiency with which humans process natural scenes. Using a forced-choice saccade task we show here that when two scenes are simultaneously flashed in the left and right hemifields, human participants can reliably make saccades to the side containing an animal in as little as 120 ms. Low level differences between target and distractor images were unable to account for these exceptionally fast responses. The results suggest a very fast and unexpected route linking visual processing in the ventral stream with the programming of saccadic eye movements.},
  number = {11},
  journal = {Vision Research},
  author = {Kirchner, H and Thorpe, Sj},
  year = {2006},
  keywords = {Visual Pathways,Humans,Psychomotor Performance,Reaction Time,Saccades,Photic Stimulation,Female,Male,Adult,Learning,Learning: physiology,Pattern Recognition,Visual,Visual: physiology,perrinet11sfn,assofield,Ocular,Ocular: physiology,Visual Pathways: physiology,Fixation,Saccades: physiology,Psychomotor Performance: physiology,Electrooculography,Electrooculography: methods,\#nosource},
  pages = {1762--76},
  pmid = {16289663}
}




@article{Itti01,
  title = {Computational Modelling of Visual Attention},
  volume = {2},
  issn = {1471-0048},
  doi = {10/chw2bk},
  abstract = {Five important trends have emerged from recent work on computational models of focal visual attention that emphasize the bottom-up, image-based control of attentional deployment. First, the perceptual saliency of stimuli critically depends on the surrounding context. Second, a unique 'saliency map' that topographically encodes for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Third, inhibition of return, the process by which the currently attended location is prevented from being attended again, is a crucial element of attentional deployment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. And last, scene understanding and object recognition strongly constrain the selection of attended locations. Insights from these five key areas provide a framework for a computational and neurobiological understanding of visual attention.},
  language = {en},
  number = {3},
  journal = {Nature Reviews Neuroscience},
  author = {Itti, Laurent and Koch, Christof},
  month = mar,
  year = {2001},
  pages = {194-203},
  note = {04359}
}




@article{Najemnik05,
  title = {Optimal Eye Movement Strategies in Visual Search},
  volume = {434},
  issn = {1476-4687},
  doi = {10/bcbw2b},
  abstract = {To perform visual search, humans, like many mammals, encode a large field of view with retinas having variable spatial resolution, and then use high-speed eye movements to direct the highest-resolution region, the fovea, towards potential target locations1,2. Good search performance is essential for survival, and hence mammals may have evolved efficient strategies for selecting fixation locations. Here we address two questions: what are the optimal eye movement strategies for a foveated visual system faced with the problem of finding a target in a cluttered environment, and do humans employ optimal eye movement strategies during a search? We derive the ideal bayesian observer3,4,5,6 for search tasks in which a target is embedded at an unknown location within a random background that has the spectral characteristics of natural scenes7. Our ideal searcher uses precise knowledge about the statistics of the scenes in which the target is embedded, and about its own visual system, to make eye movements that gain the most information about target location. We find that humans achieve nearly optimal search performance, even though humans integrate information poorly across fixations8,9,10. Analysis of the ideal searcher reveals that there is little benefit from perfect integration across fixations\textemdash{}much more important is efficient processing of information on each fixation. Apparently, evolution has exploited this fact to achieve efficient eye movement strategies with minimal neural resources devoted to memory.},
  language = {en},
  number = {7031},
  journal = {Nature},
  author = {Najemnik, Jiri and Geisler, Wilson S.},
  month = mar,
  year = {2005},
  pages = {387-391},
  note = {00730}
}



@article{Redmon15,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {03448}
}


@article{Ren17,
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  volume = {39},
  issn = {0162-8828},
  shorttitle = {Faster {{R}}-{{CNN}}},
  doi = {10/gc7rmb},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  number = {6},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Ren, S. and He, K. and Girshick, R. and Sun, J.},
  month = jun,
  year = {2017},
  keywords = {Feature extraction,convolutional neural network,Training,attention mechanisms,COCO 2015 competitions,Convolutional codes,deep VGG-16 model,Detectors,faster-R-CNN,full-image convolutional features,GPU,graphics processing units,high-quality region proposals,ILSVRC,MS COCO datasets,neural nets,object detection,Object detection,object detection accuracy,PASCAL VOC 2007,PASCAL VOC 2012,Proposals,real-time object detection,region proposal,region proposal networks,RPN,Search problems},
  pages = {1137-1149},
  note = {08051}
}


@article{Mirza18,
	title = {Human visual exploration reduces uncertainty about the sensed world},
	volume = {13},
	issn = {1932-6203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/29304087},
	doi = {10.1371/journal.pone.0190429},
	abstract = {In previous papers, we introduced a normative scheme for scene construction and epistemic (visual) searches based upon active inference. This scheme provides a principled account of how people decide where to look, when categorising a visual scene based on its contents. In this paper, we use active inference to explain the visual searches of normal human subjects; enabling us to answer some key questions about visual foraging and salience attribution. First, we asked whether there is any evidence for 'epistemic foraging'; i.e. exploration that resolves uncertainty about a scene. In brief, we used Bayesian model comparison to compare Markov decision process (MDP) models of scan-paths that did-and did not-contain the epistemic, uncertainty-resolving imperatives for action selection. In the course of this model comparison, we discovered that it was necessary to include non-epistemic (heuristic) policies to explain observed behaviour (e.g., a reading-like strategy that involved scanning from left to right). Despite this use of heuristic policies, model comparison showed that there is substantial evidence for epistemic foraging in the visual exploration of even simple scenes. Second, we compared MDP models that did-and did not-allow for changes in prior expectations over successive blocks of the visual search paradigm. We found that implicit prior beliefs about the speed and accuracy of visual searches changed systematically with experience. Finally, we characterised intersubject variability in terms of subject-specific prior beliefs. Specifically, we used canonical correlation analysis to see if there were any mixtures of prior expectations that could predict between-subject differences in performance; thereby establishing a quantitative link between different behavioural phenotypes and Bayesian belief updating. We demonstrated that better scene categorisation performance is consistently associated with lower reliance on heuristics; i.e., a greater use of a generative model of the scene to direct its exploration.},
	number = {1},
	journal = {PLOS ONE},
	author = {Mirza, M. Berk and Adams, Rick A. and Mathys, Christoph and Friston, Karl J.},
	editor = {Kiebel, Stefan},
	month = jan,
	year = {2018},
	pmid = {29304087},
	pages = {e0190429}
}

@article{Friston12,
    abstract = {If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organized behavior: namely, the imperative to minimize the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free-energy minimization. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or assimilated into beliefs about the world.},
    author = {Friston, Karl J and Adams, Rick A. and Perrinet, Laurent U and Breakspear, Michael},
    bdsk-url-1 = {http://dx.doi.org/10.3389/fpsyg.2012.00151},
    date = {2012},
    date-modified = {2019-02-24 23:49:43 +0100},
    doi = {10.3389/fpsyg.2012.00151},
    issn = {1664-1078},
    journal = {Frontiers in Psychology},
    keywords = {free energy,active inference,Bayesian model,eye movements},
    title = {Perceptions as Hypotheses: Saccades as Experiments},
    url = {http://dx.doi.org/10.3389/fpsyg.2012.00151},
    volume = {3},
    year = {2012}
}

@article{Traver10,
	Author = {Javier Traver, V and Bernardino, Alexandre},
	Date-Modified = {2019-02-27 10:10:54 +0100},
	Journal = {Robotics and Autonomous Systems},
	Number = {4},
	Pages = {378--398},
	Publisher = {Elsevier},
	Title = {A review of log-polar imaging for visual perception in robotics},
	Volume = {58},
	Year = {2010}}
