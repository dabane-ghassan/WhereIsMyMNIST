% !TEX root = paper.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US


@article{Bahill75,
  title = {The Main Sequence, a Tool for Studying Human Eye Movements},
  author = {Bahill, A. Terry and Clark, Michael R. and Stark, Lawrence},
  date = {1975-01-01},
  journaltitle = {Mathematical Biosciences},
  journal = {Mathematical Biosciences},
  volume = {24},
  pages = {191--204},
  issn = {0025-5564},
  doi = {10/b655hq},
  url = {http://www.sciencedirect.com/science/article/pii/0025556475900759},
  urldate = {2020-01-14},
  abstract = {The astronomical term “main sequence” has been applied to the relationships between duration, peak velocity, and magnitude of human saccades over a thousandfold range of magnitude. Infrared photodiodes aimed at the iris-sclera border and a digital computer were used in experiments to derive the main sequence curves. In the pulse width modulation model, the duration of the controller signal pulse determines saccadic amplitude and peak velocity. The high-frequency burst of the oculomotoneurons needs to be only one-half the duration of the saccade, because of the “apparent inertia” of the eyeball.},
  file = {/Users/laurentperrinet/Zotero/storage/RJJ469GV/Bahill et al. - 1975 - The main sequence, a tool for studying human eye m.pdf;/Users/laurentperrinet/Zotero/storage/F49PMD68/0025556475900759.html},
  langid = {english},
  note = {01092},
  number = {3}
}



@article{Strasburger11,
  title = {Peripheral Vision and Pattern Recognition: {{A}} Review},
  shorttitle = {Peripheral Vision and Pattern Recognition},
  author = {Strasburger, Hans and Rentschler, Ingo and Jüttner, Martin},
  date = {2011-05-01},
  journaltitle = {Journal of Vision},
  journal = {Journal of Vision},
  volume = {11},
  pages = {13--13},
  issn = {1534-7362},
  doi = {10/fx28dx},
  url = {https://jov.arvojournals.org/article.aspx?articleid=2191825},
  urldate = {2020-01-14},
  file = {/Users/laurentperrinet/Zotero/storage/UAHQ53WB/Strasburger et al. - 2011 - Peripheral vision and pattern recognition A revie.pdf;/Users/laurentperrinet/Zotero/storage/VCRPMPSZ/article.html},
  langid = {english},
  note = {00489},
  number = {5}
}




@article{Fischer2007a,
  title = {Self-{{Invertible 2D Log}}-{{Gabor Wavelets}}},
  volume = {75},
  issn = {0920-5691},
  url = {http://link.springer.com/10.1007/s11263-006-0026-8},
  doi = {10.1007/s11263-006-0026-8},
  abstract = {Abstract— Meanwhile biorthogonal wavelets got a very popu- lar image processing tool, alternative multiresolution transforms have been proposed for solving some of their drawbacks, namely the poor selectivity in orientation and the lack of translation in- variance due to the aliasing between subbands. These transforms are generally overcomplete and consequently offer huge degrees of freedom in their design. At the same time their optimization get a challenging task. We proposed here a log-Gabor wavelet transform gathering the excellent mathematical properties of the Gabor functions with a carefully construction to maintain the properties of the filters and to permit exact reconstruction. Two major improvements are proposed: first the highest frequency bands are covered by narrowly localized oriented filters. And second, all the frequency bands including the highest and lowest frequencies are uniformly covered so as exact reconstruction is achieved using the same filters in both the direct and the inverse transforms (which means that the transform is self-invertible). The transform is optimized not only mathematically but it also follows as much as possible the knowledge on the receptive field of the simple cells of the Primary Visual Cortex (V1) of primates and on the statistics of natural images. Compared to the state of the art, the log-Gabor wavelets show excellent behavior in their ability to segregate the image information (e.g. the contrast edges) from incoherent Gaussian noise by hard thresholding and to code the image features through a reduced set of coefficients with large magnitude. Such characteristics make the transform a promising tool for general image processing tasks.},
  number = {2},
  journal = {International Journal of Computer Vision},
  year = {2007},
  date = {2007-01},
  pages = {231--246},
  keywords = {bicv-sparse,Wavelet transforms,motion_clouds,motion-clouds,sanz12jnp,vision,vacher14,image denoising,Image denoising,log-gabor,log-Gabor filters,Log-Gabor filters,oriented high-pass filters,Oriented high-pass filters,perrinet11sfn,visual system,Visual system,wavelet transforms,wavelets,assofield,denoising,filters,high-pass,image,oriented,system,transforms,wavelet},
  author = {Fischer, Sylvain and Šroubek, Filip and Perrinet, Laurent and Redondo, Rafael and Cristóbal, Gabriel},
  file = {/Users/laurentperrinet/Zotero/storage/ES4G7AYA/Fischer et al. - 2007 - Self-Invertible 2D Log-Gabor Wavelets.pdf}
}




@article{Sanz12,
  title = {Motion Clouds: Model-Based Stimulus Synthesis of Natural-like Random Textures for the Study of Motion Perception},
  volume = {107},
  issn = {1522-1598},
  url = {http://dx.doi.org/10.1152/jn.00737.2011},
  doi = {10.1152/jn.00737.2011},
  abstract = {Choosing an appropriate set of stimuli is essential to characterize the response of a sensory system to a particular functional dimension, such as the eye movement following the motion of a visual scene. Here, we describe a framework to generate random texture movies with controlled information content, i.e., Motion Clouds. These stimuli are defined using a generative model that is based on controlled experimental parametrization. We show that Motion Clouds correspond to dense mixing of localized moving gratings with random positions. Their global envelope is similar to natural-like stimulation with an approximate full-field translation corresponding to a retinal slip. We describe the construction of these stimuli mathematically and propose an open-source Python-based implementation. Examples of the use of this framework are shown. We also propose extensions to other modalities such as color vision, touch, and audition.},
  number = {11},
  journal = {Journal of Neurophysiology},
  date = {2012-03},
  year = {2012},
  pages = {3217--3226},
  keywords = {anr-trax,bicv-sparse,perrinetadamsfriston14,Natural scenes,Motion detection,motion-clouds,sanz12jnp,vacher14,Eye movements,kaplan13,Low-level sensory systems,Optimal stimulation,Python},
  author = {Sanz-Leon, Paula and Vanzetta, I. and Masson, G.S. S. and Perrinet, L.U. U. and Leon, P.S. S. and Vanzetta, I. and Masson, G.S. S. and Perrinet, L.U. U. and Sanz-Leon, Paula and Vanzetta, I. and Masson, G.S. S. and Perrinet, L.U. U.},
  file = {/Users/laurentperrinet/Zotero/storage/3UYQZ97E/Sanz-Leon et al. - 2012 - Motion clouds model-based stimulus synthesis of natural-like random textures for the study of motion percep(2).pdf}}



@article{Paszke17,
  title = {Automatic Differentiation in {{PyTorch}}},
  url = {https://openreview.net/forum?id=BJJsrmfCZ},
  abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua...},
  urldate = {2019-03-18},
  date = {2017-10-28},
  year = {2017},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  journal = {preprint},
  file = {/Users/laurentperrinet/Zotero/storage/I7KADJXH/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf;/Users/laurentperrinet/Zotero/storage/NJ7IU2ZL/forum.html},
  note = {00918}
}




@article{Lecun1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  volume = {86},
  issn = {0018-9219},
  doi = {10/d89c25},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  number = {11},
  journal = {Proceedings of the IEEE},
  date = {1998-11},
  year = {1998},
  pages = {2278-2324},
  keywords = {Feature extraction,convolution,Neural networks,Machine learning,2D shape variability,back-propagation,backpropagation,Character recognition,cheque reading,complex decision surface synthesis,convolutional neural network character recognizers,document recognition,document recognition systems,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,GTN,handwritten character recognition,handwritten digit recognition task,Hidden Markov models,high-dimensional patterns,language modeling,Multi-layer neural network,multilayer neural networks,multilayer perceptrons,multimodule systems,optical character recognition,Optical character recognition software,Optical computing,Pattern recognition,performance measure minimization,Principal component analysis,segmentation recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  file = {/Users/laurentperrinet/Zotero/storage/2RD3LVL6/726791.html},
  note = {14279}
}




@article{Dauce18,
	Abstract = {What motivates the action in the absence of a definite reward? Taking the case of visuomotor control, we consider a minimal control problem that is how select the next saccade, in a sequence of discrete eye movements, when the final objective is to better interpret the current visual scene. The visual scene is modeled here as a partially-observed environment, with a generative model explaining how the visual data is shaped by action. This allows to interpret different action selection metrics proposed in the literature, including the Salience, the Infomax and the Variational Free Energy, under a single information theoretic construct, namely the view-based Information Gain. Pursuing this analytic track, two original action selection metrics named the Information Gain Lower Bound (IGLB) and the Information Gain Upper Bound (IGUB) are then proposed. Showing either a conservative or an optimistic bias regarding the Information Gain, they strongly simplify its calculation. An original fovea-based visual scene decoding setup is then proposed, with numerical experiments highlighting different facets of artificial fovea-based vision. A first and principal result is that state-of-the-art recognition rates are obtained with fovea-based saccadic exploration, using less than 10\% of the original image's data. Those satisfactory results illustrate the advantage of mixing predictive control with accurate state-of-the-art predictors, namely a deep neural network. A second result is the sub-optimality of some classical action-selection metrics widely used in literature, that is not manifest with finely-tuned inference models, but becomes patent when coarse or faulty models are used. Last, a computationally-effective predictive model is developed using the IGLB objective, with pre-processed visual scan-path read-out from memory, bypassing computationally-demanding predictive calculations. This last simplified setting is shown effective in our case, showing both a competing accuracy and a good robustness to model flaws.},
	Author = {Dauc\'e, Emmanuel},
	Date-Added = {2019-03-08 16:12:29 +0100},
	Date-Modified = {2019-03-08 16:37:30 +0100},
	Doi = {10/gfrhgj},
	File = {/Users/laurentperrinet/Zotero/storage/QUYVDZTT/Dauc{\'e} - 2018 - Active Fovea-Based Vision Through Computationally-.pdf;/Users/laurentperrinet/Zotero/storage/VS9U4UNG/Dauc{\'e} - 2018 - Active Fovea-Based Vision Through Computationally-.pdf},
	Issn = {1662-5218},
	Journal = {Frontiers in Neurorobotics},
	Keywords = {active inference,active vision,Convolutional Neural Networks (CNN),End-effector control,Foveated vision,information gain,intrinsic motivation,Saccadic eye movements,visual scene interpretation},
	Title = {Active Fovea-Based Vision Through Computationally-Effective Model-Based Prediction},
	Volume = {12},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10/gfrhgj}}

@article{Sandler18,
	Abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	Archiveprefix = {arXiv},
	Author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	Date-Added = {2019-03-08 14:02:48 +0100},
	Date-Modified = {2019-03-08 14:02:48 +0100},
	Journal = {arXiv:1801.04381 [cs]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = jan,
	Note = {00241},
	Shorttitle = {{{MobileNetV2}}},
	Title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
	Year = {2018}}

@article{Vo12,
	Abstract = {One might assume that familiarity with a scene or previous encounters with objects embedded in a scene would benefit subsequent search for those items. However, in a series of experiments we show that this is not the case: When participants were asked to subsequently search for multiple objects in the same scene, search performance remained essentially unchanged over the course of searches despite increasing scene familiarity. Similarly, looking at target objects during previews, which included letter search, 30 seconds of free viewing, or even 30 seconds of memorizing a scene, also did not benefit search for the same objects later on. However, when the same object was searched for again memory for the previous search was capable of producing very substantial speeding of search despite many different intervening searches. This was especially the case when the previous search engagement had been active rather than supported by a cue. While these search benefits speak to the strength of memory-guided search when the same search target is repeated, the lack of memory guidance during initial object searches\textemdash{}despite previous encounters with the target objects\textemdash{}demonstrates the dominance of guidance by generic scene knowledge in real-world search. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	Author = {V\~o, Melissa L.-H. and Wolfe, Jeremy M.},
	Doi = {10.1037/a0024147},
	File = {/Users/lolo/Zotero/storage/3AUVQLFG/V{\~o} and Wolfe - 2012 - When does repeated search in scenes involve memory.pdf;/Users/lolo/Zotero/storage/DUPQTZQJ/2011-12267-001.html},
	Issn = {1939-1277(Electronic),0096-1523(Print)},
	Journal = {Journal of Experimental Psychology: Human Perception and Performance},
	Keywords = {Contextual Associations,Episodic Memory,Visual Perception,Visual Search},
	Number = {1},
	Pages = {23-41},
	Shorttitle = {When Does Repeated Search in Scenes Involve Memory?},
	Title = {When Does Repeated Search in Scenes Involve Memory? {{Looking}} at versus Looking for Objects in Scenes},
	Volume = {38},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1037/a0024147}}

@article{He15,
	Abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	Author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	File = {/Users/laurentperrinet/Zotero/storage/VHTT3FKQ/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;/Users/laurentperrinet/Zotero/storage/TZD8MFU5/1502.html},
	Journal = {arXiv:1502.01852 [cs]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
	Month = feb,
	Note = {04208},
	Shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
	Title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
	Year = {2015}}

@article{Kummerer16,
	Abstract = {Here we present DeepGaze II, a model that predicts where people look in images. The model uses the features from the VGG-19 deep neural network trained to identify objects in images. Contrary to other saliency models that use deep features, here we use the VGG features for saliency prediction with no additional fine-tuning (rather, a few readout layers are trained on top of the VGG features to predict saliency). The model is therefore a strong test of transfer learning. After conservative cross-validation, DeepGaze II explains about 87\% of the explainable information gain in the patterns of fixations and achieves top performance in area under the curve metrics on the MIT300 hold-out benchmark. These results corroborate the finding from DeepGaze I (which explained 56\% of the explainable information gain), that deep features trained on object recognition provide a versatile feature space for performing related visual tasks. We explore the factors that contribute to this success and present several informative image examples. A web service is available to compute model predictions at http://deepgaze.bethgelab.org.},
	Archiveprefix = {arXiv},
	Author = {K\"ummerer, Matthias and Wallis, Thomas S. A. and Bethge, Matthias},
	Eprint = {1610.01563},
	Eprinttype = {arxiv},
	File = {/Users/laurentperrinet/Zotero/storage/XBMG4DQN/K{\"u}mmerer et al. - 2016 - DeepGaze II Reading fixations from deep features .pdf;/Users/laurentperrinet/Zotero/storage/CMPHRVXY/1610.html},
	Journal = {arXiv:1610.01563 [cs, q-bio, stat]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition,Statistics - Applications},
	Month = oct,
	Note = {00075},
	Primaryclass = {cs, q-bio, stat},
	Shorttitle = {{{DeepGaze II}}},
	Title = {{{DeepGaze II}}: Reading Fixations from Deep Features Trained on Object Recognition},
	Year = {2016}}

@article{Treisman80,
	Author = {Treisman, Anne M and Gelade, Garry},
	Doi = {10.1016/0010-0285(80)90005},
	Journal = {Cognitive psychology},
	Keywords = {****,attention},
	Note = {11957},
	Number = {1},
	Pages = {97--136},
	Title = {A Feature-Integration Theory of Attention},
	Volume = {12},
	Year = {1980},
	Bdsk-Url-1 = {https://doi.org/10.1016/0010-0285(80)90005}}

@article{Samonds18,
	Abstract = {Saccadic eye movements during free viewing exhibit patterns that reflect a strategy to increase neural responses by matching motor behavior with the statistics of the natural world and with the processing limitations of sensory systems.},
	Author = {Samonds, Jason M. and Geisler, Wilson S. and Priebe, Nicholas J.},
	Copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	Doi = {10/gfgt3k},
	Issn = {1546-1726},
	Journal = {Nature Neuroscience},
	Language = {En},
	Month = nov,
	Note = {00002},
	Number = {11},
	Pages = {1591},
	Title = {Natural Image and Receptive Field Statistics Predict Saccade Sizes},
	Volume = {21},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10/gfgt3k}}

@article{Kirchner06,
	Abstract = {Previous ultra-rapid go/no-go categorization studies with manual responses have demonstrated the remarkable speed and efficiency with which humans process natural scenes. Using a forced-choice saccade task we show here that when two scenes are simultaneously flashed in the left and right hemifields, human participants can reliably make saccades to the side containing an animal in as little as 120 ms. Low level differences between target and distractor images were unable to account for these exceptionally fast responses. The results suggest a very fast and unexpected route linking visual processing in the ventral stream with the programming of saccadic eye movements.},
	Author = {Kirchner, H and Thorpe, Sj},
	Doi = {10.1016/j.visres.2005.10.002},
	Issn = {0042-6989},
	Journal = {Vision Research},
	Keywords = {Visual Pathways,Humans,Psychomotor Performance,Reaction Time,Saccades,Photic Stimulation,Female,Male,Adult,Learning,Learning: physiology,Pattern Recognition,Visual,Visual: physiology,perrinet11sfn,assofield,Ocular,Ocular: physiology,Visual Pathways: physiology,Fixation,Saccades: physiology,Psychomotor Performance: physiology,Electrooculography,Electrooculography: methods,\#nosource},
	Number = {11},
	Pages = {1762--76},
	Pmid = {16289663},
	Title = {Ultra-Rapid Object Detection with Saccadic Eye Movements: Visual Processing Speed Revisited},
	Volume = {46},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.visres.2005.10.002}}

@article{Itti01,
	Abstract = {Five important trends have emerged from recent work on computational models of focal visual attention that emphasize the bottom-up, image-based control of attentional deployment. First, the perceptual saliency of stimuli critically depends on the surrounding context. Second, a unique 'saliency map' that topographically encodes for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Third, inhibition of return, the process by which the currently attended location is prevented from being attended again, is a crucial element of attentional deployment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. And last, scene understanding and object recognition strongly constrain the selection of attended locations. Insights from these five key areas provide a framework for a computational and neurobiological understanding of visual attention.},
	Author = {Itti, Laurent and Koch, Christof},
	Date-Modified = {2019-03-08 16:38:23 +0100},
	Doi = {10/chw2bk},
	Issn = {1471-0048},
	Journal = {Nature Reviews Neuroscience},
	Number = {3},
	Pages = {194-203},
	Title = {Computational Modelling of Visual Attention},
	Volume = {2},
	Year = {2001},
	Bdsk-Url-1 = {https://doi.org/10/chw2bk}}

@article{Najemnik05,
	Abstract = {To perform visual search, humans, like many mammals, encode a large field of view with retinas having variable spatial resolution, and then use high-speed eye movements to direct the highest-resolution region, the fovea, towards potential target locations1,2. Good search performance is essential for survival, and hence mammals may have evolved efficient strategies for selecting fixation locations. Here we address two questions: what are the optimal eye movement strategies for a foveated visual system faced with the problem of finding a target in a cluttered environment, and do humans employ optimal eye movement strategies during a search? We derive the ideal bayesian observer3,4,5,6 for search tasks in which a target is embedded at an unknown location within a random background that has the spectral characteristics of natural scenes7. Our ideal searcher uses precise knowledge about the statistics of the scenes in which the target is embedded, and about its own visual system, to make eye movements that gain the most information about target location. We find that humans achieve nearly optimal search performance, even though humans integrate information poorly across fixations8,9,10. Analysis of the ideal searcher reveals that there is little benefit from perfect integration across fixations\textemdash{}much more important is efficient processing of information on each fixation. Apparently, evolution has exploited this fact to achieve efficient eye movement strategies with minimal neural resources devoted to memory.},
	Author = {Najemnik, Jiri and Geisler, Wilson S.},
	Date-Modified = {2019-03-08 16:36:32 +0100},
	Doi = {10/bcbw2b},
	Journal = {Nature},
	Number = {7031},
	Pages = {387-391},
	Title = {Optimal Eye Movement Strategies in Visual Search},
	Volume = {434},
	Year = {2005},
	Bdsk-Url-1 = {https://doi.org/10/bcbw2b}}

@article{Redmon15,
	Abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	Author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = jun,
	Note = {03448},
    journal = {arxiv},
	Title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
	Year = {2015}}

@article{Ren17,
	Abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	Author = {Ren, S. and He, K. and Girshick, R. and Sun, J.},
	Doi = {10/gc7rmb},
	Issn = {0162-8828},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Keywords = {Feature extraction,convolutional neural network,Training,attention mechanisms,COCO 2015 competitions,Convolutional codes,deep VGG-16 model,Detectors,faster-R-CNN,full-image convolutional features,GPU,graphics processing units,high-quality region proposals,ILSVRC,MS COCO datasets,neural nets,object detection,Object detection,object detection accuracy,PASCAL VOC 2007,PASCAL VOC 2012,Proposals,real-time object detection,region proposal,region proposal networks,RPN,Search problems},
	Month = jun,
	Note = {08051},
	Number = {6},
	Pages = {1137-1149},
	Shorttitle = {Faster {{R}}-{{CNN}}},
	Title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
	Volume = {39},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10/gc7rmb}}

@article{Mirza18,
	Abstract = {In previous papers, we introduced a normative scheme for scene construction and epistemic (visual) searches based upon active inference. This scheme provides a principled account of how people decide where to look, when categorising a visual scene based on its contents. In this paper, we use active inference to explain the visual searches of normal human subjects; enabling us to answer some key questions about visual foraging and salience attribution. First, we asked whether there is any evidence for 'epistemic foraging'; i.e. exploration that resolves uncertainty about a scene. In brief, we used Bayesian model comparison to compare Markov decision process (MDP) models of scan-paths that did-and did not-contain the epistemic, uncertainty-resolving imperatives for action selection. In the course of this model comparison, we discovered that it was necessary to include non-epistemic (heuristic) policies to explain observed behaviour (e.g., a reading-like strategy that involved scanning from left to right). Despite this use of heuristic policies, model comparison showed that there is substantial evidence for epistemic foraging in the visual exploration of even simple scenes. Second, we compared MDP models that did-and did not-allow for changes in prior expectations over successive blocks of the visual search paradigm. We found that implicit prior beliefs about the speed and accuracy of visual searches changed systematically with experience. Finally, we characterised intersubject variability in terms of subject-specific prior beliefs. Specifically, we used canonical correlation analysis to see if there were any mixtures of prior expectations that could predict between-subject differences in performance; thereby establishing a quantitative link between different behavioural phenotypes and Bayesian belief updating. We demonstrated that better scene categorisation performance is consistently associated with lower reliance on heuristics; i.e., a greater use of a generative model of the scene to direct its exploration.},
	Author = {Mirza, M. Berk and Adams, Rick A. and Mathys, Christoph and Friston, Karl J.},
	Doi = {10.1371/journal.pone.0190429},
	Editor = {Kiebel, Stefan},
	Issn = {1932-6203},
	Journal = {PLOS ONE},
	Month = jan,
	Number = {1},
	Pages = {e0190429},
	Pmid = {29304087},
	Title = {Human visual exploration reduces uncertainty about the sensed world},
	Url = {http://www.ncbi.nlm.nih.gov/pubmed/29304087},
	Volume = {13},
	Year = {2018},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pubmed/29304087},
	Bdsk-Url-2 = {https://doi.org/10.1371/journal.pone.0190429}}

@article{Friston12,
	Abstract = {If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organized behavior: namely, the imperative to minimize the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free-energy minimization. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or assimilated into beliefs about the world.},
	Author = {Friston, Karl J and Adams, Rick A. and Perrinet, Laurent U and Breakspear, Michael},
	Date-Modified = {2019-02-24 23:49:43 +0100},
	Doi = {10.3389/fpsyg.2012.00151},
	Issn = {1664-1078},
	Journal = {Frontiers in Psychology},
	Keywords = {free energy,active inference,Bayesian model,eye movements},
	Title = {Perceptions as Hypotheses: Saccades as Experiments},
	Url = {http://dx.doi.org/10.3389/fpsyg.2012.00151},
	Volume = {3},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.3389/fpsyg.2012.00151}}

@article{Traver10,
	Author = {Javier Traver, V and Bernardino, Alexandre},
	Date-Modified = {2019-02-27 10:10:54 +0100},
	Journal = {Robotics and Autonomous Systems},
	Number = {4},
	Pages = {378--398},
	Publisher = {Elsevier},
	Title = {A review of log-polar imaging for visual perception in robotics},
	Volume = {58},
	Year = {2010}}

@article{Butko2010infomax,
	title={Infomax control of eye movements},
	author={Butko, Nicholas J and Movellan, Javier R},
	journal={IEEE Transactions on Autonomous Mental Development},
	volume={2},
	number={2},
	pages={91--107},
	year={2010},
	publisher={IEEE}
}

@inproceedings{kortum1996implementation,
	title={Implementation of a foveated image coding system for image bandwidth reduction.},
	author={Kortum, Philip and Geisler, Wilson S},
	booktitle={Human Vision and Electronic Imaging},
	volume={2657},
	pages={350--360},
	year={1996}
}

@article{dauce2018active,
	title={Active Fovea-Based Vision Through Computationally-Effective Model-Based Prediction},
	author={Dauc{\'e}, Emmanuel},
	journal={Frontiers in neurorobotics},
	volume={12},
	pages={76},
	year={2018},
	publisher={Frontiers}
}

@article{fischer2007self,
	title={Self-invertible 2D log-Gabor wavelets},
	author={Fischer, Sylvain and {\v{S}}roubek, Filip and Perrinet, Laurent and Redondo, Rafael and Crist{\'o}bal, Gabriel},
	journal={International Journal of Computer Vision},
	volume={75},
	number={2},
	pages={231--246},
	year={2007},
	publisher={Springer}
}

@inproceedings{mnih2014recurrent,
	title={Recurrent models of visual attention},
	author={Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and others},
	booktitle={Advances in neural information processing systems},
	pages={2204--2212},
	year={2014}
}
@inproceedings{fu2017look,
	title={Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition},
	author={Fu, Jianlong and Zheng, Heliang and Mei, Tao},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={4438--4446},
	year={2017}
}

@article{denil2012learning,
	title={Learning where to attend with deep architectures for image tracking},
	author={Denil, Misha and Bazzani, Loris and Larochelle, Hugo and de Freitas, Nando},
	journal={Neural computation},
	volume={24},
	number={8},
	pages={2151--2184},
	year={2012},
	publisher={MIT Press}
}

@article{mishkin1983object,
	title={Object vision and spatial vision: two cortical pathways},
	author={Mishkin, Mortimer and Ungerleider, Leslie G and Macko, Kathleen A},
	journal={Trends in neurosciences},
	volume={6},
	pages={414--417},
	year={1983},
	publisher={Elsevier}
}

@article{sparks1987sensory,
	title={Sensory and motor maps in the mammalian superior colliculus},
	author={Sparks, David L and Nelson, Ion S},
	journal={Trends in Neurosciences},
	volume={10},
	number={8},
	pages={312--317},
	year={1987},
	publisher={Elsevier}
}

@article{connolly1984representation,
	title={The representation of the visual field in parvicellular and magnocellular layers of the lateral geniculate nucleus in the macaque monkey},
	author={Connolly, Michael and Van Essen, David},
	journal={Journal of Comparative Neurology},
	volume={226},
	number={4},
	pages={544--564},
	year={1984},
	publisher={Wiley Online Library}
}

@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@article{joel2002actor,
	title={Actor--critic models of the basal ganglia: New anatomical and computational perspectives},
	author={Joel, Daphna and Niv, Yael and Ruppin, Eytan},
	journal={Neural networks},
	volume={15},
	number={4-6},
	pages={535--547},
	year={2002},
	publisher={Elsevier}
}

@book{sutton1998reinforcement,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	volume={1},
	year={1998},
	publisher={MIT press Cambridge}
}

@article{takahashi2008silencing,
	title={Silencing the critics: understanding the effects of cocaine sensitization on dorsolateral and ventral striatum in the context of an actor/critic model},
	author={Takahashi, Yuji and Schoenbaum, Geoffrey and Niv, Yael},
	journal={Frontiers in neuroscience},
	volume={2},
	pages={14},
	year={2008},
	publisher={Frontiers}
}

@article{simonyan2014very,
	title={Very deep convolutional networks for large-scale image recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={arXiv preprint arXiv:1409.1556},
	year={2014}
}

@inproceedings{szegedy2015going,
	title={Going deeper with convolutions},
	author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={1--9},
	year={2015}
}
