% !TEX root = DauceAlbigesPerrinet2020.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US
\section{Setup}
\label{sec:principles}
%=================================================================
%=================================================================
\subsection{Experimental design}
In order to implement our visual processing setup, we provide a simplified visual environment toward which a visual agent can act on. This visual search task is formalized and simplified in a way reminiscent to classical psychophysical experimentation: an observer is asked to classify digits (for instance as taken from the MNIST dataset, as introduced by~\cite{Lecun1998}) as they are shown with a given size on a computer display. However, these digits can be placed at random positions on the display, and visual clutter is added as a background to the image (see Figure~\ref{fig:intro}-A). In order to vary the difficulty of the task, different parameters are controlled, such as the target eccentricity, the background noise period and and the signal/noise ratio (SNR).
The agent initially fixates the center of the screen. Due to the peripheral clutter, he needs to explore the visual scene through saccades to provide the answer. He controls a foveal visual sensor that can move over the visual scene through saccades (see Figure~\ref{fig:intro}-B). When a saccade is actuated, the center of fixation moves toward a new location, which updates the visual input (see Figure~\ref{fig:intro}-C).
The lower the SNR and the larger the initial target eccentricity, the more difficult the identification. There is a range of eccentricities for which it is impossible to identify the target from a single glance, so that a saccade is necessary to reduce the relative eccentricity and issue a proper response. This setup implies also that the position of the object may be detected in the peripheral clutter \emph{before} being properly identified.

This setup provides the conditions for a separate processing of the visual information. On the one side, the detailed information present at the center of fixation needs to be analyzed to provide specific environmental cues. On the other side, the full visual field, i.e. mainly the low resolution part surrounding the fovea, needs to be processed in order to identify regions of interest that deserve fixation. This basically means making a choice of “what’s interesting next”. The actual content of putative peripheral locations does not need to be known in advance, but it needs to look interesting enough, and of course to be reachable by a saccade. This is reminiscent of the What/Where visual processing separation observed in primates' ventral and dorsal visual pathways~\cite{mishkin1983object}.

\begin{figure}[t!]%%[p!]
	\centering{\includegraphics[width=\linewidth]{fig_methods}}
	\caption{%
		{\bf Computational graph}. Based on the general anatomy of the visual pathways, we define two streams of information, one stream for identifying a target (``What''?), the other for categorizing it in visual space (``Where''?).
		\A The visual display is constructed the following way: first a $128\times 128$ natural-like background noise is generated, characterized by noise contrast, mean spatial frequency and bandwidth~\cite{Sanz12}. Then, a sample digit is selected from the MNIST dataset (of size $28\times 28$), rectified, multiplied by a contrast factor and overlaid on the background at a random position (see another example in Figure~\ref{fig:intro}-A, \DIS ). Last, a circular mask is put on. %
		\B The visual display is then transformed in 2 sensory inputs: (i)~A $28\times 28$ central foveal-like snippet is fed to a classification network (``What'' pathway). (ii)~A log-polar set of oriented visual features is fed to the ``Where'' pathway. This log-polar input is generated by a bank of filters whose centers are positioned on a log-polar grid and whose radius increases proportionally with the eccentricity. % by processing the periphery with a log-polar encoding
		\C The ``What'' network is implemented using the three-layered LeNet neural network~\cite{Lecun1998}. This network outputs a vector predicting the accuracy of detecting the correct digit. In parallel, the ``Where'' network is implemented by a three-layered neural network consisting of the retinal log-polar input, two hidden layers (fully-connected linear layers combined with a ReLU non-linearity) with $1000$ units each and a collicular-like accuracy map at the output. This map has a similar log-polar (retinotopic) organization and predicts the accuracy at the hypothetical position of a saccade. Both network learn to associate the output with the ground truth through back-propagation.
		\D
		For a given display, the network generates two sensory inputs and provides two accuracy outputs. The two streams converge toward a decision layer that compares the central and the peripheral accuracy which are predicted by both pathways, in order to decide whether to issue a saccadic or a categorical response. If the predicted accuracy in the output of the ``What'' network is higher than that predicted in the ``Where'' network,  we interrupt the visual search and classify the foveal image using the ``What'' pathway such as to give the answer~(\ANS ).  In the other case, the position of maximal activity in the ``Where'' pathway serves to generate a saccade which shifts the center of gaze. For each saccade realized, the center of vision is displaced and the process is repeated.%
		\label{fig:methods}}%
\end{figure}%
%%------------------------------%
%
\subsection{Computational implementation}
%
In order to learn the task, a simple ``Deep Learning'' framework is sufficient to demonstrate the effectiveness of a neural network through the success of the training. More specifically, the modern parametric classifiers are composed of many layers (hence the terminology) that can be trained through gradient descent over arbitrary input and output feature spaces. For our specific problem, the anatomy of the agent is made of two separate pathways for which a different processing is realized by two different neural networks (see Figure~\ref{fig:methods}). The proposed computational architecture is connected in a closed-loop fashion to the visual environment, with the capacity to produce saccades whose effect is to shift the visual field from one visual position to another. %Crucially, the processing of the visual field is done through distinct pathways, each pathway being assumed to rely on different morphologies.
By analogy with biological vision, the target identification is assumed to rely on the very central part of the retina (the fovea), that comes with higher density of cones, and thus higher spatial precision. In contrast, the saccade planning should rely on the full visual field, with peripheral regions having a lower density of sensors and thus a lower sensitivity to high spatial frequencies.

In a stationary condition, where the target's position and identity do not change over time, each saccade thus provides a new viewpoint over the scene, allowing to form a new estimation of the target identity. Following the active inference setup~\cite{Najemnik05, Friston12}, we assume that, instead of trying to detect the actual position of the target, the agent tries to maximize the counter-factual benefit in scene understanding that would be gained by any potential saccade. The focus is thus put on action selection metric rather than spatial representation. This means in short estimating how accurate a categorical target classifier will be after moving the eye. In a full setup, predictive action selection means first predicting the future visual field denoted $x'$ which is obtained at the center of fixation, and then predicting how good the estimate of the target identity, denoted $y$, i.e. $p(y|x')$, will be at this location. In practice, predicting a future visual field over all possible saccades is too computationally expensive.  Better off instead is to record, for every context $x$, the improvement obtained in recognizing the target after different saccades $a, a', a'', \ldots$. If $a$ is a possible saccade and $x'$ the corresponding future visual field, the result of the central categorical classifier over $x'$ can either be correct ($1$) or incorrect ($0$). If this experiment is repeated many times over many visual scenes, the probability of correctly classifying the future visual field $x'$ from $a$ is a number between $0$ and $1$, that reflects the frequency of correct classifications. The putative effect of every saccade can thus be condensed in a single number, the \emph{accuracy}, that quantifies the final benefit of issuing saccade $a$ from the current observation $x$.  Extended to the full action space $A$, this forms an accuracy map that should monitor the selection of saccades. This accuracy map can be trained by trials and errors, with the final classification success or failure used as a teaching signal. Our main assumption here is that such a \emph{predictive accuracy map} is at the core of a realistic saccade-based vision systems.

From the active inference standpoint, the separation of the scene analysis in two independent tasks relies on a simple ``Naïve Bayes'' assumption (see Methods).
Each processing is assumed to be realized in parallel through different pathways by analogy with the ventral and dorsal pathways in the visual pathways (see Figure~\ref{fig:methods}). A first classifier is thus assigned to process only the pixels found at the center of fixation, while a second one processes the full visual field with a retina-mimetic central log-polar magnification. The first one is called the ``What'' network, and the second one is the ``Where'' network (see Figure~\ref{fig:where} for details). This combination of a scalar drive with action selection is reminiscent of the actor/critic principle proposed for long time in the reinforcement learning community~\cite{sutton1998reinforcement}. In biology, the ventral and the dorsolateral division of the striatum have been suggested to implement such an actor-critic separation~\cite{joel2002actor, takahashi2008silencing}. Consistently with those findings, our central accuracy drive and peripheral action selection map can respectively be considered as the ``critic'' and the ``actor'' of an accuracy-driven action selection scheme, with foveal identification/disambiguation taken as a ``visual reward''.

The operations that transform the initial visual data should preserve the initial retinotopic organization, so as to form a final retinotopic accuracy map. Accordingly with the visual data, the retinotopic accuracy map may thus provide more detailed accuracy predictions in the center, and coarser accuracy predictions in the periphery. Finally, each different initial visual field may bring out a different accuracy map, indirectly conveying information about the target retinotopic position. A final action selection (motor map) should then overlay the accuracy map through a winner-takes-all mechanism (see Figure~\ref{fig:methods}-D), implementing the saccade selection in biologically plausible way, as it is thought to be done in the superior colliculus, a brain region responsible for oculomotor control~\cite{sparks1987sensory}. The saccadic motor output showing a similar log-polar compression than the visual input, the saccades should be more precise at short than at long distance (and several saccades may be necessary to precisely reach distant targets). %

In practice, the ``What'' and `Where'' networks are both implemented in \texttt{pytorch}~\cite{NEURIPS2019_9015}, and trained with gradient descent over multiple layers. %A first classifier is thus assigned to process only the pixels found at the center of fixation, while a second one processes the full visual field with a retinotopic log-polar encoding. The first one is called the ``What'' network, and the second one is the ``Where'' network (see Figure~\ref{fig:where} for details).
Each network is trained and tested separately. Because the training of the ``Where'' pathway depends on the accuracy given by the ``What'' pathway (and not the reverse), we trained the latter first, though a joint learning also yielded similar results. % TODO : à vérifier
Finally, these are evaluated in a coupled, dynamic vision setup.
%=================================================================
\section{Results}
\label{sec:results}
%=================================================================
%------------------------------%
\begin{figure}[t!]%%[p!]
	%\flushleft{\bf (A) \hspace{4.2cm} (B) \hspace{2cm} (C) \hspace{4cm} (D)\hspace{6cm}}
	\centering{{\bf a.} \hspace{1.6cm} {\bf b.} \hspace{1.6cm} {\bf c.} \hspace{1.6cm} {\bf d.} \hspace{1.6cm} {\bf e.}}
	\centering{\A\includegraphics[width=.9\linewidth]{CNS-saccade-8.png}}
	\centering{\B\includegraphics[width=.9\linewidth]{CNS-saccade-20.png}}
	\centering{\C\includegraphics[width=.9\linewidth]{CNS-saccade-46.png}}
	\centering{\D\includegraphics[width=.9\linewidth]{CNS-saccade-47.png}}
	\centering{\E\includegraphics[width=.9\linewidth]{CNS-saccade-32.png}}
	% \includegraphics[]{../../2019-07-15_CNS/figures/CNS-saccade-8.png} % TRUE
	\caption{
		{\A-- \E Representative samples of active vision after training the ``Where'' network}: \A-- \B classification success samples, \C-- \E classification failure samples. Digit contrast set to $70\%$. From left to right: {\bf a.}~The initial 128$\times$128 visual display, with blue cross giving the initial center of gaze.
		The visual input is retinotopically transformed and sent to the multi-layer neural network implementing the ``Where'' pathway.
		{\bf b.}~Magnified reconstruction of the visual input, as it shows off from the primary visual features through an inverse log-polar transform.
		{\bf c.-d.}~Color-coded radial representation of the output accuracy maps, with dark violet for the lower accuracies, and yellow for the higher accuracies. The network output (``Predicted'') is visually compared with the ground truth (``True''). %
		{\bf e.}~The foveal image as the $28 \times 28$ central snippet extracted from the visual display after doing a saccade, with label prediction and success flag in the title.
		\label{fig:saccades}}%
\end{figure}%
%%------------------------------%
%=================================================================
\subsection{One saccade setup}
%=================================================================
After training, the ``Where'' pathway is now capable to predict an accuracy map (see Figure~\ref{fig:saccades}), whose maximal argument drives the eye toward a new viewpoint with one saccade. There, a central snippet is extracted, that is processed through the ``What'' pathway, allowing to predict the digit's label. Examples of this simple sequence with one saccade are presented in Figure~\ref{fig:saccades}, when the digits contrast parameter is set to $70\%$ and the digits eccentricity varies between $0$ and $40$ pixels. The presented examples correspond to strong eccentricity cases, when the target is hardly visible on the display (Figure~\ref{fig:saccades}a), and almost invisible on the reconstructed input (Figure~\ref{fig:saccades}b). The radial maps (Figure~\ref{fig:saccades}c-d) respectively represent the actual and the predicted accuracy maps. The final focus (foveal area at the location of the selected saccade) is represented in Figure~\ref{fig:saccades}e, with cases of classification success (Figure~\ref{fig:saccades}A-B) and cases of classification failures (Figure~\ref{fig:saccades}C-E). In the case of successful detection (Figure~\ref{fig:saccades}A-B), the accuracy prediction is not perfect and the digit is not perfectly centered on the fovea. This ``close match'' still allows for a correct classification, as the digit's pixels are fully present on the fovea. The case of Figure~\ref{fig:saccades}B and~\ref{fig:saccades}C is interesting for it shows two cases of a bimodal prediction, indicating that the network is capable of doing multiple detections in a single pass, that is, at a glance. The case of Figure~\ref{fig:saccades}C corresponds to a false detection, with the true target detected still, though with a lower intensity. The case of Figure~\ref{fig:saccades}D is a ``close match'' detection that is not precise enough to correctly center the visual target. Some pixels of the digit being invisible on the fovea, the label prediction is mistaken. The last failure case (Figure~\ref{fig:saccades}E) corresponds to a correct localization that is harmed by a wrong label prediction, which is due to the ``What'' classifier inherent error rate. % We observed that either the detection of the object's position was correct, thus allowing a classification proportional to the accuracy of the ``what'' pathway, either that the predicted accuracy map was wrong and generated a wrong classification with an accuracy at chance level.

%------------------------------%
%: see Figure~\ref{fig:results}
\begin{figure}[t!]%%[p!]
	%\flushleft{\bf (A) \hspace{4.2cm} (B) \hspace{2cm} (C) \hspace{4cm} (D)\hspace{6cm}}
	% \centering{\includegraphics[width=\linewidth]{CNS-results-contrast.png}}
	%\centering{\includegraphics[width=\linewidth]{CNS-results-contrast.png}}
	\centering{\includegraphics[width=\linewidth]{../figures/fig_results_robust_contrast.pdf}}
	% \includegraphics[]{../../2019-07-15_CNS/figures/CNS-saccade-8.png} % TRUE
	\caption{
		{\bf Effect of contrast and target eccentricity.} %
		The active vision agent is tested for different target eccentricities (in pixels) and different contrasts to estimate a final classification rate. (Transparent) light orange bars: pre-saccadic accuracy from the central classifier ('No saccade') with respect to the target's eccentricity, averaged over $1000$ trials per eccentricity. Blue bars: post-saccadic classification rate. % \if 0\ICANN{\color{blue} TODO: show three levels of noise Low (0.5) median (1.2) high (2). TODO: compare with Accuracy max (knowing the position)}\fi
		\label{fig:results}}%
\end{figure}%
%------------------------------%
To test for the robustness of our framework, the same experiment was repeated at different signal-to-noise ratios (SNR) of the input images. Both pathways being interdependent, it is indeed crucial to disentangle the relative effect of both sources of errors in the final accuracy. By manipulating the SNR and the target eccentricity, one can precisely monitor the network detection and recognition capabilities, with a detection task ranging from ``easy'' (small shift, strong contrast) to ``highly difficult'' (large shift, low contrast). The digit recognition capability is systematically evaluated in Figure~\ref{fig:results} for different eccentricities and different SNRs. We test the final accuracy of the ssytem for three target contrasts conditions ranging from 30\% to 70\% of the maximal contrast, and 10 different eccentricities ranging from 4 to 40 pixels. It is averaged over $1,000$ trials both on the initial central snippet and the final central snippet (that is, at the landing of the saccade). The (transparent) orange bars provide the initial classification rate (without saccade) and the blue bars provide the final classification rate (after saccade) -- see Figure~\ref{fig:results}. As expected, the accuracy decreases in both cases with the eccentricity, for the targets become less and less visible in the periphery. The decrease is rapid in the pre-saccadic case: the accuracy drops to the baseline level for a target distance of approximately $20$ pixels from the center of gaze. The post-saccadic accuracy has a much wider range, with a slow decrease up to the border of the visual display (40 pixels away from the center).
When varying the target contrast, the pre-saccadic accuracy profile is scaled by the reference accuracy (obtained with a central target), whose values are approximately $92\%$, $82\%$ and $53\%$ for contrasts of $70$, $50$ and $30$\%. The post-saccadic accuracy profile undergoes a similar scaling at the different contrast values, indicating the critical dependence of the global setup to the central processing reliability.

The high contrast case (see Figure~\ref{fig:results}) provides the greatest difference between the two profiles, with an accuracy approaching $90\%$ at the center and $60\%$ at the periphery. This allows to recognize digits after one saccade in a majority of cases, up to the border of the image, from a very scarce peripheral information.


With decreasing target contrast, a general decrease of the accuracy is observed, both at the center and at the periphery, with about 10\% decrease with a contrast of $0.5$, and $40\%$ decrease with a contrast of $0.3$. In addition, the proportion of false detections also increases as contrast decreases. At $40$ pixels away from the center, the false detection rate is approximately $40\%$ for a contrast of $0.7$, $60\%$ for a contrast of $0.5$ and $80\%$ for a contrast of $0.3$ (with a recognition close to the baseline at the periphery in that case). The difference between the initial and the final accuracies is maximal for eccentricities ranging from $15$ to $30$ pixels. This optimal range reflects a proportion of the visual field around the fovea where the target detection is possible, but not its identification. The visual agent knows \emph{where} the target is, without exactly knowing \emph{what} it is.

\subsection{Analysis}
This full covering of the 128$\times$128 image range is done at a much lesser cost than what would be done by a systematic image scan, as in classic computer vision. Taking $n$ the number of pixels in the original image (in our case $n=128\times128=16384$), our log-polar encoding provides $O(\log n)$ log-polar visual features by construction. The total visual data processed is the addition of the $C$ pixels processed at the fovea and the $O(\log n)$ log-polar visual features processed at the periphery. The total processing cost is thus $O(C+\log n)$. Taking $C$ as a constant, the total processing cost can be said $O(\log n)$ (for constant processing times do not change the order).
In the case of multiple saccades (see next section), the total cost is $O(k\times(C+\log n))$ with $k$ the number of saccades. If the number of saccades $k$ is bounded by a constant $K$, this allows to estimate the processing cost as $O(K\times(C+\log n))$ in the worst case, that also resumes to $O(\log n)$.
This is to be contrasted, for instance, with the linear cost obtained with a full convolutional scan with a window of size C and a stride of 1, that is precisely $O(C\times n)$. Various optimizations can of course be considered, of which the well-known max-pooling principle used in deep learning, but anyway
image processing (without compression loss) is generally considered as linear in the size of the visual data processed \cite{strengert2006pyramid}

Our sub-linear processing time thus justifies a strategy that may have been chosen in a variety of natural vision systems. The compromise between the urgency to detect and the need to be accurate may justify the different balances which may exist in different species. In particular, this may justify the differences observed between preys (with a less sparse cone density at the periphery) and predators (with a tendency toward  denser foveal regions). % TODO ref needed !!

%
\subsection{Multiple saccades setup}
%
In our simulation results, the post-saccadic accuracy is found to overtake the pre-saccadic accuracy \emph{except} when the target is initially close to the center of gaze. When closely inspecting the 1-10 pixels eccentricity range in our first experiment (not shown), a decision frontier between a positive and a negative information gain is found at 2-3 pixels away from the center. Inside that range, no additional saccade is expected to be produced, and a categorical response should be given instead. It is crucial here to understand that this empirical accuracy difference can be predicted, by construction, as the difference of the maximal outputs of the ``Where'' and the ``What'' pathways. This difference-of-accuracies prediction can serve as a decision criterion before actuating the saccade, like a GO/NOGO signal. It is moreover interpretable as an approximation of the information gain provided by the ``Where'' pathway, with the true label log-posterior seen as a sample of the posterior entropy (see Equation~\ref{eq:IG} in section~\ref{sec:IG}).

After the first saccade, while the decision criterion is not attained, additional saccades may be pursued in order to search for a better centering. In the case of a false detection for instance, the central accuracy estimate should be close to the baseline, and may allow to ``explain away'' the current center of gaze and its neighborhood, encouraging to actuate long-range saccades toward less salient peripheral positions, making it possible to escape from initial prediction errors. This incitement to select a saccade ``away'' from the central position is reminiscent of a well-known phenomenon in vision known as the ``inhibition of return''~\cite{Itti01}. Combining accuracy predictions from each pathway may thus allow to refine saccades selection in a way that complies with the sequential processing observed in biological vision. Note that extended to a multi-target case, the Information Gain maximization principle still holds as a general measure of scene understanding improvement through multiple saccades. It is uncertain however wether biologically realistic implementations would be possible in that case. In particular, we predict that such a mechanism is dependent on the class of inputs, and would be for instance different when searching for faces as compared to digits.

Some of the most peripheral targets are thus difficult to detect in just one saccade, resulting in degraded performances at the periphery (see Figure~\ref{fig:results}). Even when correctly detected, our log-polar action maps also precludes precise centering. As a consequence, peripheral targets are generally poorly centered after the first saccade, as shown for instance in Figure~\ref{fig:saccades}-D, resulting in classification errors. The possibility to perform a sequential search using more saccades is thus crucial to allow for a better recognition. Results on multi-saccades visual search results are presented in Figure~\ref{fig:results-saccades}.

%------------------------------%
%: see Figure~\ref{fig:results}
\begin{figure}[t!]%%[p!]
	%\flushleft{\bf (A) \hspace{4.2cm} (B) \hspace{2cm} (C) \hspace{4cm} (D)\hspace{6cm}}
	%\centering{\includegraphics[width=\linewidth]{fig-results-saccades.png}}
	\centering{{\bf (A)}
		\includegraphics[width=\linewidth]{../figures/fig_multi_saccades_example.pdf}}
	\centering{{\bf (B)}
		\includegraphics[width=\linewidth]{../figures/fig_results_robust_saccades.pdf}}
	%\includegraphics[]{../../2019-07-15_CNS/figures/CNS-saccade-8.png} % TRUE
	\caption{
		{\bf Multiple saccade setup.} %
		\A Example of a trial with a sequence of $3$ saccades. The subjective visual field is reconstructed from the log-polar visual features, with the red square delineating the $28\times28$ foveal snippet, after $0$, $1$, $2$ and $3$ saccades (from left to right). After the first saccade, the accuracy predicted by the ``Where'' network is higher than that predicted by the ``What'' network and a corrective saccade is realized to center the target. After this second saccade, the foveal accuracy is higher than that predicted in the periphery and the answer \ANS\ is given.
		\B Average classification accuracies measured for different target eccentricities (in pixels) and a different number of saccades. Target contrast set to $70\%$. (Transparent) light orange bars: pre-saccadic central accuracy (``0 saccade'') with respect to eccentricity, averaged over $1000$ trials per eccentricity. Blue bars: Final classification rate after one, two and three saccades (from left to right, respectively).
		\label{fig:results-saccades}}%
\end{figure}%
%------------------------------%
An example of a trial with a sequence of 3 saccades is shown in Figure~\ref{fig:results-saccades}-A. A hardly visible peripheral digit target is first approximately shifted to the foveal zone thanks to the first saccade. Then, a new retinal input centered at the new point of fixation is computed, such that it generates a novel predicted accuracy map. The second saccade allows to improve the target centering. As the predicted foveal accuracy given by the ``What'' network is higher than the peripheral one given by the ``Where'' network, a third saccade would not improve the centering: The stopping criteria is met. In practice, $1$ or $2$ saccades were sufficient in most trials to reach the actual target. Another behavior was also observed for some ``bad start'' which exhibited a false localization (as in Figure~\ref{fig:saccades}-C for instance), when the target is shifted away in the opposite direction and the agent can not recover from its initial error. From Figure~\ref{fig:results-saccades}-B, this case can be estimated at about $15\%$ of the cases for the most peripheral targets.

Overall, as shown in Figure~\ref{fig:results-saccades}-B, the corrective saccades implemented in this multiple saccade setup provide a significant improvement in the classification accuracy. Except at the center, the accuracy increases by about $10\%$ both for the mid-range and the most peripheral eccentricities. Most of the improvement however is provided by the first corrective saccade. The second corrective saccade only shows a barely significant improvement of about $2\%$ which is only visible at the periphery. The following saccades would mostly implement target tracking, without providing additional accuracy gain. A 3-saccades setup finally allows a wide covering of the visual field, providing a close to central recognition rate at all eccentricities, with the residual peripheral error putatively corresponding to the ``bad start'' target misses cases.
