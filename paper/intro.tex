% !TEX root = DauceAlbigesPerrinet2020.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US
\section{Introduction}
\label{sec:intro}
\subsection{Problem statement}
%------------------------------%
% >>>>>>>>>>>> LuP is here <<<<<<<<<<<<<<<<<<<<<<<
%: see Figure~\ref{fig:intro}
%Past 10 years have seen the disrupting development of deep learning based image processing. Indeed, t
The field of computer vision was recently recast by the outstanding capability of convolution-based deep networks to capture the semantic content of images and photographs. Human observers performance is now outreached by computer algorithms in specific image categorization tasks~\cite{He15}. One facet of this success relies on a reduction of parameter complexity through the weight sharing imposed in convolutional neural networks.
%Initially trained on energy greedy, high performance computers, these algorithms are now designed to work on more common hardware such as desktop computers with dedicated GPU hardware~\cite{Sandler18}.
However, despite lots of efforts spent in optimizing the processing, visual processing is still done at a cost that scales linearly with the image size as all regions of the image, even the “boring” ones are systematically scanned and processed. % in parallel through dedicated hardware at a significant computational cost.
%Inherent to this problem is the combinatorial explosion State-of-the art classification
Computer vision algorithms consequently manipulate millions of variables and for videos, this causes a subsequent energy consumption even with small images and a prohibitive cost for large images. With the need to detect visual objects at a glance while running on resource-constrained embedded hardware, for instance in autonomous driving, this introduces a necessary trade-off between efficiency and accuracy.

In contrast, when human vision is considered, things work differently. First, the  general performance for an ecological real-time sensory flow is still greater than that of computer vision.
Indeed, object recognition can be achieved by the human visual system both rapidly, --~in less than 100 ms~\cite{Kirchner06}~-- and at a low energy cost ($<5~W$).
On top of that, it is mostly self-organized, robust to visual transforms or lighting conditions and can learn with a few examples. If many different anatomical features may explain this efficiency, a main difference lies in the fact that its sensor (the retina) combines a non homogeneous sampling of the world with the capacity to rapidly change its center of fixation. On the one hand, the retina is composed of two separate systems: a central, high definition fovea (a disk of about 6 degrees of diameter in visual angle around the center of gaze) and a large, lower definition peripheral area~\cite{Strasburger11}.
On the other hand, the human vision is \emph{dynamic}. The retina is attached on the back of the eye which is capable of low latency, high speed eye movements. In particular, saccades are stereotyped eye movements that allow for efficient changes of the position of the center of gaze: they take about $200~\ms$ to initiate, last about $200~\ms$ and usually reach a maximum velocity of approx 600 degrees per second~\cite{Bahill75}. The scanning of a full visual scene is thus not done in parallel but sequentially, and only scene-relevant regions of interest are scanned through saccades. This implies a \emph{decision process} at each step that decides \emph{where to look next}. This behavior is prevalent in biological vision (about a saccade every 2-3 seconds, that is, almost a billion saccade in a lifetime). The interplay of those two features allows human observers to engage in an integrated action perception loop which sequentially scans and analyses the different parts of the image.

%The promise of artificial vision to identify objects in natural images is ever increasing.
% However, these algorithms are still far from human performances, even for simple tasks.

Take for instance the case of an encounter with a friend in a crowded café. To catch the moment at which she arrives, you need to visually search for her face despite the sensory clutter in the visual field. To do so, you need to scan relevant parts of the visual scene with your gaze. Doing a saccade at these locations will allow you to recognize your friend. The main difficulty of this task is to identify a particular object \emph{class} (e.g. human faces) given all their possible spatial configurations and respective geometrical visual transformations. Searching for \emph{any} face in a peripheral and crowded display needs to precede the recognition of a specific face identity.


%It is one type of active inference~\cite{Friston12} (see below) and we will envision herein how to incorporate it to classical computer vision schemes.
% (1 / 2.5 * 3600 * 24 * 365 * 75 = 946080000.0 ~= .95e9) X (wakeful + REM = .66)
%
\subsection{State of the art}

To take benefit from this visuomotor behavior, it is important to understand both its computational and neurophysiological principles. First, the joint problem of target localization and identification is a classical problem of visual search in computer vision. Addressing apparently simple questions such as ``find the green bottle on the table'', it is of broad interest in machine learning, computer vision and robotics, but also in neuroscience, as it speaks to the mechanisms underlying foveation and more generally to low-level attention mechanisms.
When restricted to a mere ``feature search''~\cite{Treisman80}, many computational solutions already exist. Notably, recent advances in deep-learning have provided efficient models such as faster-RCNN~\cite{Ren17} or YOLO~\cite{Redmon16}. %5}.
Their object search implementations predict in the image the probability of proposed bounding boxes around visual objects. While rapid, the number of boxes may significantly increase with image size and the approach more generally necessitates dedicated hardware to run in real time.

In parallel, human visual scan-path over natural images provides ways to define \emph{saliency maps}, that quantify the attractiveness of the different parts of an image that are consistent with the detection of objects of interest. Essential to understand and predict saccades, they also serve as phenomenological models of attention. Estimating the saliency map from  a luminous image is a classical problem in neuroscience, that was shown consistent with a distance from baseline image statistics known as the ``Bayesian surprise''~\cite{Itti01}. The saliency approach was recently updated using deep learning to estimate saliency maps over large databases of natural images~\cite{Kummerer17}. %6}.
While these methods are efficient at predicting the probability of fixation, they miss an essential component in the action perception loop: they operate on the full image while the retina operates on the non-uniform, foveated sampling of visual space (see Figure~\ref{fig:intro}-B).
Herein, we believe that this fact is an essential factor to reproduce and understand the active vision process.

Foveated models of vision have been considered for long time in robotics and computer vision as a way to leverage the visual scene scaling problem. Focal computer vision relies on a non-homogeneous compression of an image, that maintains the pixel information at the center of fixation and strongly compresses it at the periphery, including pyramidal encoding~\cite{kortum1996implementation,Butko2010infomax}, local wavelet decomposition~\cite{dauce2018active} and log-polar encoding~\cite{fischer2007self,Traver10}.
A recent deep-learning based implementation of such compression shows that in a video flow, a log-polar sampling of the image is sufficient to provide a reconstruction of the whole image~\cite{Kaplanyan19}. However, such algorithm lacks a system predicting the best saccading action to perform. In summary, though focal and multiscale encoding is now largely considered in static computer vision, sequential implementations have not been shown effective enough to overtake static object search methods.
% {\color{red} \textbf{rev2} In line 81, authors claim that “sequential implementations have not been shown effective enough to overtake static object search methods.” However, there are at least three models (FOD, TAM and Infomax) which wer  shown to be superior to (in some performance metric) classical sliding-window approach. So, the authors should revise their statement.}
Several implementations of a focal sequential search in visual processing can be found in the literature, with various degrees of biological realism~\cite{mnih2014recurrent,fu2017look}, that often rely on a simplified focal encoding, long training procedures and bounded sequential processing. More realistic attempts to combine foveal encoding and sequential visual search can be found in~\cite{Butko2010infomax,denil2012learning,dauce2018active} and which will be compared further on with our approach.
% {\color{red} \textbf{Rev 2}
% In line 86, authors say “can be found in [10,11,16] that will be compared further on with our approach.” However, this comparison is only in the form of a discussion (lines 555-585). Upon reading line 86, I expected to see experimental comparisons. In fact, no actual (experimental) comparisons with other models are done. As a reader, at the least, I expected a comparison with a baseline model which implements the classical sliding-window method. }

In contrast to phenomenological (or ``bottom-up'') approaches, active models of vision~\cite{Najemnik05,Butko2010infomax,Friston12} provide the ground principles of saccadic exploration. In general, they assume the existence of a generative model from which both the target position and category can be inferred through active sampling. This comes from the constraint that the visual sensor is foveated but can generate a saccade.
Several studies are relevant to our endeavor. First, one can consider optimal strategies to solve the problem of the visual search of a target~\cite{Najemnik05}. In a setting similar to that presented in Figure~\ref{fig:intro}-A, where the target is an oriented edge and the background is defined as pink noise, authors show first that a Bayesian ideal observer comes out with an optimal strategy, and second that human observers are close to that optimal performance. Though well predicting sequences of saccades in a perception action loop, this model is limited by the simplicity of the display (elementary edges added on stationary noise, a finite number of locations on a discrete grid) and by the abstract level of modeling. Despite these (inevitable) simplifications, this study could successfully predict some key characteristics of visual scanning such as the trade-off between memory content and speed. Looking more closely at neurophysiology, the study of~\cite{Samonds18} allows to go further in understanding the interplay between saccadic behavior and the statistics of the input. In this study, authors were able to manipulate the size of saccades by monitoring key properties of the presented (natural) images. For instance, smaller images generate smaller saccades. %Interestingly, they also predicted the size of saccades for different species, including mice which lack a foveal region, from the size of visual receptive fields.{ \color{magenta} One key prediction of this study which is relevant for our problem is the fact that saccades seem optimal to \emph{a priori} decorrelate the visual input, that is, to minimize redundancy in the sequence of generated saccades, knowing the statistics of the visual inputs.}

A further modeling perspective is provided by~\cite{Friston12}. In this setup, a full description of the visual world is used as a generative process. An agent is completely described by the generative model governing the dynamics of its internal beliefs and is interacting with this image by scanning it through a foveated sensor, just as described in Figure~\ref{fig:intro}. Thus, equipping the agent with the ability to actively sample the visual world %enables to explore the idea that actions (saccadic eye movements) are
allows to interpret saccades as optimal experiments, by which the agent seeks to confirm predictive models of the (hidden) world. One key ingredient to this process is the (internal) representation of counterfactual predictions, that is, the probable consequences of possible hypothesis as they would be realized into actions (here, saccades). Following such an active inference scheme~\cite{Mirza18} numerical simulations reproduce sequential eye movements that fit well with empirical data. %Compared to~\citet{Najemnik05},
Saccades %are not the output of a value-based cost function, but
are here a consequence of an active seek for the agent to minimize the uncertainty about his beliefs, knowing his priors on the generative model of the visual world.

\subsection{Outline}


Stemming from the active vision principles, our aim is to produce a principled and resource-effective model of vision.
We start from an elementary visual search problem, that is how to locate an object in a large, cluttered image, and take human vision as a guide for efficient design.  {\color{green} Motivation du modèle à développer. }

Our framework is made as general as possible, with minimal mathematical treatment, to speak largely to fragmented domains, such as machine learning, neuroscience and robotics. We thus expect our foveated active vision model to be helpful for each of those domains.
The paper is organized as follows.
After this introduction, the principles underlying accuracy-based saccadic control are defined in the second section. We first define notations, variables and equations for the generative process governing the experiment and the generative model for the active vision agent.
Complex combinatorial inferences are here replaced by separate pathways, i.e. the spatial (``Where'') and categorical (``What'') pathways, whose output is combined to infer optimal eye displacements and subsequent identification of the target. Our agent, equipped with a foveated sensor, should learn an optimal behavior strategy to actively scan the visual scene. Implementation details are provided in the methods section, giving ways to reproduce our results,  showing in particular how to simplify the learning using accuracy-driven action maps. Numerical simulations are presented in the results section, demonstrating the applicability of this framework to tasks with different complexity levels. %This allows us to derive some limits of the agent and, as in~\cite{Najemnik05}, {\color{magenta} [TODO] we draw some analogies with biologically observed eye movements}.
The last section finally summarizes the results, showing its relative advantages in comparison with other frameworks, and providing ways toward possible improvements.

%It is known that inverting a generative model over a large (one-step ahead) hypothesis space of all possible saccades is computationally-intensive. % (think for instance of face category as a very large categorical space over a large visual transformation space) with no obvious neurophysiological counterpart. (see Figure~\ref{fig:intro}-C)
%Although we similarly include a generative process of the visual world,

%as conatining {\bf (containing??)} images of a handwritten random digit (drawn from the MNIST database) at a random position and embedded in a cluttered noise .
%. % as defined by a generative (internal) modelWe will use this constraint as an asset
%to which also contributes to minimizing the overall computational cost of finding a target.
%Taking such priors, we
%and explore its key properties.
