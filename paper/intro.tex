\section{Introduction}

\subsection{Issue}

\subsection{State of the art}

\subsection{Outline}

\subsubsection{Notations}
\begin{itemize}
	\item $\boldsymbol{x}$ : visual field (image)
	\item $\boldsymbol{y}$ : target category (categorical)
	\item $\boldsymbol{z}$ : target position (real coordinates or categorical, head-centered referential)
	\item $\boldsymbol{e}$ : eyes orientation (real coordinates or categorical, head-centered referential)
	\item $\boldsymbol{u} = \boldsymbol{z} - \boldsymbol{e}$ : target position (real coordinates or categorical, retinocentric referential)

\end{itemize}

Generative model :
$$ \boldsymbol{x} \sim P(X|\boldsymbol{y}, \boldsymbol{u}) $$

Full inference (posterior):
$$ P(Y, U|\boldsymbol{x}) \propto  P(\boldsymbol{x}|Y, U) $$

%Independence assumption :
%$$ P(Y, U|\boldsymbol{x}) = P(Y|\boldsymbol{x})  P(U|\boldsymbol{x}) $$

Partial inference on object category:
$$ P(Y|\boldsymbol{x}, \boldsymbol{u}) \propto  P(\boldsymbol{x}|Y, \boldsymbol{u}) $$

Partial inference on object position:
$$ P(U|\boldsymbol{x}, \boldsymbol{y}) \propto  P(\boldsymbol{x}|U, \boldsymbol{y}) $$

Marginals:
\begin{itemize}
\item $ P(Y|\boldsymbol{x}) = \int P(Y|\boldsymbol{x}, \boldsymbol{u}) d\boldsymbol{u}$
\item $ P(U|\boldsymbol{x}) = \int P(U|\boldsymbol{x}, \boldsymbol{y}) d\boldsymbol{y}$
\end{itemize}


\subsubsection{General case: Visual information gain maximization}\label{sec:case1}

Consider a view $\boldsymbol{x}$ generated from a target $\boldsymbol{y}$ viewed at retinocentric position $\boldsymbol{u}$. 

Consider first that :
\begin{itemize}
	\item The generative model  $p(X|\boldsymbol{y}, \boldsymbol{u})$ is known
	\item The retinocentric position  $\boldsymbol{u}$ is known.
	\item The view $\boldsymbol{x}$ is known.
	\item The target category  $\boldsymbol{y}$ is unknown.
\end{itemize} 



The question comes how to choose the new retinocentric position $\boldsymbol{u}'$ in order to maximize the \emph{mutual information} between $\boldsymbol{x}|\boldsymbol{u}$ (current view) and $\boldsymbol{x}'|\boldsymbol{u}'$ (future view).

In general, the visual Information Gain between two visual fields $\boldsymbol{x}|\boldsymbol{u}$  and $\boldsymbol{x}'|\boldsymbol{u}'$ is:

\begin{align*}
\text{IG}(\boldsymbol{x}|\boldsymbol{u}; \boldsymbol{x}'| \boldsymbol{u}') 
&= -\log p(\boldsymbol{x}|\boldsymbol{u}) 
+ \log p(\boldsymbol{x}|\boldsymbol{u}, \boldsymbol{x}', \boldsymbol{u}')
\end{align*}

\paragraph{Information Gain Lower Bound}
Consider now that given  $\boldsymbol{x}$ and $\boldsymbol{u}$, the target category  $\boldsymbol{y}$ can be \emph{inferred} using Bayes rule, i.e.:
$$ P(Y|\boldsymbol{x}, \boldsymbol{u}) \propto  P(\boldsymbol{x}|Y, \boldsymbol{u}) $$
Then, it can be shown (see \cite{dauce2018}) that :
$$\text{IG}(\boldsymbol{x}|\boldsymbol{u}; \boldsymbol{x}'| \boldsymbol{u}') \geq \mathbb{E}_{\boldsymbol{y}\sim p(Y|\boldsymbol{x}, \boldsymbol{u})} \left[\log p(\boldsymbol{y}|\boldsymbol{x}', \boldsymbol{u}') - \log(\pi(\boldsymbol{y})) \right]$$
with  $\pi(\boldsymbol{y})$ the prior over the $\boldsymbol{y}$'s .
When the prior is uniform, the information gain lower bound (IGLB) simplifies to $\mathbb{E}_{\boldsymbol{y}\sim p(Y|\boldsymbol{x}, \boldsymbol{u})} \left[\log p(\boldsymbol{y}|\boldsymbol{x}', \boldsymbol{u}')\right] + c$, with $c$ a constant.

\paragraph{Predictive approach}
One can adopt a \emph{predictive} approach to choose the new eye orientation $\boldsymbol{e}'$:
\begin{itemize}
	\item First choose a new retinocentric position $\boldsymbol{u}'$ that will maximize the  information gain.
	\item Then choose $\boldsymbol{e}'$ such that $$\boldsymbol{z} - \boldsymbol{e}' = \boldsymbol{u}'$$ i.e. $$\boldsymbol{e}' = \boldsymbol{e} + \boldsymbol{u} - \boldsymbol{u}'$$
\end{itemize}

The predictive approach needs three predictive steps:
\begin{itemize}
	\item $p(Y|\boldsymbol{x}, \boldsymbol{u})$ is the current posterior over the target category inferred from the current observation,
	\item $\boldsymbol{x}'\sim p(X|\boldsymbol{y},\boldsymbol{u}')$ is the predicted view generated by the model assuming that the target $\boldsymbol{y}$ is seen from from $\boldsymbol{u}'$,
	\item and $p(\boldsymbol{y}|\boldsymbol{x}', \boldsymbol{u}')$ is the predicted posterior from   assumption $\boldsymbol{y}$, given $\boldsymbol{x}'$ and $\boldsymbol{u}'$.
\end{itemize}

The optimal new retinocentric position is then:
\begin{align*}
\hat{\boldsymbol{u}}' &= \underset{\boldsymbol{u}' }{\text{ argmax }} 
 \mathbb{E}_{\boldsymbol{y}\sim p(Y|\boldsymbol{x}, \boldsymbol{u})}  
 \left[\mathbb{E}_{ \boldsymbol{x}' \sim p(X|\boldsymbol{y}, \boldsymbol{u}')}
 \left[\log p(\boldsymbol{y}|\boldsymbol{x}', \boldsymbol{u}')\right]\right]\\
  %&= \underset{\boldsymbol{u}' \in \mathcal{U}}{\text{ argmax }} A(\boldsymbol{u}'|\boldsymbol{x}, \boldsymbol{u})
\end{align*}

Taking $\delta \boldsymbol{e} = \boldsymbol{u} - \boldsymbol{u}'$ the eye displacement, 
\begin{align*}
\widehat{\delta\boldsymbol{e}} &= \underset{\delta\boldsymbol{e} }{\text{ argmax }} 
\mathbb{E}_{\boldsymbol{y}\sim p(Y|\boldsymbol{x}, \boldsymbol{u})}  
\left[\mathbb{E}_{ \boldsymbol{x}' \sim p(X|\boldsymbol{y}, \boldsymbol{u}- \delta \boldsymbol{e})}
\left[\log p(\boldsymbol{y}|\boldsymbol{x}', \boldsymbol{u}-\delta\boldsymbol{e})\right]\right]\\
%&= \underset{\delta\boldsymbol{e}}{\text{ argmax }} A(\delta\boldsymbol{e}|\boldsymbol{x}, \boldsymbol{u})
\end{align*}

%For each possible target identity $\boldsymbol{y}$, $A_{\boldsymbol{y}}(\boldsymbol{u}') = \mathbb{E}_{ \boldsymbol{x}' \sim p(X|\boldsymbol{y}, \boldsymbol{u}')}
%\left[\log p(\boldsymbol{y}|\boldsymbol{x}', \boldsymbol{u}')\right]$ is the \emph{class-specific log posterior} map and 


\subsubsection{Link with acuracy maps}
In a model-based approach, the \emph{accuracy maps} can be calculated using a parametric classifier : 
\begin{itemize}
	\item Given a training set $\{(x_1, u_1, y_1), ..., (x_n, u_n, y_n)\}$:
	\begin{itemize}
		\item Train a classifier $p_\theta$ that estimates $p(Y|\boldsymbol{x}, \boldsymbol{u})$. 
	\end{itemize}
	\item Then, for each class $\boldsymbol{y}$, taking $\tilde{\boldsymbol{y}}\sim p_\theta(Y|\boldsymbol{x}, \boldsymbol{u})$,\emph{ the classification rate $r_\theta(\boldsymbol{y}, \boldsymbol{u})$ is an estimator of the posterior expectation :}
\begin{align*}
r_\theta(\boldsymbol{y}, \boldsymbol{u}) 
%&= \frac{1}{|\{i:\boldsymbol{y}_i = \boldsymbol{y}\}|}\\
&= \mathbb{E}_{ \boldsymbol{x} \sim p(X|\boldsymbol{y}, \boldsymbol{u})}
\mathbb{E}_{\tilde{\boldsymbol{y}}\sim p_\theta(Y|\boldsymbol{x}, \boldsymbol{u})} \delta_{\boldsymbol{y}=\tilde{\boldsymbol{y}}}\\
&= \mathbb{E}_{ \boldsymbol{x} \sim p(X|\boldsymbol{y}, \boldsymbol{u})} p_\theta(\boldsymbol{y}|\boldsymbol{x}, \boldsymbol{u})\\
&\simeq \mathbb{E}_{ \boldsymbol{x} \sim p(X|\boldsymbol{y}, \boldsymbol{u})} p(\boldsymbol{y}|\boldsymbol{x}, \boldsymbol{u})
\end{align*} 
that forms an \emph{accuracy map} for each class $\boldsymbol{y}$.\\
\item Then, given a mixture $q$ on the target category, 
\begin{align*}
 r_\theta(\boldsymbol{u}|q) &= \mathbb{E}_{\boldsymbol{y}\sim q(Y)} \left[\mathbb{E}_{ \boldsymbol{x} \sim p(X|\boldsymbol{y}, \boldsymbol{u})}  p_\theta(\boldsymbol{y}|\boldsymbol{x}, \boldsymbol{u}) \right]
  %&\simeq \mathbb{E}_{\boldsymbol{y}\sim q(Y)}  
%\left[\mathbb{E}_{ \boldsymbol{x} \sim p(X|\boldsymbol{y}, \boldsymbol{u})}
%p(\boldsymbol{y}|\boldsymbol{x}', \boldsymbol{u}')\right]
\end{align*} is a mixture of accuracy maps.
\end{itemize}

%Then:
%$$\tilde{A}_{\boldsymbol{y}}(\boldsymbol{u}') = \mathbb{E}_{ \boldsymbol{x}' \sim p(X|\boldsymbol{y}, \boldsymbol{u}')}$$

\emph{(TODO : Attention il faudrait à partir de maintenant une carte qui moyenne les log posteriors car l'espérance du log n'est pas exactement égale au log de l'espérance, i.e. $r_{\theta}'(\boldsymbol{u}|q) = \mathbb{E}_{\boldsymbol{y}\sim q(Y)} \left[\mathbb{E}_{ \boldsymbol{x} \sim p(X|\boldsymbol{y}, \boldsymbol{u})}  \log p_\theta(\boldsymbol{y}|\boldsymbol{x}, \boldsymbol{u}) \right]$).}
\newline

One can now select $\boldsymbol{u}'$ with the parametric estimator:
\begin{align*}
\hat{\boldsymbol{u}}' &\simeq \underset{\boldsymbol{u}' }{\text{ argmax }} 
\mathbb{E}_{\boldsymbol{y}\sim p(Y|\boldsymbol{x}, \boldsymbol{u})}  
\left[\mathbb{E}_{ \boldsymbol{x}' \sim p(X|\boldsymbol{y}, \boldsymbol{u}')}
\left[\log p_\theta(\boldsymbol{y}|\boldsymbol{x}', \boldsymbol{u}')\right]\right]\\
&= \underset{\boldsymbol{u}' }{\text{ argmax }}  r_\theta(\boldsymbol{u}'|p(Y|\boldsymbol{x}, \boldsymbol{u}))
%&= \underset{\boldsymbol{u}' \in \mathcal{U}}{\text{ argmax }} A(\boldsymbol{u}'|\boldsymbol{x}, \boldsymbol{u})
\end{align*}



%with $A(\delta\boldsymbol{e}|\boldsymbol{x}, \boldsymbol{u})$ the IGLB\footnote{Information Gain Lower Bound} map.

and equivalently one can optimize $\delta\boldsymbol{e}$ with a \emph{transformation map}:
$$r_\theta(\delta\boldsymbol{e}|\boldsymbol{u}, q) = r_\theta(\boldsymbol{u}-\delta\boldsymbol{e}|q)$$
i.e.:
\begin{align*}
\widehat{\delta\boldsymbol{e}} &= \underset{\delta\boldsymbol{e}}{\text{ argmax }} 
 r_\theta(\delta\boldsymbol{e}|\boldsymbol{u}, p(Y|\boldsymbol{x}, \boldsymbol{u}))
%&= \underset{\boldsymbol{u}' \in \mathcal{U}}{\text{ argmax }} A(\boldsymbol{u}'|\boldsymbol{x}, \boldsymbol{u})
\end{align*}

It must be noticed that the transformation map is maximal at $\delta\boldsymbol{e} = \boldsymbol{u} - \hat{\boldsymbol{u}}'$. Each initial $\boldsymbol{u}$ provides a different transformation map, that is a shift of the original log-accuracy map (\emph{Ergodic assumption??}).
 
\subsubsection{Colliculus map}
Consider now that the target retinocentric position  $\boldsymbol{u}$ is unknown.


Then the transformation map becomes :

\begin{align*}
\widehat{\delta\boldsymbol{e}} &= \underset{\delta\boldsymbol{e}}{\text{ argmax }} 
\mathbb{E}_{\boldsymbol{u}\sim p(U|\boldsymbol{x})}\left[r_\theta(\delta\boldsymbol{e}|\boldsymbol{u}, p_\theta(Y|\boldsymbol{x}, \boldsymbol{u}))\right]\\
&= \underset{\delta\boldsymbol{e}}{\text{ argmax }} Q(\delta\boldsymbol{e}|\boldsymbol{x})
\end{align*}
with $Q(\delta\boldsymbol{e}|\boldsymbol{x})$ the \emph{action value} map, that is a mixture of transformation maps given the view $\boldsymbol{x}$ and the marginal posterior estimate $p(U|\boldsymbol{x})$. 

We assume in the following that a parametric action value map $Q_\psi$ can be trained on top of the parametric classifier $p_\theta$ and its log accuracy map $r_\theta$.
The training set is $\{(\boldsymbol{x}_1, \boldsymbol{u}_1), ..., (\boldsymbol{x}_n, \boldsymbol{u}_n)\}$ and the classifier learns to associate each $\boldsymbol{x}$ with its full action value map $Q(.|\boldsymbol{x})$. 

\subsubsection{Algorithms}

Once $p_\theta$ and $Q_\psi$ are trained, the recognition algorithm is straightforward:  

\paragraph{Single saccade algorithm:}
\begin{enumerate}
	\item Read the view $\boldsymbol{x}$
	\item Choose $\delta\boldsymbol{e}$ according to $Q_\psi(.|\boldsymbol{x})$
	\item Move the eye 
	\item Update the view $\boldsymbol{x}'$
	\item Identify the target with $\tilde{\boldsymbol{y}} \sim p_\theta(Y|\boldsymbol{x}',\hat{\boldsymbol{u}}')$
\end{enumerate}


\paragraph{Multi saccades algorithm:}
\begin{enumerate}
\item $\pi(Y) \leftarrow$ uniform distribution
\item Read the view $\boldsymbol{x}$
\item Choose $\delta\boldsymbol{e}$ according to $Q_\psi(.|\boldsymbol{x})$
\item Repeat several times up to posterior confidence threshold:
	\begin{enumerate}
		\item Move the eye 		
		\item Read $\boldsymbol{x}$
		\item $\pi(Y) \leftarrow \pi(Y) \times p_\theta(Y|\boldsymbol{x},\hat{\boldsymbol{u}}')$
		\item Normalize $\pi$ 
		\item Choose $\delta\boldsymbol{e}$ according to $Q_\psi(.|\boldsymbol{x})$ (with some inhibition of return mechanism)
	\end{enumerate}
\end{enumerate}

