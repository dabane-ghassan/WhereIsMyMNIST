% !TEX root = DauceAlbigesPerrinet2020.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US
\section{Introduction}
\label{sec:intro}
\subsection{Problem statement}
%------------------------------%
% >>>>>>>>>>>> LuP is here <<<<<<<<<<<<<<<<<<<<<<<
%: see Figure~\ref{fig:intro}
%Past 10 years have seen the disrupting development of deep learning based image processing. Indeed, t
The field of computer vision was recently recast by the outstanding capability of convolution-based deep neural networks to capture the semantic content of images and photographs. There are now many image categorization tasks for which human performance is outreached by computer algorithms~\cite{He15}. One of the reasons explaining this breakthrough is a strong reduction in the number of parameters used to train the network, through a massive sharing of weights in the convolutional layers.
%Initially trained on energy greedy, high performance computers, these algorithms are now designed to work on more common hardware such as desktop computers with dedicated GPU hardware~\cite{Sandler18}.
Reducing the number of parameters and/or the size of the visual data that needs to be processed is thus a key for further improvements, and despite lots of efforts since both in  hardware and software optimization, the processing of pixel-based images is still done at a cost that scales linearly with the image size, for all the pixels present in the image, even the ones that are useless for the task at hand, are systematically processed by the computer algorithm. % in parallel through dedicated hardware at a significant computational cost.
%Inherent to this problem is the combinatorial explosion State-of-the art classification
Current computer vision algorithms consequently manipulate millions of pixels and variables with a subsequent energy consumption, even in the case of downsampled images, and with a still prohibitive cost for large images and videos. The need to detect visual objects at a glance while running on resource-constrained embedded hardware, for instance in autonomous driving, introduces a necessary trade-off between efficiency and accuracy, that is in urgent need to be addressed under renewed mathematical and computational frameworks.

In contrast, when human vision is considered, things work differently. First, the  general performance {\color{magenta} for an ecological real-time sensory flow is still greater than that of computer vision}.
Indeed, object recognition can be achieved by the human visual system both rapidly, --~in less than 100 ms~\cite{Kirchner06}~-- and at a low energy cost ($<5~W$).
On top of that, it is mostly self-organized, robust to visual transforms or lighting conditions and can learn with few examples. If many different anatomical features may explain this efficiency, a main difference lies in the fact that its sensor (the retina) combines a non homogeneous sampling of the world with the capacity to rapidly change its center of fixation. On the one hand, the retina is composed of two separate systems: a central, high definition fovea (a disk of about 6 degrees of diameter in visual angle around the center of gaze) and a large, lower definition peripheral area~\cite{Strasburger11}.
On the other hand, the human vision is \emph{dynamic}. The retina is attached on the back of the eye which is capable of low latency, high speed eye movements. In particular, saccades are stereotyped eye movements that allow for efficient changes of the position of the center of gaze: they take about $200~\ms$ to initiate, last about $200~\ms$ and usually reach a maximum velocity of approx 600 degrees per second~\cite{Bahill75}. The scanning of a full visual scene is thus not done in parallel but sequentially, and only scene-relevant regions of interest are scanned through saccades. This implies a \emph{decision process} at each step that decides \emph{where to look next}. This behavior is prevalent in biological vision (about a saccade every 2-3 seconds, that is, almost a billion saccade in a lifetime). The interplay of peripheral search and focal inspection allows human observers to engage in an integrated action/perception loop which sequentially scans and analyses the different parts of the image.

%The promise of artificial vision to identify objects in natural images is ever increasing.
% However, these algorithms are still far from human performances, even for simple tasks.

{\color{green} VISUAL SEARCH IN GENERAL}
{\color{magenta} Take for instance the case of an encounter with a friend in a crowded café. To catch the moment of his/her arrival, a face-seeking visual search is needed under heavy sensory clutter conditions. To do so, relevant parts of the visual scene need to be scanned sequentially with the gaze. Each saccade may potentially allow  to recognize your friend, provided it is accurately focused on each target face.} {\color{green} Je suis pas tres a l'aise avec cet exemple?...}. The main feature of this task is thus the monitoring of a particular \emph{class} of objects (e.g. human faces) in the periphery of the visual field  before the actual eye displacement, and the processing of the foveal visual data.
%toward it given all their possible spatial configurations and respective geometrical visual transformations. 
Searching for \emph{any} face in a peripheral and crowded display needs thus to precede the recognition of a specific face \emph{identity}.


%It is one type of active inference~\cite{Friston12} (see below) and we will envision herein how to incorporate it to classical computer vision schemes.
% (1 / 2.5 * 3600 * 24 * 365 * 75 = 946080000.0 ~= .95e9) X (wakeful + REM = .66)
%
\subsection{State of the art}

%To take benefit from this visuomotor behavior, it is important to understand both its computational and neurophysiological principles. 
The visual search problem, that is finding and identifying objects in a visual scene, is a classical task in computer vision, appealing to both
%. Addressing apparently simple questions such as ``find the green bottle on the table'', it is of broad interest in 
machine learning, signal processing and robotics. Crucially, it also speaks to neuroscience, for it refers to the mechanisms underlying foveation and more generally to low-level attention mechanisms.
When restricted to a mere ``feature search''~\cite{Treisman80}, many computational solutions are proposed in the computer vision literature. Notably, recent advances in deep-learning have been proven efficient to solve the task with models such as faster-RCNN~\cite{Ren17} or YOLO~\cite{Redmon16}. %5}.
Typical object search implementations predict in the image the probability of proposed bounding boxes around visual objects. While rapid, the number of boxes may significantly increase with image size and the approach more generally necessitates dedicated hardware to run in real time \cite{feng2019computer}. Under fine-tailored algorithmic and material optimization, the visual search problem can be considered ins the best case as \emph{linear} in the number of pixels \cite{strengert2006pyramid}, which still represents a heavy load for real-time image processing. This poses the problem of the \emph{scaling} of current computer vision algorithms to large/high definition visual displays. The scaling problem becomes even more crucial when considering the visual world of living beings.

In parallel, human visual scan-path over natural images provides ways to define \emph{saliency maps}, that quantify the attractiveness of the different parts of an image that are consistent with the detection of objects of interest. Essential to understand and predict saccades, they also serve as phenomenological models of attention. Estimating the saliency map from  a luminous image is a classical problem in neuroscience, that was shown consistent with a distance from baseline image statistics known as the ``Bayesian surprise''~\cite{Itti01}. The saliency approach was recently updated using deep learning to estimate saliency maps over large databases of natural images~\cite{Kummerer17}. %6}.
While  efficient at predicting the probability of fixation, these methods miss an essential component in the action perception loop: they operate on the full image while the retina operates on the non-uniform, foveated sampling of visual space (see Figure~\ref{fig:intro}-B).
Herein, we believe that this fact is an essential factor to reproduce and understand the active vision process.

Foveated models of vision have been considered for long time in robotics and computer vision as a way to leverage the visual scene scaling problem. Focal computer vision relies on a non-homogeneous compression of an image, that maintains the pixel information at the center of fixation and strongly compresses it at the periphery, including pyramidal encoding~\cite{kortum1996implementation,Butko2010infomax}, local wavelet decomposition~\cite{dauce2018active} and log-polar encoding~\cite{fischer2007self,Traver10}.
A recent deep-learning based implementation of such compression shows that in a video flow, a log-polar sampling of the image is sufficient to provide a reconstruction of the whole image~\cite{Kaplanyan19}. However, such algorithm lacks a system predicting the best saccading action to perform. In summary, though focal and multiscale encoding is now largely considered in static computer vision, sequential implementations have not been shown effective enough to overtake static object search methods.
% {\color{red} \textbf{rev2} In line 81, authors claim that “sequential implementations have not been shown effective enough to overtake static object search methods.” However, there are at least three models (FOD, TAM and Infomax) which wer  shown to be superior to (in some performance metric) classical sliding-window approach. So, the authors should revise their statement.}
Several implementations of a focal sequential search in visual processing can be found in the literature, with various degrees of biological realism~\cite{mnih2014recurrent,fu2017look}, that often rely on a simplified focal encoding, long training procedures and bounded sequential processing. More realistic attempts to combine foveal encoding and sequential visual search can be found in~\cite{Butko2010infomax,denil2012learning,dauce2018active}, {\color{magenta} to which our approach is compared later on}.
% {\color{red} \textbf{Rev 2}
% In line 86, authors say “can be found in [10,11,16] that will be compared further on with our approach.” However, this comparison is only in the form of a discussion (lines 555-585). Upon reading line 86, I expected to see experimental comparisons. In fact, no actual (experimental) comparisons with other models are done. As a reader, at the least, I expected a comparison with a baseline model which implements the classical sliding-window method. }

In contrast to phenomenological (or ``bottom-up'') approaches, active models of vision~\cite{Najemnik05,Butko2010infomax,Friston12} provide the ground principles of saccadic exploration. In general, they assume the existence of a generative model from which both the target position and category can be inferred through active sampling. This comes from the constraint that the visual sensor is foveated but can generate a saccade.
Several studies are relevant to our endeavor. First, one can consider optimal strategies to solve the problem of the visual search of a target~\cite{Najemnik05}. In a setting similar to that presented in Figure~\ref{fig:intro}-A, where the target is an oriented edge and the background is defined as pink noise, authors show first that a Bayesian ideal observer comes out with an optimal strategy, and second that human observers are close to that optimal performance. Though well predicting sequences of saccades in a perception action loop, this model is limited by the simplicity of the display (elementary edges added on stationary noise, a finite number of locations on a discrete grid) and by the abstract level of modeling. Despite these (inevitable) simplifications, this study could successfully predict some key characteristics of visual scanning such as the trade-off between memory content and speed. Looking more closely at neurophysiology, the study of~\cite{Samonds18} allows to go further in understanding the interplay between saccadic behavior and the statistics of the input. In this study, authors were able to manipulate the size of saccades by monitoring key properties of the presented (natural) images. For instance, smaller images generate smaller saccades. %Interestingly, they also predicted the size of saccades for different species, including mice which lack a foveal region, from the size of visual receptive fields.{ \color{magenta} One key prediction of this study which is relevant for our problem is the fact that saccades seem optimal to \emph{a priori} decorrelate the visual input, that is, to minimize redundancy in the sequence of generated saccades, knowing the statistics of the visual inputs.}

A further modeling perspective is provided by~\cite{Friston12}. In this setup, a full description of the visual world is used as a generative process. An agent is completely described by the generative model governing the dynamics of its internal beliefs and is interacting with this image by scanning it through a foveated sensor, just as described in Figure~\ref{fig:intro}. Thus, equipping the agent with the ability to actively sample the visual world %enables to explore the idea that actions (saccadic eye movements) are
allows to interpret saccades as optimal experiments, by which the agent seeks to confirm predictive models of the (hidden) world. One key ingredient to this process is the (internal) representation of counterfactual predictions, that is, the probable consequences of possible hypothesis as they would be realized into actions (here, saccades). Following such an active inference scheme~\cite{Mirza18} numerical simulations reproduce sequential eye movements that fit well with empirical data. %Compared to~\citet{Najemnik05},
Saccades %are not the output of a value-based cost function, but
are here a consequence of an active seek for the agent to minimize the uncertainty about his beliefs, knowing his priors on the generative model of the visual world.

\subsection{Outline}


Stemming from the active vision principles, our aim is to produce a principled and resource-effective model of vision.
We start from an elementary visual search problem, that is how to locate an object in a large, cluttered image, and take human vision as a guide for efficient design.  {\color{green} Motivation du modèle à développer. }

Our framework is made as general as possible, with minimal mathematical treatment, to speak largely to fragmented domains, such as machine learning, neuroscience and robotics. {\color{magenta} We thus expect our foveated active vision model to be helpful for each of those domains.}
The paper is organized as follows.
After this introduction, the principles underlying accuracy-based saccadic control are defined in the second section. {\color{magenta} We first define notations, variables and equations for the generative process governing the experiment and the generative model for the active vision agent.}
Complex combinatorial inferences are here replaced by separate pathways, i.e. the spatial (``Where'') and categorical (``What'') pathways, whose output is combined to infer optimal eye displacements and subsequent identification of the target. Our agent, equipped with a foveated sensor, should learn an optimal behavior strategy to actively scan the visual scene.  Numerical simulations are presented in the results section, demonstrating the applicability of this framework to tasks with different complexity levels. %This allows us to derive some limits of the agent and, as in~\cite{Najemnik05}, {\color{magenta} [TODO] we draw some analogies with biologically observed eye movements}.
The discussion section finally summarizes the results, showing its relative advantages in comparison with other frameworks, and providing ways toward possible improvements.
Implementation details are provided in the methods section, giving ways to reproduce our results,  showing in particular how to simplify the learning using accuracy-driven action maps.

%It is known that inverting a generative model over a large (one-step ahead) hypothesis space of all possible saccades is computationally-intensive. % (think for instance of face category as a very large categorical space over a large visual transformation space) with no obvious neurophysiological counterpart. (see Figure~\ref{fig:intro}-C)
%Although we similarly include a generative process of the visual world,

%as conatining {\bf (containing??)} images of a handwritten random digit (drawn from the MNIST database) at a random position and embedded in a cluttered noise .
%. % as defined by a generative (internal) modelWe will use this constraint as an asset
%to which also contributes to minimizing the overall computational cost of finding a target.
%Taking such priors, we
%and explore its key properties.
