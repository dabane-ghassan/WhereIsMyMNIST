% !TEX root = paper.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US
\section{Introduction}

\subsection{Problem statement}

The prominence of automatic methods to identify objects in natural images is ever increasing. The performance of such systems recently reached the same performance as human observers [ref needed on imagenet]. Moreover, these systems which were trained on energy greedy, high performance computers are now designed to work on more common hardware such as desktop computers with a decent GPU. However, such methods are not yet available for mobile devices, as will be necessary for instance for the fast detection of visual objects in autonomous driving, as when one has to identify a pedestrian from a sign pole. More importantly, the robustness of such methods is still lower than that of humans. Indeed, it is still difficult for such a system to learn to categorize a particular object class given all the possible spatial configurations and the respective geometrical visual transformations. This explosion of combinations is currently handled by increasing accordingly the number of parameters, hence the energy consumption of such methods. As a consequence, state-of-the art classification architectures contain many millions parameters while still handling relatively small images.

On the contrary, the human visual system is able to perform such a feat very rapidly (less than 100 ms~\citep{Kirchner06}) and at a low energy cost (< 5W). On top of that, the system is mostly autonomous, robust to visual transform or lighting conditions and can learn with a few examples. If many different anatomical features may explain such efficiency, a main difference of the human visual system with classical computer vision approaches in the fact that its sensor (the retina) combines a non homogeneous sampling of the world with the capacity to rapidly change its center of fixation. Indeed, on the one hand, the retina is composed of two separate systems: a central, high definition fovea and a large, peripheral area. On the other hand, the retina is attached on the back of the eye which is capable of low latency, high speed (> 500 degrees per second) eye movements. In particular, saccades allow for efficient changes of the position of the center of gaze. The interplay of those two properties allow to engage observers in an action / perception-cycle which sequentially scans the different parts of the image. This behavior is prevalent during our lifetime (2/3 saccades per second = Zillions per life). This behavior is one type of active inference~\citep{Friston2012} and we will envision herein how to incorporate it to classical computer vision schemes.

To explain and take advantage of this visual behavior, it is of particular importance to understand its computational and biological (neurophysiological) principles. One main hypothesis regarding this active vision is that visual scenes most often consist of a single visual object of interest. Take for instance the case of a conversation with a friend air a noisy cafe. To ease the understanding of his voice and emotion you will track his face despite all the remaining visual clutter. Such a visual experience-can be simplified in a manner reminiscent to psychophysical experiments. An observer is asked to classify digits (for instance as taken from the MNist database) as they are shown on a computer display. However, these digits can be placed at a random positions on the display, and visual clutter is added as a background to the image (see Figure~\ref{fig:intro}-A). This defines more precisely our problem: how do we identify a small object in a large image while not knowing its position?

This joint problem of localization and identification is the classical problem of visual search in neuroscience. Such problem is very general and can address complex questions such as "find the green bottle on the table". Here, we will restrict ourselves to a simple "feature search"~\citep{Treisman80}. Such a problem found many solutions in computer vision. Notably, recent advances in deep-learning have provided with efficient models such as faster-RCNN~\citep{Ren17} or Yolo~\citep{Redmon15}. This last implementation is particularly interesting as it predicts in the image the probability of proposed bounding boxes around the visual object. While rapid, the amount of such boxes greatly increases with image size and necessitates a dedicated hardware. When limiting our problem to a few objects of interest in the image, this strategy amounts to a classical problem in neuroscience, that is, the transformation of a luminous image into a saliency map~\citep{Itti01}. Such a computation is essential to understand and predict saccades but also as models of attention. Recently, deep learning methods have extended this  model by learning the computation of saliency maps over large databases of natural images [Kammerer 2014]. While these methods are efficient at predicting the probability of fixation, they miss an essential point in the action perception cycle: They operate on the full image while the retina operates on the non-uniform, foveated sampling of visual space ( see Figure~\ref{fig:intro}-B). As such, we believe that this is an essential step to reproduce and understand this active vision process.

An interesting perspective is given with previous modeling of foveated sensors. Such non-uniform sampling of visual space is often modeled as a log-polar conformal mapping~\citep{Traver10} which has a long history in computer vision and Robotics. A first property of this mapping is the separation between the foveal and the peripheral areas as we defined above. this transformation has also other notable properties, such as the correspondence (by way of translations) in the radial and angular directions to rotations and scalings (respectively) in the visual domain. However, this sensor is most often not coupled to an action (but see [ref needed)). In this paper, we aim at addressing the fragmentation of these studies respective to their fields (Machine learning, neuroscience, robotics) to propose a novel computational model of foveated active vision.


 %------------------------------%
%: see Figure~\ref{fig:intro}
%: \seeFig{map}
\begin{figure}%[!ht]%%[p!]
%\centering{\includegraphics[width=\linewidth]{figure_map}}
%\vspace*{-.1cm}
\caption{
{\bf Problem setting}:
(A)~After a fixation period, one observer is presented with a luminous display which shows a target (here a digit) at a random position. The display is presented for a short period but enough to perform a saccade on the potential target. In particular, the configuration of the display is such that by adding clutter and reducing the sire of the digit it may become necessary to perform a saccade to be able to identify the digit. %Finally, the observer identifies the digit.
(B)~We show a prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window used to ensure fixation during that window (green shaded area). Overlaid is a simulation of the retinotopic map at the onset of the display and after a (successful) saccade. This demonstrates that the position of the target has to be interred from a degraded (sampled) image and that a correct identification is mediated by the action to the location of the target \emph{before seeing it}.
\label{fig:map}}%
\end{figure}%
%%------------------------------%





\subsection{State of the art}


\subsection{Outline}
