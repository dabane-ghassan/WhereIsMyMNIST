% !TEX root = paper.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US
\section{Introduction}
\label{sec:intro}
\paragraph{Problem statement}
%------------------------------%
%: see Figure~\ref{fig:intro}
\begin{figure}[b!]%%[p!]
	\centering{	\includegraphics[width=\linewidth]{fig_intro}} %
	\caption{%
		{\bf Problem setting}: In ecological settings, the visual system faces a tricky problem when searching for \emph{any} target in a cluttered environment. It is synthesized in the following experiment: %
		(A)~After a fixation period ('FIX') of $200~\ms$, an observer is presented with a luminous display ('DIS') showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of $500~\ms$ (light shaded area in B), that is enough to perform at most one saccade on the potential target ('SAC'). In particular, the configuration of the display is such that by adding clutter and reducing the size of the digit it may become necessary to perform a saccade to be able to identify the digit.  Finally, the observer has to identify the digit by a keypress ('ANS'). %
		(B)~Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window used to ensure fixation during that window (green shaded area). %
		(C)~Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display ('DIS') and after a (successful) saccade ('SAC'), the dashed red box indicates the visual area of the ``what'' pathway. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from this degraded (sampled) image and that a correct identification by the ``what'' pathway is mediated by the ``where'' pathway and an action to the location of the target \emph{before seeing it}, that is before being able to actually be able to identify the target's category.%
		\label{fig:intro}}%
\end{figure}%
%%------------------------------%

The promise of image processing to identify objects in natural images is ever increasing. The performance of recent algorithms outreaches that of human observers in specific image categorization tasks~\citep{He15}. Initially trained on energy greedy, high performance computers, they are now designed to work on more common hardware such as desktop computers with a decent GPU~\citep{Sandler18}.
They are however not yet available for mobile devices, or fast enough to detect visual objects in autonomous driving, as when one has to distinguish a pedestrian from a sign pole {\bf (ref?)}. More importantly, their robustness is still lower than that of humans. Indeed, a still difficult task is to learn to categorize a particular object class given all possible spatial configurations and respective geometrical visual transformations. This explosion of combinations is currently handled by increasing accordingly the number of parameters, hence the energy consumption of such methods. As a consequence, state-of-the art classification architectures contain many millions parameters while still handling relatively small images.

The human visual system, in contrast, is able to perform such a feat very rapidly, in less than 100 ms~\citep{Kirchner06}, and at a low energy cost ($<5~W$). On top of that, this system is mostly autonomous, robust to visual transforms or lighting conditions and can learn with a few examples. If many different anatomical features may explain such efficiency,  A main difference of human vision with respect to computer vision lies in the fact that its sensor (the retina) combines a non homogeneous sampling of the world with the capacity to rapidly change its center of fixation. Indeed, on the one hand, the retina is composed of two separate systems: a central, high definition fovea (a disk of about 6 degrees of diameter in visual angle around the center of gaze) and a large, lower definition peripheral area. On the other hand, the retina is attached on the back of the eye which is capable of low latency, high speed eye movements.  In particular, saccades allow for efficient changes of the position of the center of gaze: They take about $200~\ms$ to initiate, last about $200~\ms$ and usually reach a maximum velocity of approx 600 degrees per second. This behavior is prevalent during our lifetime (about a saccade every 2-3 seconds, that is, almost a billion saccade in a lifetime).  The interplay of those two features allows human observers to engage in an action perception loop which sequentially scans and analyses the different parts of the image.
%It is one type of active inference~\citep{Friston12} (see below) and we will envision herein how to incorporate it to classical computer vision schemes.
% (1 / 2.5 * 3600 * 24 * 365 * 75 = 946080000.0 ~= .95e9) X (wakeful + REM = .66)

Take for instance the case of a conversation with a friend at a noisy cafÃ©. To ease the understanding of his voice and emotion you will track his face despite all the remaining sensory clutter by scanning relevant parts of his face with your gaze {\bf (exemple un peu trop ``simple''?)}. 
 Such a visual experience can be formalized in a manner reminiscent to classical psychophysical experiments: An observer is asked to classify digits (for instance as taken from the MNIST database) as they are shown on a computer display. However, these digits can be placed at random positions on the display, and visual clutter is added as a background to the image (see Figure~\ref{fig:intro}-A). This opens the possibility that the position of this object may be detected in the clutter without being able to identify it before making a saccade to it (see Figure~\ref{fig:intro}-C). This defines more precisely our problem: how do we localize a small object in a large image while knowing \emph{a priori} its class but not its category? This generic visual search problem is of broad general interest in machine learning, computer vision and robotics, but also to neuroscience, as it speaks to the mechanisms underlying foveation and more generally to low-level attention mechanisms.





\paragraph{State of the art}


To take advantage of this visual behavior, it is of particular importance to understand both its computational and neurophysiological principles.
Active vision methods~\citep{Najemnik05,Butko2010infomax,Friston12}, that provide the ground principles of saccadic exploration, assume the existence of a generative model from which both the target position and category can be inferred through active sampling. This comes from the constraint that the visual sensor is foveated but can generate a saccade. 
Several studies are relevant to our endeavor. First, one can consider optimal strategies to solve the problem of the visual search of a target~\citep{Najemnik05}. In a setting similar to that presented in Figure~\ref{fig:intro}-A, where the target is an oriented edge and the background is defined as pink noise, authors show first that a Bayesian ideal observer provides with an optimal strategy, and second that human observers are close to that optimal performance. Though they can predict a sequence of saccades in this perception action loop, this model is limited by the simplicity of the display (elementary edges added on stationary noise and a finite number of locations on a discrete grid) and by the abstract level of modeling. Despite these (inevitable) simplifications, this study could successfully predict some key characteristics of visual scanning such as the trade-off between memory content and rapidity.

Looking more closely at neurophysiology, the study of~\citep{Samonds18} allows to go further in our understanding of the interplay between saccadic behavior and the statistics of the input. In this study, authors were able to manipulate the size of saccades by manipulating key properties of the presented (natural) images. For instance, smaller images generate smaller saccades. Interestingly, they also explained the size of saccades for different species, including mice which lack a foveal region, by the size of visual receptive fields. One key prediction of this study which is relevant for our problem is the fact that saccades seem optimal to \emph{a priori} decorrelate the visual input, that is, to minimize redundancy in the sequence of generated saccades, knowing the statistics of visual inputs.

A further modeling perspective is provided by~\citep{Friston12}. In this model, we first have a full description of the visual world as a generative process, here of the presentation of faces, knowing they are constituted of independent components: mouth, nose, eyes, etc... An agent is completely described by giving the generative model governing the dynamics of its internal beliefs and is interacting with this image by scanning it through a foveated sensor, just as we described in Figure~\ref{fig:intro}. Equipping the agent with the ability to actively sample the visual world enables to explore the idea that actions (saccadic eye movements) are optimal experiments, by which the agent seeks to confirm predictive models of the (hidden) world. One key ingredient to this process is the (internal) representation of counterfactual predictions, that is, of the probable consequences of possible hypothesis as they would be realized into actions (here, saccades).

Such a model constitutes an Active Inference scheme~\citep{Mirza18} and simulations of the resulting optimization scheme reproduce sequential eye movements which fit well with empirical data. Compared to~\citet{Najemnik05}, saccades are not the output of a value-based cost function but a consequence of the seek for the agent to minimize the uncertainty about his beliefs, knowing his priors on the generative model of the visual world. Such an approach applies well to our setting, as described in Figure~\ref{fig:intro}. 



\paragraph{Outline}
The goal of this work is thus to emulate a model of active vision which is able to infer the position of a visual target independently from its category.
Stemming from the active vision general principles, our aim is to produce a realistic model that may both explain the essential features of human vision and provide ways toward efficient computer implementations.
However, inverting a generative model over a large (one-step ahead) hypothesis space of all possible saccades is computationally-intensive. % (think for instance of face category as a very large categorical space over a large visual transformation space) with no obvious neurophysiological counterpart.  (see Figure~\ref{fig:intro}-C)
Although we similarly include a generative process of the visual world, complex inferences are replaced by separate and cheaper pathways, i.e. the spatial (``where'') and categorical (``what'') pathways, whose knowledge is combined to infer optimal eye displacement and subsequent identification.    
%as conatining {\bf (containing??)} images of a handwritten random digit (drawn from the MNIST database) at a random position and embedded in a cluttered noise . 
In addition,  the agent is equipped with a foveated sensor, %and with the ability to actively scan the visual image as defined by a generative (internal) modelWe will use this constraint as an asset to 
which also contributes to minimize the overall computational cost of finding a target. Knowing such priors, we optimize the behavior of this agent and explore its key properties.
To that aim, this paper is organized as follows.

After this introduction, we define the methods in section~\ref{sec:methods}. First we  define notations, variables and equations for the generative process governing the experiment and the generative model for the active vision agent. In particular, we  derive our method to simplify the learning of an optimal agent given these definitions. In section~\ref{sec:results}, we  then show results of numerical simulations of this agent. We  first demonstrate some applications of this framework to different levels of complexity of the problem. This  allows us to derive some limits of this agent and, as in~\citep{Najemnik05}, we  draw some analogies with biologically observed eye movements. Finally, in section~\ref{sec:discussion}, we  summarize these results in comparison with other similar schemes. We  conclude by showing the relative advantages of using this active inference approach.
