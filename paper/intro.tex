% !TEX root = paper.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US
\section{Introduction}
\label{sec:intro}
\paragraph{Problem statement.}
%------------------------------%
%: see Figure~\ref{fig:intro}


Past 10 years have seen the disrupting development of deep learning based image processing. 
Indeed the field of computer vision has been recast by the outstanding capability of convolution-based deep networks to capture the semantic content of images and photographs. Image processing algorithms recently outreached the performance of human observers in specific image categorization tasks~\citep{He15}. Their success relies on a reduction of parameter complexity through weight sharing  in convolutional neural networks applied over the full image. 
Initially trained on energy greedy, high performance computers, they are now designed to work on more common hardware such as desktop computers with dedicated GPU hardware~\citep{Sandler18}.
Despite lot of efforts spent in optimizing the processing costs, the processing of large images is  done at a cost that scales linearly with the image size. All regions, even the “boring” ones are systematically scanned and processed in parallel through dedicated hardware at a significant computational cost.
%Inherent to this problem is the combinatorial explosion State-of-the art classification
Image processing architectures consequently contain millions of parameters with subsequent energy consumption while still handling relatively small images. This introduces a trade-off between efficiency and average accuracy, for instance in autonomous driving, with the need to detect visual objects at a glance while running on resource-constrained embedded hardware. 

In contrast, when human vision is considered, things work differently. First, the  general performance is still greater than that of computer vision.
Indeed, object recognition can be achieved by the human visual system both rapidly, --~in less than 100 ms~\citep{Kirchner06}~-- and at a low energy cost ($<5~W$). 
On top of that, it is mostly self-organized, robust to visual transforms or lighting conditions and can learn with a few examples. If many different anatomical features may explain this efficiency, a main difference lies in the fact that its sensor (the retina) combines a non homogeneous sampling of the world with the capacity to rapidly change its center of fixation. On the one hand, the retina is composed of two separate systems: a central, high definition fovea (a disk of about 6 degrees of diameter in visual angle around the center of gaze) and a large, lower definition peripheral area. 
On the other hand, the human vision is \emph{dynamic}. The retina is attached on the back of the eye which is capable of low latency, high speed eye movements. In particular, saccades allow for efficient changes of the position of the center of gaze: they take about $200~\ms$ to initiate, last about $200~\ms$ and usually reach a maximum velocity of approx 600 degrees per second. The scanning of a full visual scene is thus  not done in parallel but sequentially, and only scene-relevant regions of interest are scanned through saccades. This implies a \emph{decision process} at each step that decides \emph{where to look next}. This behavior is prevalent during our lifetime (about a saccade every 2-3 seconds, that is, almost a billion saccade in a lifetime). The interplay of those two features allows human observers to engage in an integrated action perception loop which sequentially scans and analyses the different parts of the image.

%The promise of artificial vision to identify objects in natural images is ever increasing.
% However, these algorithms are still far from human performances, even for simple tasks. 

Take for instance the case of an encounter with a friend in a crowded café. To catch the moment at which she arrives, you need to visually search for her face despite the sensory clutter in the visual field. To do so, you need to scan relevant parts of the visual scene with your gaze. Doing a saccade at these locations will allow you to recognize your friend. The main difficulty of this task is to identify a particular object \emph{class} (e.g. human faces) given all their possible spatial configurations and respective geometrical visual transformations. Searching for \emph{any} face in a peripheral and crowded display needs to precede the recognition of a specific face identity. 


%It is one type of active inference~\citep{Friston12} (see below) and we will envision herein how to incorporate it to classical computer vision schemes.
% (1 / 2.5 * 3600 * 24 * 365 * 75 = 946080000.0 ~= .95e9) X (wakeful + REM = .66)
%
\paragraph{State of the art.}

To take benefit from this visuomotor behavior, it is important to understand both its computational and neurophysiological principles. First, the joint problem of target localization and identification is a classical problem of visual search in computer vision. It is very general and may address apparently simple questions such as ``find the green bottle on the table''. 
When restricted to a mere ``feature search''~\citep{Treisman80}, many solutions are proposed. Notably, recent advances in deep-learning have provided efficient models such as faster-RCNN~\citep{Ren17} or YOLO~\citep{Redmon15}. 
Their object search implementations predict in the image the probability of proposed bounding boxes around visual objects. While rapid, the number of boxes may significantly increases with image size and the approach more generally necessitates dedicated hardware to run in real time. 

In parallel, human visual scan-path over natural images provide ways to define \emph{saliency maps}, that quantify the attractiveness of the different parts of an image, that are consistent with the detection of objects of interest. Essential to understand and predict saccades, they also serve as phenomenological models of attention. Estimating the saliency map from  a luminous image is a classical problem in neuroscience, that was shown consistent with a distance from baseline image statistics known as the ``Bayesian surprise''~\citep{Itti01}. The saliency approach was recently updated using deep learning to estimate saliency maps over large databases of natural images~\citep{Kummerer16}. 
While these methods are efficient at predicting the probability of fixation, they miss an essential component in the action perception loop: they operate on the full image while the retina operates on the non-uniform, foveated sampling of visual space (see Figure~\ref{fig:intro}-B). 
Herein, we believe that this fact is an essential factor to reproduce and understand the active vision process.

Foveated models of vision have been considered for long time in robotics and computer vision as a way to leverage the visual scaling problem. Focal image processing relies a non-homogeneous compression of an image, that maintains the pixel information at the center of fixation and strongly compresses it at the periphery, including pyramidal encoding \citep{kortum1996implementation,Butko2010infomax}, local wavelet decomposition \citep{dauce2018active} and logpolar encoding \citep{fischer2007self,Traver10}. Though focal and multiscale encoding is now largely considered in static computer vision, sequential implementations have not been shown effective enough to overtake static object search methods. Several implementations of a focal sequential search in visual processing can be found in the literature, with various degrees of biological realism \citep{mnih2014recurrent,fu2017look}, that often rely on a simplified focal encoding, long training procedures and bounded sequential processing. More realistic attempts to combine foveal encoding and sequential visual search can be found in \citep{Butko2010infomax,denil2012learning,dauce2018active}, that will be compared further on with our approach.


In contrast to phenomenological (or ``bottom-up'') approaches, active models of vision~\citep{Najemnik05,Butko2010infomax,Friston12} provide the ground principles of saccadic exploration. In general, they assume the existence of a generative model from which both the target position and category can be inferred through active sampling. This comes from the constraint that the visual sensor is foveated but can generate a saccade. 
Several studies are relevant to our endeavor. First, one can consider optimal strategies to solve the problem of the visual search of a target~\citep{Najemnik05}. In a setting similar to that presented in Figure~\ref{fig:intro}-A, where the target is an oriented edge and the background is defined as pink noise, authors show first that a Bayesian ideal observer comes out with an optimal strategy, and second that human observers are close to that optimal performance. Though well predicting sequences of saccades in a perception action loop, this model is limited by the simplicity of the display (elementary edges added on stationary noise, a finite number of locations on a discrete grid) and by the abstract level of modeling. Despite these (inevitable) simplifications, this study could successfully predict some key characteristics of visual scanning such as the trade-off between memory content and speed. Looking more closely at neurophysiology, the study of~\citep{Samonds18} allows to go further in understanding the interplay between saccadic behavior and the statistics of the input. In this study, authors were able to manipulate the size of the saccades by monitoring key properties of the presented (natural) images. For instance, smaller images generate smaller saccades. Interestingly, they also predicted the size of saccades for different species, including mice which lack a foveal region, from the size of visual receptive fields. One key prediction of this study which is relevant for our problem is the fact that saccades seem optimal to \emph{a priori} decorrelate the visual input, that is, to minimize redundancy in the sequence of generated saccades, knowing the statistics of the visual inputs.

A further modeling perspective is provided by~\citep{Friston12}. In this setup, a full description of the visual world is used as a generative process. An agent is completely described by giving the generative model governing the dynamics of its internal beliefs and is interacting with this image by scanning it through a foveated sensor, just as described in Figure~\ref{fig:intro}. Thus, equipping the agent with the ability to actively sample the visual world %enables to explore the idea that actions (saccadic eye movements) are 
allows to interpret saccades as optimal experiments, by which the agent seeks to confirm predictive models of the (hidden) world. One key ingredient to this process is the (internal) representation of counterfactual predictions, that is, the probable consequences of possible hypothesis as they would be realized into actions (here, saccades). Following such an active inference scheme~\citep{Mirza18} numerical simulations reproduce sequential eye movements that fit well with empirical data. %Compared to~\citet{Najemnik05}, 
Saccades %are not the output of a value-based cost function, but 
are here a consequence of an active seek for the agent to minimize the uncertainty about his beliefs, knowing his priors on the generative model of the visual world. 

\paragraph{Outline.}
Stemming from the active vision general principles, our aim is to produce a principled model that may both explain the essential features of human vision and provide ways toward efficient computer implementations. We also aim at reunifying the fragmentation of the many different approaches respective to their fields (Machine learning, neuroscience, robotics), and envisage an integrated computational model of foveated active vision. It is known that inverting a generative model over a large (one-step ahead) hypothesis space of all possible saccades is computationally-intensive. % (think for instance of face category as a very large categorical space over a large visual transformation space) with no obvious neurophysiological counterpart. (see Figure~\ref{fig:intro}-C)
%Although we similarly include a generative process of the visual world,
Herein, we hypothesize that complex combinatorial inferences can be replaced by separate pathways, i.e. the spatial (``where'') and categorical (``what'') pathways, whose knowledge is combined to infer optimal eye displacements and subsequent identification of the target. 
%as conatining {\bf (containing??)} images of a handwritten random digit (drawn from the MNIST database) at a random position and embedded in a cluttered noise . 
We will thus define an agent equipped with a foveated sensor and with the ability to actively scan the visual image, %. % as defined by a generative (internal) modelWe will use this constraint as an asset 
%to which also contributes to minimizing the overall computational cost of finding a target. 
%Taking such priors, we 
learn an optimal behavior strategy and explore its key properties.

This paper is organized as follows: After this introduction, we define the principles underlying accuracy-based saccadic control in section~\ref{sec:principles}. We first define notations, variables and equations for the generative process governing the experiment and the generative model for the active vision agent. In particular, we derive our method to simplify the learning of an optimal agent given these definitions. In section \ref{sec:implementation}, implementation details are given, providing ways to reproduce our results. In section~\ref{sec:results}, preliminary results of numerical simulations of the agent are presented, demonstrating the applicability of this framework to different task complexity levels. This allows us to derive some limits of the agent and, as in~\citep{Najemnik05}, we draw some analogies with biologically observed eye movements. Finally, in section~\ref{sec:discussion}, we summarize these results in comparison with other similar schemes. We conclude by showing the relative advantages of using this active inference approach.
