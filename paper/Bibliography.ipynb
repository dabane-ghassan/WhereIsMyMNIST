{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Karl Friston et al. **Perceptions as hypotheses: Saccades as experiments**. In: Frontiers in Psychology 3.MAY (2012), pp. 1–20. ISSN : 16641078. DOI : 10.3389/fpsyg.2012.00151 . arXiv: NIHMS150003\n",
    "\n",
    "* **Klaus Obermayer** : The successful candidate will explore the hypothesis that object-level attentional units are essential mid-level factors which guide human eye-movements in visual scene analysis. Based on eye-fixation data from visual search tasks she/he will first build computational models to emulate the measured fixation sequences, to quantify the influence of different low- and high-level visual features, and to characterize the influence of task-driven changes in object-based attention processes. In a second step, plausible models will be integrated as “attentional modules” into a computer vision system for visual scene analysis and will be evaluated in terms of task success and the number of computations involved. Potential achievement of the project is an efficient real-time analysis of dynamic visual scenes.\n",
    "  \n",
    "* **Constantin Rothkopf** at the Technical University Darmstadt. The position\n",
    "is initially funded by the German Research Foundation DFG for three\n",
    "years. The project 'Active vision: control of eye-movements and\n",
    "probabilistic planning' aims to investigate how humans carry out\n",
    "eye-movements across different tasks and how this can be understood with\n",
    "models of probabilistic planning. Recent publications of the lab on this\n",
    "topic include the following papers:\n",
    "  - Hoppe, D., & Rothkopf, C. A. (2016). Learning rational temporal eye\n",
    "movement strategies. Proceedings of the National Academy of Sciences,\n",
    "113(29), 8332-8337.\n",
    "  - Hoppe, D., & Rothkopf, C. A. (2019). Multi-step planning of eye\n",
    "movements in visual search. Scientific reports, 9(1), 144.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency map models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Laurent Itti and Christof Koch. **A saliency-based search mechanism for overt and covert shifts\n",
    "of visual attention**. In: Vision Research 40.10-12 (2000), pp. 1489–1506."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Najemnik / Where is Waldo? / accuracy model\n",
    "  * J Najemnik and Wilson S. Geisler. **Optimal eye movement strategies in visual search**. In: Nature\n",
    "reviews. Neuroscience 434 (2005)\n",
    "- Infomax model : \n",
    "  * Nicholas J Butko and Javier R Movellan. **Infomax control of eye movements**. In: Autonomous\n",
    "Mental Development, IEEE Transactions on 2.2 (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep / transformer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID deep learning = feed forward alors que la vision est active :\n",
    "  - vision = multi-steps process\n",
    "  - approche basée sur le contrôle\n",
    "\n",
    "**Spatial transformer tutorial** : https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html\n",
    "\n",
    "par contre c'est une transfo affine, alors que nous, on pourraitait faire une transfo log-polaire...\n",
    "\n",
    "\n",
    "  * Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu **Spatial Transformer Networks**. https://arxiv.org/abs/1506.02025\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * M. Kümmerer, L. Theis, and M. Bethge **Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet**\n",
    "ICLR Workshop, 2015\n",
    "    * https://arxiv.org/pdf/1411.1045.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fovea models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Philip Kortum and Wilson S. Geisler. **Implementation of a foveated image coding system for image bandwidth reduction**. In: SPIE Proceedings 2657 (1996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nicholas J. Priebe.\n",
    "**Mechanisms Of Orientation Selectivity In Primary Visual Cortex**\n",
    "Annual Review of Vision Science. 2016, 2(1) .\n",
    "\n",
    "* A. S. Ecker, F. H. Sinz, E. Froudarakis, P. G. Fahey, S. A. Cadena, E. Y. Walker, E. Cobos, J. Reimer, et al.\n",
    "**A rotation-equivariant convolutional neural network model of primary visual cortex**\n",
    "International Conference on Learning Representations (ICLR), 2019.\n",
    "    * https://openreview.net/pdf?id=H1fU8iAqKX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crowding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothèse:__ un système pré-attentif permet un gain de performance en classification par rapport au système par défaut et qu'en cas de crowding, ce gain est perdu.\n",
    "\n",
    "* Ziskind, A.J., Hénaff, O., LeCun, Y., & Pelli, D.G. (2014) **The bottleneck in human letter recognition: A computational model**. Vision Sciences Society, St. Pete Beach, Florida, May 16-21, 2014, 56.583. http://f1000.com/posters/browse/summary/1095738\n",
    "\n",
    "* D. Pelli (2018) **Despite a 100-fold drop in cortical magnification, a fixed-size letter is recognized equally well at eccentricities of 0 to 20 deg. How can this be?.** Journal of Vision 2018;18(10):26. https://jov.arvojournals.org/article.aspx?articleid=2699020&resultClick=1\n",
    "\n",
    "* J. Zhou, N. Benson, J. Winawer, D. Pelli (2018) **Conservation of crowding distance in human V4**. Journal of Vision2018;18(10):856. https://jov.arvojournals.org/article.aspx?articleid=2699845&resultClick=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> identify gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * M. Kümmerer, T. Wallis, and M. Bethge. **Information-theoretic model comparison unifies saliency metrics**\n",
    "Proceedings of the National Academy of Science, 112(52), 16054-16059, 2015 \n",
    "    * http://www.pnas.org/content/112/52/16054.abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Pysaliency**  : M. Kümmerer, T. S. A. Wallis, and M. Bethge \n",
    "**Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics**\n",
    "The European Conference on Computer Vision (ECCV), 2018. \n",
    "    * https://github.com/matthias-k/pysaliency\n",
    "    * http://bethgelab.org/media/publications/1704.08615.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * https://etra.acm.org/2019/challenge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
