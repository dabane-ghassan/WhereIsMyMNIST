We emulate a model of active vision which aims at finding a visual target whose position and identity are unknown.
This generic visual search problem is of broad interest to machine learning, computer vision and robotics, but also to neuroscience, as it speaks to the mechanisms underlying foveation and more generally to low-level attention mechanisms. From a computer vision perspective, the problem is generally addressed by processing the different hypothesis (categories) at all possible spatial configuration through dedicated parallel hardware. The human visual system, however, seems to employ a different strategy, through a combination of a foveated sensor with the capacity of rapidly moving the center of fixation using saccades. 
Visual processing is done through fast and specialized pathways, one of which mainly conveying information about target position and speed in the peripheral space (the "where" pathway), the other mainly conveying  information about the identity of the target (the "what" pathway). The combination of the two pathways is expected to provide most of the useful knowledge about the external visual scene. Still, it is unknown why such a separation exists.
Active vision methods~\citep{Najemnik05,Butko2010infomax,Friston12,dauce2018active} provide the ground principles of saccadic exploration, assuming the existence of a generative model from which both the target position and identity can be inferred through active sampling.
Taking for granted that (i) the position and category of objects are independent and 
(ii) the visual sensor is foveated, we consider how to minimize the overall computational cost of finding a target. 
This justifies the design of two complementary processing pathways: first a classical image classifier, assuming that the gaze is on the object, and second a peripheral processing pathway learning to identify the position of a target in retinotopic coordinates. 
This framework was tested on a simple task of finding digits in a large, cluttered image (see figure). Results demonstrate the benefit of specifically learning where to look, and this before actually identifying the target category (with cluttered noise ensuring the category is not readable in the periphery). 
In the ``what'' pathway, the accuracy drops to the baseline at mere 6 pixels away from the center of fixation, while issuing a saccade is beneficial in up to 26 pixels around, allowing a much wider covering of the image. The difference between the two distributions forms an ``accuracy gain'', that quantifies the benefit of issuing a saccade with respect to a central prior. Until the central classifier is confident, the system should thus perform a saccade to the most likely target position. 
The different accuracy predictions, such as the ones done in the ``what'' and the ``where'' pathway, may also explain more elaborate decision making, such as the inhibition of return. The approach is also energy-efficient as it includes the strong compression rate performed by retina and V1 encoding, which is preserved up to the action selection level. The computational cost of this active inference strategy may thus be way less than that of a brute force framework. This provides evidence of the importance of identifying "putative interesting targets" first and we highlight some possible extensions of our model both in computer vision and modeling.	
\if 0\CNS	
Visual processing in the brain is done through fast and specialized pathways, one of which mainly conveying information about target position and speed in the peripheral space (the "where" pathway), the other mainly conveying  information about the category of the target (the "what" pathway). The combination of the two pathways is expected to provide most of the useful knowledge about the external visual scene. Still, it is unknown why such a separation exists.
From a computational perspective, it is indeed possible to train a classifier that may recognize both the target's category and position in a single  pass. Current solutions leverage this problem by processing the different hypothesis (categories) at all possible spatial configuration. However, this may be costly in terms of computing time especially without dedicated parallel hardware.
In contrast, the human visual system employs a combination of a foveated sensor with the capacity of rapidly moving the center of fixation using saccades. However, in the foveated image subjects need to locate where the target is without necessarily identifying what it is.  This indeeds provides the capacity to sequentially process visual information, by first identifying the target position in the periphery of the retina, and, after actuating a saccade, identify the target category through detailed retinocentric examination. Indeed, the position and category of objects in images are a priori independent and we hypothesize that the primary retinotopic layers overlay a central area mostly dedicated to object categorization and a peripheral area mostly dedicated to object position in space.
We formalized this problem in a probabilistic setting, allowing to design two interacting image processing pathways: first a classical image classifier, assuming that the gaze is  on the object on the one side, combined with a second processing pathway learning to identify the position of a target (whatever it is) on the other side. Until the classifier is confident, the system should perform a saccade to the most likely target position in the image. Overall, the computational cost of this active inference strategy is less than that in a classical, holistic framework.
We tested this framework on a simple task of finding digits in a large, cluttered image. Results demonstrate that it is possible to correctly learn the position of a putatively interesting visual target, and this before actually identifying its category through foveation (with cluttered noise ensuring the category is actually unreadable in the periphery). We compared the results of this model with classical psychophysical results in visual search {\bf (???)}. This provides evidence of the importance of identifying "putative interesting targets" first in computer vision and we highlight some predictions of our model.
\fi
% TODO: ACTIVE INFERENCE? LOGPOLAR ENCODING? SALIENCY MAPS?