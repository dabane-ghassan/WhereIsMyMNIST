We emulate an active vision model which aims at finding a target whose position and category are unknown. From a computer vision perspective, this visual search problem is generally addressed by inferring the different possible identities at all possible spatial configuration. However, this may be costly in terms of computing time. The human visual system, however, seems to employ a different strategy, through a combination of a foveated sensor with the capacity of rapidly moving the center of fixation using saccades. Visual processing is done through two specialized pathways, one mainly conveying information about target position and speed in the peripheral space (the "where" pathway), the other mainly information about the category of the target (the "what" pathway). The interplay between the two pathways is expected to provide most of the useful knowledge about the external visual scene. Still, it is unknown why such a separation exists. We will propose an active vision process as the ground principles of saccadic exploration, assuming the existence of a generative model from which both the target position and category can be inferred through active sampling. We will take for granted that the position and category of objects are independent and (ii) the visual sensor is foveated, we consider how to minimize the overall computational cost of finding a target. This justifies the design of two complementary processing pathways: first a classical image classifier, assuming that the gaze is on the object, and second a peripheral processing pathway learning to infer the position of a target in retinotopic coordinates, independently to its category. This framework was tested on a simple task of finding digits in a large, cluttered image. Results demonstrate the benefit of specifically learning where to look, and this before actually identifying the target category (with cluttered noise ensuring the category is not readable in the periphery). In the “what” pathway, the accuracy drops to the baseline at mere 5 pixels away from the center of fixation, while issuing a saccade is beneficial in up to 26 pixels around, allowing a much wider covering of the image. The difference between the two distributions forms an “accuracy gain”, that quantifies the benefit of issuing a saccade with respect to a central prior. Until the central classifier is confident, the system should thus perform a saccade to the most likely target position. The different accuracy predictions, such as the ones done in the “what” and the “where” pathway, may also explain more elaborate decision making, such as the inhibition of return. The approach is also energy-efficient as it includes the strong compression rate performed by retina and V1 encoding, which is preserved up to the action selection level. The computational cost of this active inference strategy may thus be way less than that of a brute force framework. This provides evidence of the importance of identifying "putative interesting targets" first and we highlight some possible extensions of our model both in computer vision and modeling.
% TODO: We compared the results of this model with classical psychophysical results in visual search
% TODO: ACTIVE INFERENCE? LOGPOLAR ENCODING? SALIENCY MAPS?
