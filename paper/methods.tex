% !TEX root = paper.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US
%=================================================================
\section{Principles}
\label{sec:principles}
%=================================================================
%=================================================================
%------------------------------%

\begin{figure}[t!]%[b!]%%[p!]
	\centering{	\includegraphics[width=\linewidth]{fig_intro}} %
	\caption{%
		{\bf Problem setting}: In generic, ecological settings, the visual system faces a tricky problem when searching for one target (from a class of targets) in a cluttered environment. It is synthesized in the following experiment: %
		\A After a fixation period \FIX\ of $200~\ms$, an observer is presented with a luminous display \DIS\ showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of $500~\ms$ (light shaded area in B), that is enough to perform at most one saccade on the potential target (\SAC , here successful). Finally, the observer has to identify the digit by a keypress \ANS . %
		\B Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window \FIX\ and the temporal window during which a saccade is possible (green shaded area). %
		\C Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display \DIS\ and after a saccade \SAC , the dashed red box indicating the visual area of the ``what'' pathway. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from a degraded (sampled) image. In particular, the configuration of the display is such that by adding clutter and reducing the size of the digit, it may become necessary to perform a saccade to be able to identify the digit. The computational pathway mediating the action has to infer the location of the target \emph{before seeing it}, that is, before being able to actually identify the target's category from a central fixation. %
		\label{fig:intro}}%
\end{figure}%
%%------------------------------%

%: see Figure~\ref{fig:methods}
\begin{figure}[t!]%%[p!]
\centering{\includegraphics[width=\linewidth]{fig_methods}}
\caption{%
{\bf Methods for simulating active vision}:
\A Input images are composed of three different random processes: one choosing a sample image from the MNIST database (of size $28\times 28$) and placing it at a random position within the circular mask on the $128\times 128$ display. Then, this image is rectified and multiplied by a contrast factor and finally embedded in a natural-like noise characterized by noise contrast, mean spatial frequency and bandwidth~\citep{Sanz12} (see an example in Figure~\ref{fig:intro}-A, \DIS ). %
\B This full-sized image is transformed into 2 images: a central foveal-like central snippet which is fed to a classification network (``what'' pathway) and a retinal-like image which will be fed to the ``where'' pathway. This log-polar transform is implemented by a bank of filters whose centers are positioned on a log-polar grid and whose radius increases proportionally with eccentricity. %
\C 
%The ``where'' pathway is implemented by a three-layered neural network consisting of the retinal input, two hidden layers with $1000$ units each and a log-polar output. To train that network, we used a pre-calculated accuracy map which records the average accuracy over the testing set, knowing a mis-estimation in both axis (respectively $\Delta X$ and $\Delta Y$) of the actual position of the target. %
The ``what'' network is implemented using the three-layered LeNet CNN~\citep{Lecun1998}, while the ``where'' network is implemented by a three-layered neural network consisting of the retinal input, two hidden layers with $1000$ units each and a collicular-like accuracy map.  This map has a similar topographic map and predicts the accuracy of each hypothetical position of a saccade.  Each layer associates a fully-connected linear transform with a ReLU non-linearity. To learn to associate the output of the network with the ground truth, supervised training is performed using back-propagation with a binary cross entropy loss. 
\D 
If the predicted accuracy in the output of the ``where'' network is higher than that predicted in the ``what'' network, the position of maximal activity in the ``where'' pathway serves to generate a saccadic-like action which moves the center of gaze. % Then, and classify the foveal image using the ``what'' pathway. %
%\E TODO describe log polar
%\F TODO accuracy map 
\label{fig:methods}}%
\end{figure}%
%%------------------------------%

In our setting, the search experience is formalized and simplified in a way reminiscent to classical psychophysic experiments: an observer is asked to classify digits (for instance as taken from the MNIST database) as they are shown on a computer display. However, these digits can be placed at random positions on the display, and visual clutter is added as a background to the image (see Figure~\ref{fig:intro}-A). This opens the possibility that the position of the object may be detected in the clutter without being identified in the first place (see Figure~\ref{fig:intro}-C). This defines more precisely our problem: how do we localize an object in a large image while knowing \emph{a priori} its category but not its identity? This generic visual search problem is of broad interest in machine learning, computer vision and robotics, but also in neuroscience, as it speaks to the mechanisms underlying foveation and more generally to low-level attention mechanisms.

In this study, the visual scene is made of a target object placed in the foreground at a random position over a noisy background. An agent controls a foveal visual sensor that can move over the visual scene through saccades (see Figure~\ref{fig:intro}).
%We will define a neural network which implements this control process.
The agent aims at understanding the visual scene, here identifying both the target position and identity from visual samples.

\subsection{Active inference}
Active inference assumes a hidden external state $e$, which is known indirectly through its effects on the sensor. The external state corresponds to the physical environment. The visual field $x$ is the state of the sensors, that is, a partial view of the visual scene, measured through a generative process : $x\sim p(X|e)$. The real physical state $e$ being hidden, a parametric model $\theta$ is assumed to allow for an estimate of the cause of the current visual field through model inversion thanks to Bayes formula, in short:
$$p(E|x) \propto p(x|E;\theta)$$


The external state is assumed to split in two (independent) components, namely $e = (u,y)$ with $u$ the interoceptive body posture (in our case the gaze orientation) and $y$ the object shape (or object identity).
It is also assumed that a set of motor commands $A = \{..., a, ...\}$ (here saccades) may control the body posture, but not the object's identity, so that $y$ is invariant to $a$.

In a predictive setup, the consequence of every saccade should be analyzed through model inversion \emph{over the future observations}, that is, predicting the effect of every action to choose the one that may optimize future inferences. The benefit of each action should be quantified through a certain metric (future accuracy, future posterior entropy, future variational free energy, ...), that depend on the current inference $p(U,Y|x)$. The saccade $a$ that is selected thus provides a new visual sample from the scene statistics. If well chosen, it should improve the understanding of the scene (here the target position and category). However, estimating in advance the effect of every action over the range of every possible object shapes and body postures is combinatorially hard, even in simple cases such as vision, and thus infeasible in practice.

%To infer the position of the target, we will use the fundamental hypothesis outlined in Figure~\ref{fig:intro}: The position of an object is independent from its category.
The predictive setup necessitates in practice to restrain the generative model in order to reduce the range of possible combinations. One such restriction, known as the ``Na√Øve Bayes'' assumption, considers the independence of the factors that are the cause of the sensory view.
The independence hypothesis allows considering the position $u$ and the category $y$ being independently inferred from the current visual field, i.e $p(U,Y|x) = p(U|x) p(Y|x)$. This property is strictly true in our setting and is very generic in vision for simple classes (such as digits) and simple displays (but see~\citep{Vo12} for more complex visual scene grammars).
%
This independence assumption allows to separate the scene analysis in two independent tasks. A first task consists in identifying the target (namely inferring $y$ from $x$) and a second task consists in localizing the target (namely inferring $u$ from $x$). Each task is moreover assumed to be realized in parallel through distinct computational pathways, that are referred as the
``What'' and the ``Where'' pathways by analogy with the brain (see figure \ref{fig:methods})
%\bf [ref needed]. \emph{Note that %from the retinotopic projection of the visual information,
%this independence is conditional on action: both pathways should update their beliefs upon decisions made in each respective pathway {\bf (je ne comprends pas bien cette phrase?)}}}\fi. 
However, we will here simplify the setting by considering only one possible saccade.
Each pathway is here assumed to rely on different sensor morphologies. By analogy with biological vision, the target identification is assumed to rely on the very central part of the retina (the fovea), that comes with higher density of cones, and thus higher spatial precision. In contrast, the target localization should rely on full visual field, with peripheral regions having a lower sensor density and a lesser sensitivity to high spatial frequencies.

\subsection{Metric training}
Next, the effect of a saccade is to shift the visual field from one place to another.
Concretely, each saccade provokes a new visual field $x'$ and a new subjective position $u'$, while the target identity $y$ remains unchanged. Examining the current visual field $x$ allows to form two hypotheses, namely $p(U|x)$ and $p(Y|x)$. It may happen, however, that the current inferences may not be accurate enough and there may be a ``better'' eye direction from which more evidence could be grabbed, i.e. it may be worth issuing a saccade so that $p(U'|x')$ and $p(Y|x')$ should be more accurate. Choosing the next saccade thus means using a model to predict how accurate $p(U|x)$ and $p(Y|x)$ will be after the saccade realization.
% a full sequence of operations comprises first an initial visual examination through the where and the what pathways. This . followed by ($ii$) a decision, ($iii$) a saccade realization and ($iv$) a second visual examination that should finally ($v$) determine the category of the target.
It is worth noting that active inference needs either the current identity $y$ or the current eye direction $u$ to be readable from the present view, in order to effectively predict future inferences, through computationally intensive predictions. In detail, modeling the full sequence of operations that lead to both estimate $p(U'|x')$ and $p(Y|x')$ means predicting the future visual field $x'$ over all possible saccades, that may be tremendously costly in case of large visual fields. 
Better off instead is to form a statistics over the (scene understanding) benefit obtained from past saccades in the same context, that is forming an \emph{accuracy map} from the current view. This is the essence of the \emph{sampling-based metric prediction} that we develop here. The putative effect of every saccade should be condensed in a single number, the \emph{accuracy}, that quantifies the final benefit of issuing saccade $a$ %regarding the target identity, both assuming $p(U|\boldsymbol{x})$ and $p(Y|\boldsymbol{x})$
from the current observation $x$. If $a$ is a possible saccade and $x'$ the corresponding future visual field, the result of the categorical classifier over $x'$ can either be correct (1) or incorrect (0).
If this experiment is repeated many times over many visual scenes, the probability of correctly classifying the future visual field $x'$ from $a$ forms a probability, i.e. a number between 0 and 1, that reflects the proportion of correct and incorrect classifications.
% when issuing a saccade $a$ after seeing $\boldsymbol{x}$ (the initial visual field).
It more or less corresponds to inferring the true target identity $\hat{y}$, i.e. $p(\hat{y}|x')$, including the update of the eye direction, that is a sample of the ``real'' generative process.
In a biological setting, this would be acchieved for instance by catch-up saccades that would scan the area neighboring the saccade that was actually issued. To sum up, a main assumption here is that instead of trying to detect the actual position of the target, it is better for the agent to estimate how accurate the categorical classifier will be after moving the eye. This forms an accuracy map that may be learned through trials and errors, by actuating saccades %after processing the visual input,
and taking the final classification success or failure as a teaching signal.
Such a \emph{predictive accuracy map} is assumed to be the core of a realistic saccade-based vision system.

Note that compared to a brute force approach which would scan for all possible positions in an image, this map should compress the information, as exemplified by a retinotopic map. The map should be mostly organized radially, preserving the initial retinotopic organization. %with high predicted accuracies reflecting a high probability of target presence at given locations.
The operations that transform the initial primary visual data should preserve the initial retinotopic organization, so as to form a final retinotopic accuracy map (see figure \ref{fig:methods}). Accordingly with the initial data, the retinotopic accuracy map may thus provide more detailed accuracy predictions in the center, and coarser accuracy predictions in the periphery.
%and telling how accurate the categorical classifier will be after the saccade is carried out~\citep{Dauce18}. %The set of all possible saccade predictions should
Finally, each different initial visual field may bring out a different accuracy map, indirectly conveying information about the target retinotopic position.
A final action selection (motor map) should then overlay the accuracy map through a winner-takes-all mechanism, implementing the saccade selection in biologically plausible way, as it is thought to be done in the superior colliculus, a brain region responsible for oculo-motor control [TODO:ref needed].

\subsection{Information gain and the inhibition of return}
From the information theory standpoint, each saccade comes with fresh visual information about the visual scene that can be quantified by an \emph{information gain}, namely:
\begin{align}
\text{IG}_\text{max} &= \max_{u'} \log p(y|u',x',x, u) - \log p(y|x, u)\\
     &\simeq \max_{u'} \log p(y|x') - \log p(y|x)
\end{align}
with the left term representing the future accuracy (after the saccade is realized) and the right term representing the current accuracy as it is obtained from the 'what' pathway. The accuracy gain may be averaged over many saccades and many initial eccentricities (so that the information gain may be close to zero when the initial $u$ is very central).

For the saccade is subject to predictions errors and execution noise, the actual $u'$ may be different from the initial prediction. The final accuracy, as instantiated in the accuracy map, contains this intrinsic imprecision, and is thus necessary lower than the optimal one. The consequence is that in some cases, the approximate information gain may become negative, when the future accuracy is actually lower than the current one. This is for instance the case when the target is centered on the fovea. This should encourage the agent to select a saccade ``away'' from the central position, which is reminiscent of a well-known phenomenon in vision known as the ``inhibition of return''~\citep{Itti01}. Combining accuracy predictions from each pathway may thus allow to refine saccades selection in a way that complies with biological vision.
%Our main argument is that such an accuracy map is trainable in a rather straightforward way,

\section{Implementation}
\label{sec:implementation}
To test the validity of our hypothesis, let us find a function implementing the ``where'' network. This function should be able to find the position of an object knowing only the degraded retinal image. Here, we describe the methods that we will follow to find that function, from the generative models (first external and then internal) to the actual implementation of the ``where'' pathway. %

\subsection{Exteroceptive Generative model}
%=================================================================
We define here the generative model for input display images as shown first in Figure~\ref{fig:intro}-A (\DIS ) and as implemented in Figure~\ref{fig:methods}-A.

\paragraph{Targets.} Following a common hypothesis regarding active vision, visual scenes consist of a single visual object of interest. We use the MNIST database of handwritten digits introduced by~\citet{Lecun1998}: %Indeed, we are here focused on the problem of localization (``where'' pathway) and classification solutions (``what'' pathway) abound for this class of targets.
Samples are drawn from the database of $60000$ grayscale $28\times 28$ pixels images and separated between a training and a validation set (see below the description of the ``where'' network).

\paragraph{Full-scale images.} Each sample position is draw a random in a full-scale image of size $128\times 128$. To enforce isotropic saccades, a centered circular mask covering the image (of radius $64$ pixels) is defined, and the position is such that the embedded sample fits entirely into that circular mask.

\paragraph{Background noise setting.} To implement a realistic background noise, we generate synthetic textures~\citep{Sanz12} using a bi-dimensional random process. %The generated texture images are of the same size than the full-image.
The texture is designed to fit well with the statistics of natural images. We chose an isotropic setting where textures are characterized by solely two parameters, one controlling the median spatial frequency $sf_0$ of the noise, the other controlling the bandwidth around the central frequency. Equivalently, this can be considered as the band-pass filtering of a random white noise image. Finally, these images are rectified to have a normalized contrast.

\paragraph{Mixing the signal and the noise.} Finally, both the noise and the target image are merged into a single image. Two different strategies are used. A first strategy emulates a transparent association, with an average luminance computed at each pixel, while a second strategy emulates an opaque association, choosing for each pixel the maximal value. The quantitative difference was tested in simulations, but proved to have a marginal importance.
%
\subsection{Interoceptive generative model}
%=================================================================
%
We now define the simplified anatomy of the agent, which is composed of two separate pathways.

\paragraph{Foveal vision and the ``what'' pathway}
First, foveal vision is defined as the $28\times 28$ pixels image centered at the point of fixation (see dashed red box in Figure~\ref{fig:intro}-C). This image is then directly passed to the agent's visual categorical pathway (the ``What'' pathway). This is realized by the known ``LeNet'' classifier~\citep{Lecun1998}, that processes the $28 \times 28$ central pixels to identify the target category. Such a network is directly provided (and unmodified) by the pyTorch library~\citep{Paszke17}, and consists of a 3-layered Convolutional Neural Network. It is trained over the (centered) MNIST database after approx $20$ training epochs. %Input images are rectified (\emph{with a mean and standard deviation of respectively $0.1307$ and $0.3081$ {\bf why??}}).} \fi 
The network outputs a vector representing the probability of detecting each of the $10$ digits. We use the argument of the output neuron with maximum probability, to categorize each image. This strategy achieves an average $98.7\%$ accuracy on the validation dataset~\citep{Lecun1998}.%

\paragraph{Retinal transform: Peripheral vision and log Polar encoding}
%
% page 476 de :
%
% https://www.asc.ohio-state.edu/golubitsky.4/reprintweb-0.5/output/papers/6120261.pdf
%
% Estimates ofw0D0.087 and2D0.051 inappropriate units can be obtained from Drasdo‚Äôs data on the human retina,andaandbare constants. These values correspond to a magnification factor of 11 .5 mm/degrees of visual angle at the fovea and 5.75 mm/degrees
%An interesting perspective is given with previous modeling of such foveated sensors. 
The non-uniform sampling of visual space is adequately modeled as a log-polar conformal mapping~\citep{Traver10} which has a long history in computer vision and robotics. A first property of this mapping is the separation between the foveal and the peripheral areas as we defined above. This transformation has also other notable properties, such as the correspondence by way of translations in the radial and angular directions to respectively rotations and scalings in the visual domain. However, this sensor is to our knowledge most often not coupled to an action. 
%\if 0\ICANN
% {\color{blue}(but see~\citep{ref_needed)}}\fi. 

First, both the visual features and the expected target position may to be expressed in retinal coordinates which we choose here to be log-polar as it provides a good fit with observations in mammals~\citep{Traver10}. On the visual side, we extracted local visual features as oriented edges as the combination of the retinotopic transform with that of the primary visual cortex~\citep{Fischer2007a}. The centers of these first and second order orientation filters are radially organized around the center of fixation, with small and tightened receptive fields at the center and more large and scarce receptive fields at the periphery, see Figure~\ref{fig:methods}-B. The size of the filters increases proportionally to eccentricity. The filters are organized in $10$ spatial eccentricity scales (respectively placed at around $2$, $3$, $4.5$, $6.5$, $9$, $13$, $18$, $26$, $36.5$ , and $51.3$ pixels from the center) and $16$ different azimuth angles allowing them to cover most of the original $128 \times 128 $ image. At each of these position, we computed $10$ different edge orientations and $2$ different phases (symmetric and anti-symmetric) using log-Gabor filters~\citep{Fischer2007a}. This finally implements a (fixed) bank of linear filters which model the receptive fields of the input to the primary visual cortex.

From any input image ($128\times 128=16384$ pixels) is linearly transformed into a retinal activity vector $\boldsymbol{x}$. Note that to ensure the balance of the coefficients across scales, the images are first whitened. The length of this vector is $1600$ such that the retinal filter compresses the original image by about 90\%, with high spatial frequencies preserved at the center and only low spatial frequencies conserved at the periphery. In practice, this filters are pre-computed and placed into a matrix for a rapid transform of batches of input displays into retinal transforms. This matrix transformation allows also the evaluation of a reconstructed visual image given a retinal activity vector thanks to the pseudo-inverse matrix of the forward transform matrix. In summary, the full-sized images are transformed into a peripheral retinal image which will be fed to the ``where'' pathway.

\paragraph{Collicular representation: accuracy map}
The output of the ``Where'' pathway is defined as an \emph{accuracy map} representing the probability of the presence of a target in the visual field, independently of its identity. As the retinotopic map, this target accuracy map is also organized radially in a log-polar fashion, making the target position estimate more precise at the center and fuzzier at the periphery. This modeling choice is reminiscent of the approximate log-polar organization of the superior colliculus (SC) motor map. In ecological conditions, the accuracy map is trained by sampling, i.e. by "trial and error", using for instance corrective saccades to compute (a posteriori) the probability of a correct localization. In a computer simulation however, this induces a combinatorial explosion which does render the calculation not amenable.

However, as we generated the display, we know the position of the target (which is hidden to the agent). Moreover, we observed that we could also evaluate the accuracy of the classifier (that is, of the fixed ``what'' pathway) knowing the translational shift imposed to the input foveal image by a saccade of known amplitude. Knowing the size of the $28\times 28$ input image, this generates a $55\times 55$ accuracy map (larger shift correspond to a target outside the fovea and thus an accuracy corresponding to the chance level of $10\%$). This classifier displays a high accuracy at the center (with a value of $98.7\%$ corresponding to the validation score without any translational shift), and a¬†fast decreasing accuracy with target eccentricity. %, as shown in Figure~\ref{fig:results}-D. 
By assuming ergodicity, knowing the centered accuracy map allows to rapidly predict for each visual sample the full accuracy map at each pixel by shifting the centered accuracy map on the true position of the target. Such a computational shortcut is allowed by the independence of the categorical performance with position. % thanks to the independence which is implemented in the exteroceptive generative model.

Finally, this full accuracy map is log-polar projected to provide the expected accuracy of each hypothetical saccade in a retinotopic space (see Figure~\ref{fig:methods}-B). In practice, we use the energy of the filters at each position as a proxy to quantify the projection from the metric space of the display to the retinotopic space. This generates a filter bank at $10$ spatial eccentricity scales and $16$ different azimuth angles, and $160$ output filters. Each filter is normalized such that the value at each log-polar position is the average of the values which are integrated in visual space. Applied to the full sized ground truth accuracy map computed in metric space, this gives an accuracy map at different location of a retinotopic motor space. Such transform is again implemented by a simple matrix multiplication which can be pre-computed to fasten calculations. Practically, this also allows to compute an inverse transform using the pseudo-inverse matrix of the forward transform. In particular, we use that inverse transform to represent the accuracy predicted by any given log-polar vector, but also to compute the position of maximal accuracy in metric space to set up the sensor displacement.

%\subsection{Implementing the ``where'' pathway}
%=================================================================

\subsection{Classifier training using deep learning}
Modern parametric classifiers are composed of many layers (hence the term ``Deep Learning'') that can be trained through gradient descent over arbitrary input and output feature spaces. The ease of use of those tightly optimized training algorithms allows for the quantification of the difficulty of a task through the failure or success of such training. Consider the retinal transform $\boldsymbol{x}$ as the input and a log-polar retinotopic vector $\boldsymbol{a}$ made of $n$ Bernouilli probabilities (success probabilities) as the output. %Following the active inference framework, we will train
The network is trained to predict the distribution $\boldsymbol{a}$ knowing the retinal input $\boldsymbol{x}$ by comparing it to the known ground truth distribution computed over the motor map. As a loss function, we will naturally use the Kullback-Leibler divergence between the ground truth and the predicted map.

%The ``What'' pathway will be given from the literature. In contrast, the ``Where'' pathway takes the into account in order to tell whether a target is present at the different peripheral locations, in order to monitor future saccades.

In practice, the parametric neural network is made of an input (retinal) layer, two fully connected hidden layers of size 1000
%{\bf (!! 1000 !!)} $500$ and {\bf(!! 1000 !!)} $2000$ units respectively
and an output layer, with ReLu activation between each layer, except at the output which uses a sigmoid function to ensure that the output is compatible with the representation of a likelihood (see Figure~\ref{fig:methods}-C).
%In addition, we tested the effect of $50 \%$ drop-out on the last hidden layer.
Another improvement in convergence speed that was obtained by using batch normalization. The network is trained over $500,000$ saccades on full-images, using the binary cross-entropy loss as the error signal, with a learning rate equal to $10^{-4}$ and stochastic gradient descent with a momentum to improve convergence. This scalar measures the distance between both distributions (it is always positive and null if and only if they are equal). The training is done for $25$ epochs in about $1$ hours on a laptop. The code is written in Python (version 3.7.6) with pyTorch library~\citep{Paszke17} (version 1.1.0). The full scripts for reproducing the figures and extending the results to a full range of parameters is available at \url{https://github.com/laurentperrinet/WhereIsMyMNIST}. % TODO: change to https://github.com/SpikeAI/DauceAlbigesPerrinet19 when public
