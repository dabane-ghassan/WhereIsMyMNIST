% !TEX root = plos-paper.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US
%=================================================================
\section*{Principles}
\label{sec:principles}
%=================================================================
%=================================================================
%------------------------------%

\begin{figure}[t!]%[b!]%%[p!]
	\centering{	\includegraphics[width=\linewidth]{fig_intro}} %
	\caption{%
		{\bf Problem setting}: In generic, ecological settings, the visual system faces a tricky problem when searching for one target (from a class of targets) in a cluttered environment. It is synthesized in the following experiment: %
		\A After a fixation period \FIX\ of $200~\ms$, an observer is presented with a luminous display \DIS\ showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of $500~\ms$ (light shaded area in B), that is enough to perform at most one saccade on the potential target (\SAC , here successful). Finally, the observer has to identify the digit by a keypress \ANS. \emph{NB}: the target contrast is here enhanced for a better readability. %
		\B Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window \FIX\ and the temporal window during which a saccade is possible (green shaded area). %
		\C Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display \DIS\ and after a saccade \SAC , the dashed red box indicating the foveal region. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from a degraded (sampled) image. In particular, the configuration of the display is such that by adding clutter and reducing the contrast of the digit, it may become necessary to perform a saccade to be able to identify the digit. The computational pathway mediating the action has to infer the location of the target \emph{before seeing it}, that is, before being able to actually identify the target's category from a central fixation. }%
		\label{fig:intro} %
\end{figure}%
%%------------------------------%

%: 

For biological vision is the result of a continual optimization under strong material and energy constraints, we need to understand both its ground principles and its specific computational and material constraints in order to implement effective biomimetic vision systems.

In order to do so, we provide a simplified visual environment toward which a visual agent can act on.
The search experience is formalized and simplified in a way reminiscent to classical psychophysic experiments: an observer is asked to classify digits (for instance as taken from the MNIST dataset, as introduced by~\cite{Lecun1998}) as they are shown on a computer display.
However, these digits can be placed at random positions on the display, and visual clutter is added as a background to the image (see Figure~\ref{fig:intro}-A).
In order to vary the difficulty of the task, different parameters are controlled, such as the target eccentricity, the background noise period and and the signal/noise ratio (SNR).
The agent initially fixates the center of the screen. Due to the peripheral clutter, he needs to explore the visual scene through saccades to provide the answer. He controls a foveal visual sensor that can move over the visual scene through saccades (see Figure~\ref{fig:intro}-B). When a saccade is actuated, the center of fixation moves toward a new location, which updates the visual input (see Figure~\ref{fig:intro}-C).
The lower the SNR and the larger the initial target eccentricity, the more difficult the identification. There is a range of eccentricities for which it is impossible to identify the target from a single glance, so that a saccade is necessary to issue a proper response.
%We will define a neural network which implements this control process.
%The agent aims at understanding the visual scene, here identifying both the target position and identity from visual samples.
This implies in general that the position of the object may be detected in the first place in the peripheral clutter before being properly identified. 

This setup provides the conditions for a separate processing of the visual information.
%, with visuo-spatial information extracted from the peripheral clutter, and object detailed shape and identity extracted through central foveal examination. 
Indeed, in order to analyze a complex visual scene, there are two types of processing that need to be done. On the one side, you need  to analyze in detail what is at the center of fixation, that is the region of interest currently processed. On the other side, you also need to analyze the surrounding part, even if the resolution is low, in order to choose what is the next center of fixation. This basically means making a choice of “what’s interesting next”. You do not necessarily need to know what it is, but you need to know that it’s interesting enough, and of course you need to know what action to take to move the center of fixation at the right position. This is reminiscent of the What/Where separate visual processing separation observed in monkeys and humans ventral and dorsal visual pathways \cite{mishkin1983object}.


\subsection*{Active inference}

{\color{red} {\bf rev 2} Throughout the text, the variable “x” is described in several different ways: (i) state of sensor, (ii)  partial view of the scene, (iii) visual field, and (iv) visual sample. This might be confusing to the reader. Please be consistent.}

The previous reasoning can be captured by a statistical framework called a
partially observed Markov Decision Process (POMDP), {\color{red} \textbf{unclear (rev 2)} where the cause of a visual scene is couple made of
a viewpoint and scene elements}. Changing the viewpoint will conduct to a different scene rendering.
A generative model tells how the visual field should look knowing the scene elements and a certain viewpoint. 
In general, active inference assumes a hidden external state $e$, which is known indirectly through its effects on the sensor. The external state corresponds to the physical environment. Here the external state is assumed to split in two (independent) components, namely $e = (u,y)$ with $u$ the interoceptive body posture (in our case the gaze orientation, or ``viewpoint'') and $y$ the object shape (or object identity). The visual field $x$ is the state of the sensors, that is, a partial view of the visual scene, measured through the generative process : $x\sim p(X|e)$. 


Using Bayes rule, one may then infer the scene elements from the current view point (model inversion).
 The real physical state $e$ being hidden, a parametric model $\theta$ is assumed to allow for an estimate of the cause of the current visual field through model inversion thanks to Bayes formula, in short:
$$p(E|x) \propto p(x|E;\theta)$$


It is also assumed that a set of motor commands $A = \{..., a, ...\}$ (here saccades) may control the body posture, but not the object's identity, so that $y$ is invariant to $a$. Actuating a command $a$ changes the viewpoint to $u'$, which feeds the system with a new visual sample $x'\sim p(X|u', y)$. The more viewpoints you have, the more certain you are about the object identity through a chain rule sequential evidence accumulation. 

In an optimal search setup however \cite{Najemnik05}, you need to choose the next viewpoint that will help you \emph{the most} to disambiguate the scene.
In a predictive setup, the consequence of every saccade should be analyzed through model inversion \emph{over the future observations}, that is, predicting the effect of every action to choose the one that may optimize future inferences. The benefit of each action should be quantified through a certain metric (future accuracy, future posterior entropy, future variational free energy, ...), that depend on the current inference $p(U,Y|x)$. The saccade $a$ that is selected thus provides a new visual sample from the scene statistics. If well chosen, it should improve the understanding of the scene (here the target position and category). However, estimating in advance the effect of every action over the range of every possible object shapes and body postures is combinatorially hard, even in simplified setups, and thus infeasible in practice.

%To infer the position of the target, we will use the fundamental hypothesis outlined in Figure~\ref{fig:intro}: The position of an object is independent from its category.
The predictive approach necessitates in practice to restrain the generative model in order to reduce the range of possible combinations. One such restriction, known as the ``Naïve Bayes'' assumption, considers the independence of the factors that are the cause of the sensory view.
The independence hypothesis allows considering the viewpoint $u$ and the category $y$ being independently inferred from the current visual field, i.e $p(U,Y|x) = p(U|x) p(Y|x)$. This property is strictly true in our setting and is very generic in vision for simple classes (such as digits) and simple displays (but see~\cite{Vo12} for more complex visual scene grammars).
%


\subsection*{Metric training}
Next, the effect of a saccade is to shift the visual field from one place to another.
Concretely, each saccade provokes a new visual field $x'$ and a new subjective position $u'$, while the target identity $y$ remains unchanged. 
%Examining the current visual field $x$ allows to form two hypotheses, namely $p(U|x)$ and $p(Y|x)$. It may happen, however, that the current inferences may not be accurate enough and there may be a ``better'' eye direction from which more evidence could be grabbed, i.e. it may be worth issuing a saccade so that $p(U'|x')$ and $p(Y|x')$ should be more accurate. 
Choosing the next saccade thus means using a model to predict how accurate $p(U|x)$ and $p(Y|x)$ will be after the saccade realization.
% a full sequence of operations comprises first an initial visual examination through the where and the what pathways. This . followed by ($ii$) a decision, ($iii$) a saccade realization and ($iv$) a second visual examination that should finally ($v$) determine the category of the target.
%It is worth noting that active inference needs either the current identity $y$ or the current eye direction $u$ to be readable from the present view, in order to effectively predict future inferences, through computationally intensive predictions. 
In detail, modeling the full sequence of operations that lead to both estimate $p(U'|x')$ and $p(Y|x')$ means predicting the future visual field $x'$ over all possible saccades, that may yet be too costly in case of large visual fields. 
Better off instead is to form a statistics over the (scene understanding) benefit obtained from past saccades in the same context, that is forming an \emph{accuracy map} from the current view. This is the essence of the \emph{sampling-based metric prediction} that we develop here. The putative effect of every saccade should be condensed in a single number, the \emph{accuracy}, that quantifies the final benefit of issuing saccade $a$ %regarding the target identity, both assuming $p(U|\boldsymbol{x})$ and $p(Y|\boldsymbol{x})$
from the current observation $x$. If $a$ is a possible saccade and $x'$ the corresponding future visual field, the result of the categorical classifier over $x'$ can either be correct (1) or incorrect (0).
If this experiment is repeated many times over many visual scenes, the probability of correctly classifying the future visual field $x'$ from $a$ forms a probability, i.e. a number between 0 and 1, that reflects the proportion of correct and incorrect classifications.
% when issuing a saccade $a$ after seeing $\boldsymbol{x}$ (the initial visual field).
%It more or less corresponds to inferring the true target identity $\hat{y}$, i.e. $p(\hat{y}|x')$, including the update of the eye direction, that is a sample of the ``real'' generative process.
%In a biological setting, this would be acchieved for instance by catch-up saccades that would scan the area neighboring the saccade that was actually issued. 
To sum up, a main assumption here is that instead of trying to detect the actual position of the target, better off for the agent is to estimate how accurate the categorical classifier will be after moving the eye. Extended to the full action space $A$, this forms an accuracy map that may be learned through trials and errors, by actuating saccades %after processing the visual input,
and taking the final classification success or failure as a teaching signal.
Our main assumption here is that such a \emph{predictive accuracy map} is at the core of a realistic saccade-based vision systems.
{\color{red} \textbf{Unclear (rev 2)} Compared with a baseline approach that would predict for all possible gaze directions over an image}, this map should moreover be organized radially to preserve the retinotopic compression. 
%, as exemplified by a retinotopic map. The map , preserving the initial retinotopic organization. %with high predicted accuracies reflecting a high probability of target presence at given locations.

Finally, the independence assumption allows to separate the scene analysis in two independent tasks. %A first task consists in identifying the target (namely inferring $y$ from $x$) and a second task consists in localizing the target (namely inferring $u$ from $x$).
 Each task is assumed to be realized in parallel through distinct computational pathways, that will be referred as the
``What'' and the ``Where'' pathways by analogy with the ventral and dorsal pathways in the brain (see figure \ref{fig:methods}).
%\bf [ref needed]. \emph{Note that %from the retinotopic projection of the visual information,
%this independence is conditional on action: both pathways should update their beliefs upon decisions made in each respective pathway {\bf (je ne comprends pas bien cette phrase?)}}}\fi. 
%However, we will here simplify the setting by considering only one possible saccade.
Each pathway is here assumed to rely on different sensor morphologies. By analogy with biological vision, the target identification is assumed to rely on the very central part of the retina (the fovea), that comes with higher density of cones, and thus higher spatial precision. In contrast, the  saccade planning should rely on the full visual field, with peripheral regions having a lower sensor density and a lesser sensitivity to high spatial frequencies.
The operations that transform the initial primary visual data should preserve the initial retinotopic organization, so as to form a final retinotopic accuracy map (see figure \ref{fig:methods}C). Accordingly with the visual data, the retinotopic accuracy map may thus provide more detailed accuracy predictions in the center, and coarser accuracy predictions in the periphery.
%and telling how accurate the categorical classifier will be after the saccade is carried out~\cite{Dauce18}. %The set of all possible saccade predictions should
Finally, each different initial visual field may bring out a different accuracy map, indirectly conveying information about the target retinotopic position.
A final action selection (motor map) should then overlay the accuracy map through a winner-takes-all mechanism, implementing the saccade selection in biologically plausible way, as it is thought to be done in the superior colliculus, a brain region responsible for oculo-motor control \cite{sparks1987sensory}.
The saccadic motor output showing a similar log-polar compression than the visual input, the saccades should be more precise at short than at long distance (and several saccades may be necessary to precisely reach distant targets).

{\color{red} \textbf{Rev 2} The foveal processing module does a 10-way classification. But what about a no-digit example? Does the model predict “no-digit” or “background”? Is there a need for such a decision? Please discuss. 
}


\begin{figure}[t!]%%[p!]
	\centering{\includegraphics[width=\linewidth]{fig_methods}}
	\caption{%
		{\bf Computational graph}. Two streams of information are separated from the visual primary layers, one stream for processing the central pixels only, the other for processing the periphery with a logpolar encoding. The two streams converge toward a decision layer that compares the central and the peripheral acuracy, in order to decide whether to issue a saccadic or a categorical response. If a saccade is produced, then the center of vision is displaced toward the region that shows the higher accuracy on the accuracy map.
		\A The visual input is constructed the following way: first a  $128\times 128$  natural-like background noise is generated, characterized by noise contrast, mean spatial frequency and bandwidth~\cite{Sanz12}. Then a circular mask is put on. Last a sample digit is selected from the MNIST dataset (of size $28\times 28$), rectified, multiplied by a contrast factor and overlayed on the background at a random position (see an example in Figure~\ref{fig:intro}-A, \DIS ). %
		\B The visual input is then transformed in 2 ways: (i) a $28\times 28$ central foveal-like snippet is fed to a classification network (``What'' pathway) and (ii) a log-polar set of oriented visual features is fed to the ``Where'' pathway. This log-polar input is generated by a bank of filters whose centers are positioned on a log-polar grid and whose radius increases proportionally with the eccentricity. %
		\C 
		%The ``Where'' pathway is implemented by a three-layered neural network consisting of the retinal input, two hidden layers with $1000$ units each and a log-polar output. To train that network, we used a pre-calculated accuracy map which records the average accuracy over the testing set, knowing a mis-estimation in both axis (respectively $\Delta X$ and $\Delta Y$) of the actual position of the target. %
		The ``What'' network is implemented using the three-layered LeNet CNN~\cite{Lecun1998}, while the ``Where'' network is implemented by a three-layered neural network consisting of the retinal input, two hidden layers with $1000$ units each and a collicular-like accuracy map at the output.  This map has a similar retinotopic organization and predicts the accuracy of each hypothetical position of a saccade. % Each layer associates a fully-connected linear transform with a ReLU non-linearity. 
		To learn to associate the output of the network with the ground truth, supervised training is performed using back-propagation with a binary cross entropy loss. 
		\D 
		{\color{red} \textbf{rev 2} In Figure 2 part (D), a stopping condition is explicitly given. However, this rule is not explicitly mentioned in the text. The relevant part is the “Concurrent action selection” on page 16. This section should be improved for clarity. And, explicit connections should be drawn to Figure 2.}
		If the predicted accuracy in the output of the ``Where'' network is higher than that predicted in the ``What'' network, the position of maximal activity in the ``Where'' pathway serves to generate a saccade which shifts the center of gaze. % Then, and classify the foveal image using the ``What'' pathway. %
		%\E TODO describe log polar
		%\F TODO accuracy map 
		\label{fig:methods}}%
\end{figure}%
%%------------------------------%

\section*{Detailed implementation}
\label{sec:implementation}
%To test the validity of our hypothesis, let us find a function implementing the ``Where'' network. This function should be able to find the position of an object knowing only the degraded retinal image. Here, we describe the methods that we will follow to find that function, from the generative models (first external and then internal) to the actual implementation of the ``Where'' pathway. %

Modern parametric classifiers are composed of many layers (hence the term ``Deep Learning'') that can be trained through gradient descent over arbitrary input and output feature spaces. The ease of use of those tightly optimized training algorithms allows for the quantification of the difficulty of a task through the failure or success of the training.
The simplified anatomy of the agent is composed of two separate pathways whose processing is realized by such a neural network. Each network is trained and tested separately on distinct datasets, before being finally evaluated in a dynamic vision setup (see next section). 


\subsection*{Images generation}
%=================================================================
We define here the generative model for input display images as shown first in Figure~\ref{fig:intro}-A (\DIS ) and as implemented in Figure~\ref{fig:methods}-A.

\paragraph{Targets.} Following a common hypothesis regarding active vision, visual scenes consist of a single visual object of interest. We use the MNIST dataset of handwritten digits introduced by~\cite{Lecun1998}: %Indeed, we are here focused on the problem of localization (``Where'' pathway) and classification solutions (``What'' pathway) abound for this class of targets.
Samples are drawn from the dataset of $60000$ grayscale $28\times 28$ pixels images and separated between a training and a validation set (see below the description of the ``Where'' network).

\paragraph{Full-scale images.} 
{\color{red}\textbf{ Unclear (rev 2)}
Each sample position is drawn at random in a full-scale image of size $128\times 128$. To enforce isotropic saccades, a centered circular mask covering the image (of radius $64$ pixels) is defined, and the position is such that the embedded sample fits entirely into that circular mask.}

\paragraph{Background noise setting.} To implement a realistic background noise, we generate synthetic textures~\cite{Sanz12} using a bi-dimensional random process. %The generated texture images are of the same size than the full-image.
The texture is designed to fit well with the statistics of natural images. We chose an isotropic setting where textures are characterized by solely two parameters, one controlling the median spatial frequency $s\hspace{-2pt}f_0$ of the noise, the other controlling the bandwidth around the central frequency. Equivalently, this can be considered as the band-pass filtering of a random white noise image. The spatial frequency is optimized at $0.1\text{ pixel}^{-1}$ to fit that of the original digits. This specific spatial frequency occasionally allows to generate some ``phantom'' digit shapes in the background. Finally, these images are rectified to have a normalized contrast.

\paragraph{Mixing the signal and the noise.} Finally, both the noise and the target image are merged into a single image. Two different strategies are used. A first strategy emulates a transparent association, with an average luminance computed at each pixel, while a second strategy emulates an opaque association, choosing for each pixel the maximal value. The quantitative difference was tested in simulations, but proved to have a marginal importance.
%
\subsection*{Foveal vision and the ``What'' pathway}
%=================================================================
%


At the core of the vision system is the identification module, i.e. the ``What'' pathway. It consists of a classic convolutional classifier showing some translation invariance. This translation invariance can be measured in the form of a shift-dependent accuracy map. Importantly, it can quantify its own classification uncertainty, that may allow comparisons with the output of the ``Where'' pathway.

The foveal input is defined as the $28\times 28$ grayscale image extracted at the center of gaze (see dashed red box in Figure~\ref{fig:intro}-C). 
This image is passed unmodified to the agent's visual categorical pathway (the ``What'' pathway), that is realized by a convolutional neural network, here the well-known ``LeNet'' classifier~\cite{Lecun1998}. The network structure, that processes the input to identify the target category, is  provided (and unmodified) by the Pytorch library~\cite{Paszke17}.
It is made of 3 convolution layers followed by two fully-connected layers.
{\color{red} \textbf{rev 2} Authors claim the foveal processing has some translational invariance. However, they do not mention the use of any pooling layers in the convolutional net. Are there any pooling layers? If not, how does the conv net achieve translation invariance? Would not it be better to use max-pooling?  The architecture of the conv net is not given in the paper. And from the reference [26], I could not find it. Care must be taken when formatting the references. If the reference is a web-page, its URL must be given along with its last accessed date. }
%. It is trained over the (centered) MNIST dataset after approx $20$ training epochs. %Input images are rectified (\emph{with a mean and standard deviation of respectively $0.1307$ and $0.3081$ {\bf why??}}).} \fi 
The network output is a vector representing the probability of detecting each of the $10$ digits. The argument of the output neuron with maximum probability provides the image category. 
%This strategy achieves an average $98.7\%$ accuracy on the validation dataset~\cite{Lecun1998}.%

\begin{figure}[t!]%%[p!]
	\centering{\includegraphics[width=\linewidth]{fig_what_accmap.png}}
	\caption{\A Input samples from the ``What'' training set, with  randomly shifted targets using a Gaussian bivariate spatial offset with a standard deviation of 15 pixels. The target contrast is randomly set between 0.3 and 0.7.
	\B $55 \times 55$ shift-dependent accuracy map, measured for different target eccentricities on the test set after training.  
	}
\label{fig:accuracy}
\end{figure}

A specific dataset is constructed to train the network. It is made of  randomly shifted/randomly attenuated digits overlayed over a noisy background, as defined above. Both the offset, the contrast and the background noise render the task more difficult than the original MNIST classification. The relative contrast of the digit is randomly set between 0.3 and 0.7.  The network is trained incrementally by progressively increasing the offset variability (of a bivariate central gaussian) by increasing the standard deviation from 0 to 15 (with a maximal offset set at 25 pixels). The network is trained on a total of 75 epochs, with 60000 examples generated at each epoch from the MNIST original training set. The shifts and backgrounds are re-generated at each epoch. The shift standard deviation increases of one unit every 5 epochs.  Note that at the end of the training, many digits fall outside the center of the fovea, so that many examples are close to impossible to classify, either because of a low contrast or a too large eccentricity. At the end of the training process, the average accuracy is thus of 34\% (though it had a 91\% accuracy after the 5th epoch, when the digits were only at the center). 

{\color{red} \textbf{Unclear (rev 2)} After training, a shift-dependent accuracy map is computed by systematically testing the network accuracy on every horizontal and vertical offset, each on a set of 1000 samples generated from the MNIST test set, within a range of $+/-$27 pixels (see figure \ref{fig:accuracy}).  This forms a $55\times 55$ accuracy map showing higher accuracy at the center, and a slow decreasing accuracy with target eccentricity (with over 70\% accuracy plateau showing a shift invariance on a 7-8 pixels eccentricity radius)}. This significant shift invariance is a known effect of convolutional computation, that is obtained here at the cost of a lesser central recognition rate (around 80\%), remembering the classification task is here harder by construction. The accuracy fastly drops for greater than 10 pixels eccentricity, reaching the baseline 10\% chance level at around 20 pixels offset. 
%, as shown in(with a central value of $80\%$ corresponding to the validation score without any translational shift),




\subsection*{Peripheral vision: from log-polar feature vectors to log-polar action maps}
%
% page 476 de :
%
% https://www.asc.ohio-state.edu/golubitsky.4/reprintweb-0.5/output/papers/6120261.pdf
%
% Estimates ofw0D0.087 and2D0.051 inappropriate units can be obtained from Drasdo’s data on the human retina,andaandbare constants. These values correspond to a magnification factor of 11 .5 mm/degrees of visual angle at the fovea and 5.75 mm/degrees
%An interesting perspective is given with previous modeling of such foveated sensors. 
{\color{magenta} [[\textbf{Rev 1} : I think it would be better to first simply describe the architecture of the pathway (linear filters with retinotopic distribution, linear operation (??) and sigmoid to predict local accuracies) and then go into more detail on why the different parts where designed in a certain way. Also, maybe a figure that shows more architectural details of the "where"-pathway than figure 2 might be helpful.]]}

The ``Where'' pathway is devoted to choosing the next saccade. 
Here we assume the ``Where'' implements the following action selection: where to look next in order to reduce the uncertainty about the target identity? This implies moving the eye such as to increase the ``What'' classifier accuracy. For a given visual field, each possible future saccade has an expected accuracy, that can be trained from the ``What'' pathway output. To accelerate the training, we use a shortcut that is training the network on a translated accuracy map. The output is thus an accuracy map, that tells for each possible visuo-motor displacement the value of the future accuracy.

\paragraph{Primary visual representation: log-polar orientation filters}
In order to reduce the processing cost, and in accordance with observations \cite{connolly1984representation,sparks1987sensory}, a similar log-polar compression pattern is assumed to be conserved from the retina up to the primary motor layers.
The non-uniform sampling of the visual space is adequately modeled as a log-polar conformal mapping, as it provides a good fit with observations in mammals~\cite{Traver10} which has a long history in computer vision and robotics. 
%A first property of this mapping is the separation between the foveal and the peripheral areas as we defined above.  However, this sensor is to our knowledge most often not coupled to an action. 
%\if 0\ICANN
% {\color{blue}(but see~\cite{ref_needed)}}\fi. 
Both the visual features and the output accuracy map are to be expressed in retinal coordinates. 
{\color{magenta} [[ \textbf{Rev 1} It would be very helpful to have at least a little bit more details on what the log-polar oriented filters are. Right now there is only a reference to [28].]]} 
On the visual side, local visual features are extracted as oriented edges as a combination of the retinotopic transform with primary visual cortex filters~\cite{Fischer2007a}. The centers of these first and second order orientation filters are radially organized around the center of fixation, with small and tightened receptive fields at the center and more large and scarce receptive fields at the periphery.
% see Figure~\ref{fig:methods}-B. 
The size of the filters increases proportionally to the eccentricity.  The filters are organized in $10$ spatial eccentricity scales (respectively placed at around $2$, $3$, $4.5$, $6.5$, $9$, $13$, $18$, $26$, $36.5$ , and $51.3$ pixels from the center) and $24$ different azimuth angles allowing them to cover most of the original $128 \times 128 $ image. At each of these positions,  $6$ different edge orientations and $2$ different phases (symmetric and anti-symmetric) are computed. This finally implements a (fixed) bank of linear filters which models the receptive fields of the primary visual cortex.

To ensure the balance of the coefficients across scales, the images are first whitened and then linearly transformed into a ``primary visual'' feature vector $\boldsymbol{x}$.  The length of this vector is $2880$, such that the retinal filter compresses the original image by about 83\%, with high spatial frequencies preserved at the center and only low spatial frequencies conserved at the periphery. In practice, the bank of filters is pre-computed and placed into a matrix for a rapid transformation of input batches into  feature vectors. This matrix transformation allows also the evaluation of a reconstructed visual image given a retinal activity vector thanks to a pseudo-inverse of the forward transform matrix. In summary, the full-sized images are transformed into a primary visual feature vector which is fed to the ``Where'' pathway.

\paragraph{Visuo-motor representation: ``Collicular'' accuracy maps}

The output of the ``Where'' pathway is defined as an \emph{accuracy map} representing the recognition probability after moving the eye, independently of its identity. Like the primary visual map, this target accuracy map is also organized radially in a log-polar fashion, making the target position estimate more precise at the center and fuzzier at the periphery. This modeling choice is reminiscent of the approximate log-polar organization of the superior colliculus (SC) motor map \cite{sparks1987sensory}. In ecological conditions, this accuracy map should be trained by sampling, i.e. by ``trial and error'', using the actual recognition accuracy (after the saccade) to grade the action selection. 
% )for instance corrective saccades to compute (a posteriori) the probability of a correct localization. In a computer simulation however, this induces a combinatorial explosion which does render the calculation not amenable.}
In practice, as we generate the visual display, the position of the target (which is hidden to the agent) is known. 
Under an ergodic assumption, knowing both the translational shift imposed to the visual field by a saccade of known amplitude, and the shift-dependent accuracy map of the  ``What'' classifier (Figure~\ref{fig:accuracy}-B), the full accuracy map at each pixel can be predicted for each visual sample by shifting the central accuracy map on the true position of the target. Such a computational shortcut is allowed by the independence of the categorical performance with position. % thanks to the independence which is implemented in the exteroceptive generative model.}
{\color{magenta} \textbf{Rev 1 I don't really understand : }}{\color{blue} This full accuracy map is log-polar projected to provide the expected accuracy of each hypothetical saccade in a retinotopic space. In practice, we use the energy of the filters at each position as a proxy to quantify the projection from the metric space to the retinotopic space. This generates a filter bank at $10$ spatial eccentricity scales and $24$ different azimuth angles, i.e. $240$ output filters. Each filter is normalized such that the value at each log-polar position is the average of the values which are integrated in visual space. Applied to the full sized ground truth accuracy map computed in metric space, this gives an accuracy map at different location of a retinotopic motor space. Such transform is again implemented by a simple matrix multiplication which can be pre-computed to fasten calculations.} Practically, this also allows to compute an inverse transform using the pseudo-inverse matrix of the forward transform. In particular, that inverse transform is used to represent the accuracy predicted by any given visual feature vector, but also to compute the position of maximal accuracy in metric space to set up the sensor displacement.

%\subsection{Implementing the ``Where'' pathway}
%=================================================================

\paragraph{Classifier training}

Consider the retinal transform $\boldsymbol{x}$ as the input and a log-polar retinotopic vector $\boldsymbol{a}$ made of $n$ Bernouilli probabilities (success probabilities) as the output. %Following the active inference framework, we will train
The network is trained to predict the distribution $\boldsymbol{a}$ knowing the retinal input $\boldsymbol{x}$ by comparing it to the known ground truth distribution computed over the motor map. 
{\color{magenta} \textbf{Rev 1} According to line 409 the loss function of the classifier is a KL-divergence. However, neither the output of the accuracy map nor the retinotopic vector a are distributions. As far as I understood the, they encode a probabilities for each location. So I don't really see how a KL-divergence could be applicable here.}
The loss function that comes naturally is the Binary Cross-Entropy (negative term of the Kullback-Leibler divergence) between the ground truth and the predicted map (assuming the independence of the output map features).

%The ``What'' pathway will be given from the literature. In contrast, the ``Where'' pathway takes the into account in order to tell whether a target is present at the different peripheral locations, in order to monitor future saccades.

The parametric neural network consists of a primary visual input layer, followed by two fully connected hidden layers of size 1000  with rectified linear activation,
%{\bf (!! 1000 !!)} $500$ and {\bf(!! 1000 !!)} $2000$ units respectively
and a final output layer with a sigmoid nonlinearity to ensure that the output is compatible with a likelihood.
%In addition, we tested the effect of $50 \%$ drop-out on the last hidden layer.
%Another improvement in convergence speed that was obtained by using batch normalization. 
The network is trained on 60 epochs of 60000 samples, with a learning rate equal to $10^{-4}$ and the Adam optimizer \cite{kingma2014adam}.
%stochastic gradient descent with a momentum to improve convergence. 
%This scalar measures the distance between both distributions (it is always positive and null if and only if they are equal). 
The full training takes about $1$ hour on a laptop. The code is written in Python (version 3.7.6) with pyTorch library~\cite{Paszke17} (version 1.1.0). The full scripts for reproducing the figures and extending the results to a full range of parameters is available at \url{https://github.com/laurentperrinet/WhereIsMyMNIST}. % TODO: change to https://github.com/SpikeAI/DauceAlbigesPerrinet19 when public
