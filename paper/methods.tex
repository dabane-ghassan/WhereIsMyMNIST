% !TEX root = paper.tex
% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% !TEX spellcheck = en-US
\section{Methods}


\if 1\CNS

The visual scene is here made of a foreground (the target) and a noisy background. An agent controls a focal visual sensor that can move over the visual scene through saccades. 
% both the visual features and the expected target position to be expressed in log-polar coordinates. 
On the primary visual side, local visual features (first and second order orientation filters) are radially organized around the center of fixation, with small and tightened receptive fields at the center and more large and scarce receptive fields at the periphery. The issued observation vector $\boldsymbol{x}$ compresses the original image by about 90\%, with high spatial frequencies preserved at the center and only low spatial frequencies conserved at the periphery. 
%The target accuracy map is also organized radially in a log-polar fashion, making the target position estimate more precise at the center and fuzzier at the periphery. This modeling choice is reminiscent of the approximate log-polar organization of the superior colliculus (SC) motor map {\bf[TODO:REF]}. 
%This retinotopic organization is preserved along the visuo-motor pathway as expected from observations {\bf[TODO:REF]}.

The visual categorical pathway (the ``What'' pathway) is supposed to be realized by a classifier, that mostly processes the central pixels to identify the target category. This central classifier, that is not implemented here, should display a high accuracy at the center, and aÂ shortly decreasing accuracy with target eccentricity, as shown in figure \ref{fig:results}{\bf e}.
In contrast, the visual orientation pathway (the ``Where'' pathway) takes the full visual field into account in order to tell whether a target is present at the different peripheral locations. In detail, the visual information is  transformed so as to predict how accurate the categorical classifier will be \emph{after the saccade is carried out}. The set of all possible saccade predictions forms an \emph{accuracy map}. It is organized radially, preserving the initial retinotopic organization, with high predicted accuracies reflecting a high probability of target presence at given locations. Such  a \emph{predictive accuracy map} is assumed to be the core of saccade-based vision, with action selection relying on predicting the effect-of-action over the full saccadic motor map.

In a probabilistic setting, the accuracy map reflects the probability of correctly classifying a target over the full action space. Taking $\boldsymbol{u}$ a possible saccade and $\tilde{\boldsymbol{x}}$ the corresponding future visual field, the result of the categorical classifier over $\tilde{\boldsymbol{x}}$ can either be correct (1) or incorrect (0). If this experiment is repeated many times over many visual displays, the probability of correctly classifying $\tilde{\boldsymbol{x}}$ at $\boldsymbol{u}$ forms a probability, i.e. a number between 0 and 1, that reflects the proportion of correct and incorrect classifications when issuing a saccade $\boldsymbol{u}$ after seeing $\boldsymbol{x}$ (the initial visual field). 
Our main argument is that such an accuracy map is trainable in a rather straightforward way, through trials and errors, by actuating saccades after seeing $\boldsymbol{x}$, and taking the final classification success or failure as a teaching signal.

In our shortcut implementation, a parametric classifier is trained...

\fi



\if 0\CNS

\subssection{Notations}
\begin{itemize}
	\item $\boldsymbol{x}$ : visual field (image)
	\item $\boldsymbol{y}$ : target category (categorical)
	\item $\boldsymbol{u}$ : target position (real coordinates or categorical, retinocentric referential)

\end{itemize}

Generative model :
$$ \boldsymbol{x} \sim P(X|\boldsymbol{y}, \boldsymbol{u}) $$

Full inference (posterior):
$$ P(Y, U|\boldsymbol{x}) \propto  P(\boldsymbol{x}|Y, U) $$

Independence assumptions :
\begin{equation} 
P(Y, U) = P(Y)  P(U) \text{\emph{ (toujours vrai)}}
\label{eq:indep-1}
\end{equation}

\begin{equation}  
P(Y, U|X) = P(Y|X)  P(U|X) \text{\emph{ (faux s'il y a plusieurs cibles)}}
\label{eq:indep-2}
\end{equation}

Partial inference on object category:
$$ P(Y|\boldsymbol{x}, \boldsymbol{u}) \propto  P(\boldsymbol{x}|Y, \boldsymbol{u}) $$

Partial inference on object position:
$$ P(U|\boldsymbol{x}, \boldsymbol{y}) \propto  P(\boldsymbol{x}|U, \boldsymbol{y}) $$

Marginals:
\begin{itemize}
\item $ P(Y|\boldsymbol{x}) = \int P(Y|\boldsymbol{x}, \boldsymbol{u}) d\boldsymbol{u}$
\item $ P(U|\boldsymbol{x}) = \int P(U|\boldsymbol{x}, \boldsymbol{y}) d\boldsymbol{y}$
\end{itemize}

\subsubsection{What we did so far...}

Consider a view $\boldsymbol{x}$ that contains a single target $\boldsymbol{y}$ at unknown retinocentric position $\boldsymbol{u}$. The brain needs to guess both  $\boldsymbol{y}$ and $\boldsymbol{u}$ with limited computational resources. 
   
We assume here that the brain adopts independence assumption (\ref{eq:indep-2}), making a separation between the ``Where'' and the ``What'' pathways, forming separate (and cheaper) inferences :
\begin{itemize}
\item $p(Y|\boldsymbol{x})$
\item $p(U|\boldsymbol{x})$
\end{itemize}

Another assumption is that the category $\boldsymbol{y}$ is \emph{translationally invariant}: given a transformation $\mathcal{T}$,
$$\mathcal{T}(\boldsymbol{u}, \boldsymbol{y}) 
= (\mathcal{T}(\boldsymbol{u}), \boldsymbol{y})$$

Now, given $\boldsymbol{x}$ and the separation assumption, it is sensible to change the viewpoint to better estimate $\boldsymbol{y}$, because  $\boldsymbol{y}$ is invariant to the viewpoint transformation.

This is where \emph{active inference} comes into the play:
\begin{itemize}
\item Consider that the true target is $\hat{\boldsymbol{y}}$
\item Consider that the target current retinocentric position is $\boldsymbol{u}$
\item Then, for any translation $\delta \boldsymbol{u}$, the future posterior on the true target is estimated by:
$\mathbb{E}_{\boldsymbol{x}'\sim p(X|\hat{\boldsymbol{y}}, \boldsymbol{u}+\delta \boldsymbol{u})} p(\hat{\boldsymbol{y}}|\boldsymbol{x}')$
\item And the optimal translation is:  $\underset{\delta\boldsymbol{u}}{\text{ argmax }}  \mathbb{E}_{\boldsymbol{x}'\sim p(X|\hat{\boldsymbol{y}}, \boldsymbol{u}+\delta \boldsymbol{u})} p(\hat{\boldsymbol{y}}|\boldsymbol{x}')$
\end{itemize}

If now $\boldsymbol{u}$ is unknown and needs to be guessed from $\boldsymbol{x}$, the optimal translation is:
$$\underset{\delta\boldsymbol{u}}{\text{ argmax }} \mathbb{E}_{\boldsymbol{u}\sim p(U|\boldsymbol{x})} \mathbb{E}_{\boldsymbol{x}'\sim p(X|\hat{\boldsymbol{y}}, \boldsymbol{u}+\delta \boldsymbol{u})} p(\hat{\boldsymbol{y}}|\boldsymbol{x}')$$
with :
\begin{itemize}
\item $p(U|\boldsymbol{x})$ the inferred target position
\item and $\mathbb{E}_{\boldsymbol{x}'\sim p(X|\hat{\boldsymbol{y}}, \boldsymbol{u}+\delta \boldsymbol{u})} p(\hat{\boldsymbol{y}}|\boldsymbol{x}')$ the expected inference on the actual target.
\end{itemize}

\subsubsection{Accuracy maps}
 
In practice, it is computationally impossible to make exact guesses about the future observation $\boldsymbol{x}'$. Our second assumption is that instead of predicting future inferences on true target, the brain trains a \emph{parametric accuracy map} by experience (trial and error).


In a model-based approach, the \emph{accuracy maps} can be calculated using a parametric classifier : 
 \begin{itemize}
 \item Given a training set $\{(x_1, u_1, y_1), ..., (x_n, u_n, y_n)\}$:
 \begin{itemize}
 \item Train a classifier $p_\theta$ that estimates $p(Y|\boldsymbol{x})$. 
 \end{itemize}
 \item Then, for each class $\hat{\boldsymbol{y}}$, taking $\tilde{\boldsymbol{y}}\sim p_\theta(Y|\boldsymbol{x})$,\emph{ the classification rate $r_\theta(\boldsymbol{u})$ is an estimator of the posterior expectation :}
 \begin{align*}
 r_\theta(\boldsymbol{u}) 
 &= \mathbb{E}_{ \boldsymbol{x} \sim p(X|\hat{\boldsymbol{y}}, \boldsymbol{u})}
 \mathbb{E}_{\tilde{\boldsymbol{y}}\sim p_\theta(Y|\boldsymbol{x})} \delta_{\hat{\boldsymbol{y}}=\tilde{\boldsymbol{y}}}\\
 &= \mathbb{E}_{ \boldsymbol{x} \sim p(X|\hat{\boldsymbol{y}}, \boldsymbol{u})} p_\theta(\hat{\boldsymbol{y}}|\boldsymbol{x})\\
 &\simeq \mathbb{E}_{ \boldsymbol{x} \sim p(X|\hat{\boldsymbol{y}}, \boldsymbol{u})} p(\hat{\boldsymbol{y}}|\boldsymbol{x})
 \end{align*} 
 that forms an \emph{accuracy map} for each target position $\boldsymbol{u}$.\\
 \end{itemize}

\subsubsection{Parametric transformation (Colliculus?) map}

One can now select $\delta\boldsymbol{u}$ with the parametric estimator:
\begin{align*}
\widehat{\delta\boldsymbol{u}} &\simeq \underset{\delta\boldsymbol{u}}{\text{ argmax }} 
\mathbb{E}_{\boldsymbol{u}\sim p(U|\boldsymbol{x})}  
r_\theta(\boldsymbol{u}+\delta\boldsymbol{u})\\
%&= \underset{\boldsymbol{u}' \in \mathcal{U}}{\text{ argmax }} A(\boldsymbol{u}'|\boldsymbol{x}, \boldsymbol{u})
&= \underset{\delta\boldsymbol{u}}{\text{ argmax }} Q(\delta\boldsymbol{u}|\boldsymbol{x})
\end{align*}
with $Q(\delta\boldsymbol{u}|\boldsymbol{x})$ the \emph{transformation} map, given the view $\boldsymbol{x}$ and the marginal posterior estimate $p(U|\boldsymbol{x})$. 

It must be noticed that, given $\hat{\boldsymbol{u}} = \underset{\boldsymbol{u}}{\text{ argmax }} 
r_\theta(\boldsymbol{u}))$,  the transformation map is maximal at $\delta\boldsymbol{u} = \hat{\boldsymbol{u}} - \boldsymbol{u}$. Each initial $\boldsymbol{u}$ provides a different transformation map, that is a shift of the original accuracy map (\emph{Ergodic assumption??}).
 
We assume in the following that a parametric action value map $Q_\psi$ can be trained on top of the parametric classifier $p_\theta$ and its accuracy map $r_\theta$.
The training set is $\{(\boldsymbol{x}_1, \boldsymbol{u}_1), ..., (\boldsymbol{x}_n, \boldsymbol{u}_n)\}$ and the accuracy map classifier learns to associate each $\boldsymbol{x}$ with its full transformation map $Q(.|\boldsymbol{x})$. 



\subsubsection{Algorithms}

Once $p_\theta$ and $Q_\psi$ are trained, the recognition algorithm is straightforward:  

\paragraph{Single saccade algorithm:}
\begin{enumerate}
	\item Read the view $\boldsymbol{x}$
	\item Choose $\delta\boldsymbol{u}$ according to $Q_\psi(.|\boldsymbol{x})$
	\item Move the eye 
	\item Update the view $\boldsymbol{x}'$
	\item Identify the target with $\tilde{\boldsymbol{y}} \sim p_\theta(Y|\boldsymbol{x}')$
\end{enumerate}


\paragraph{Multi saccades algorithm:}
\begin{enumerate}
\item $q(Y) \leftarrow$ uniform distribution
\item Read the view $\boldsymbol{x}$
\item Choose $\delta\boldsymbol{u}$ according to $Q_\psi(.|\boldsymbol{x})$
\item Repeat several times up to some posterior confidence threshold:
	\begin{enumerate}
		\item Move the eye 		
		\item Read $\boldsymbol{x}$
		\item $q(Y) \leftarrow q(Y) \times p_\theta(Y|\boldsymbol{x})$
		\item normalize $q$
		\item Choose $\delta\boldsymbol{u}$ according to $Q_\psi(.|\boldsymbol{x})$ (with some inhibition of return mechanism)
	\end{enumerate}
	\item Identify the target with $\tilde{\boldsymbol{y}} \sim q(Y)$
\end{enumerate}

\subsection{Visual transformation}
\subsubsection{Wavelets}
\subsubsection{Log Gabor}

\subsection{Accuracy map}

\subsection{Network architecture}
\fi

